\chapter{``Eigenstuff''}

\section{Motivation}\label{sct4.1}

As mentioned at the very beginning, linear algebra is the study of vector spaces.  More accurately, it is the study of the \emph{category} of vector spaces.  Thus, not only are we interested in vector spaces, but we're also interested in studying linear-transformations.  In fact, linear-transformations are arguably more important than the vector spaces themselves.

We saw in the last chapter that, given choice of bases, we can associate matrices to linear-transformations.  This is incredibly useful as matrices are more concrete and amenable to (scary) things like computation.  However, there isn't just one matrix associated to a linear-transformation, but you get one matrix for every choice of bases.  Some of these matrices might be quite ugly, and others might be quite nice.  The motivating objective for this chapter is to try to find basis in which the associated matrix is \emph{nice}.

Perhaps the simplest matrix one might hope for is a \emph{diagonal} matrix.\footnote{It turns out that one can almost, but not quite, do this.  In any case, having this as an objective, even if it can't always be obtained, it sufficient for motivation.}  Let us investigate what it would require for our matrix to be diagonal.

So, let $V$ be a finite-dimensional vector space, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis for $V$, and let $T\colon V\rightarrow V$ be a linear operator.  Looking back to the defining theorem of $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ (\cref{CoordinatesLinearTransformation}), we see that the $k^{\text{th}}$ column of this matrix is given by
\begin{equation}
	\coordinates{T(b_k)}{\basis{B}},
\end{equation}
that is, the coordinates of the vector $T(b_k)\in V$ with respect to the basis $\basis{B}$.  To compute these coordinates (\cref{CoordinatesVector}), we write $T(b_k)$ as a linear combination of the elements of $\basis{B}$:
\begin{equation}\label{eqn4.1.2}
	T(b_k)=T\indices{^1_k}b_1+\cdots +T\indices{^d_k}b_d,
\end{equation}
so that
\begin{equation}
	\coordinates{T(b_k)}{\basis{B}}=\begin{bmatrix}T\indices{^1_k} \\ \vdots \\ T\indices{^d_k}\end{bmatrix}.
\end{equation}
Now, for a matrix to be diagonal, by definition, everything in the $k^{\text{th}}$ column should vanish except for possibly the $k^{\text{th}}$ entry in that column.  Thus, as $\coordinates{T(b_k)}{\basis{B}}$ is the $k^{\text{th}}$ column of $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$, if this matrix is to be diagonal, we had better have that $T\indices{^l_k}=0$ for $l\neq k$.  Plugging this back into \eqref{eqn4.1.2}, we find
\begin{equation}
	T(b_k)=T\indices{^k_k}b_k.
\end{equation}
We have found the following.
\begin{displayquote}
	If $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ is to be a diagonal matrix, it had better be the case that $T(b_k)$ is a scalar multiple of $b_k$ for all $b_k\in \basis{B}$.
\end{displayquote}

It is thus of interest to find a basis consisting of vectors $b$ such that $T(b)$ is a scalar multiple of $b$.  Such vectors have a name:  they are \emph{eigenvectors} of $T$.\footnote{By the way, in case it's not obvious, ``eigenstuff'' is a catch-all term I'm using as short-hand for ``eigenvalues, eigenvectors, and eigenspaces''.}

\section{Basic definitions}

\begin{dfn}{Eigenspaces, eigenvalues, and eigenvectors}{Eigen}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $W\subseteq V$ be a nonzero subspace, and let $\lambda \in \K$.  Then, $W$ is a \term{$\lambda$-eigenspace}\index{Eigenspace} iff
	\begin{enumerate}
		\item \label{Eigen(i)}$\restr{T}{W}=\lambda \id _W$; and
		\item \label{Eigen(ii)}$W$ is maximal with this property.
	\end{enumerate}
	\begin{rmk}
		Such a $\lambda$ is referred to as an \term{eigenvalue}\index{Eigenvalue} of $T$, and we write
		\begin{equation}
			\Eig (T)\ceqq \left\{ \lambda \in \K :\lambda \text{ is an eigenvalue of }T\text{.}\right\} .
		\end{equation}\index[notation]{$\Eig (T)$}
	\end{rmk}
	\begin{rmk}
		Nonzero elements of $W$ are \term{eigenvectors}\index{Eigenvector} of $T$ with eigenvalue $\lambda$.
	\end{rmk}
	\begin{rmk}
		Warning:  The eigenspace is \emph{not} the set of eigenvectors---we must have $0\in W$ as $W$ is a subspace, but, by definition, $0\in W$ is forbidden from being an eigenvector.
	\end{rmk}
	\begin{rmk}
		Warning:  Eigenvalues need not be unique---see \cref{exm4.2.12}.  On the other hand, they certainly will be for vector spaces---see \cref{prp4.2.13}.
		
		Eigenvectors on the other hand are essentially never unique:  any scalar multiple of an eigenvector is another eigenvector with the same eigenvalue (because eigenspaces are in particular subspaces).
	\end{rmk}
	\begin{rmk}
		To clarify, \cref{Eigen(ii)} means the following.  If $U\subseteq V$ is a nonzero subspace with $U\supseteq W$ and for which there is some $\mu \in \K$ such that $\restr{T}{U}=\mu \id _U$, then $U=W$.
	\end{rmk}
	\begin{rmk}
		Recall that (\cref{exm1.1.36}) scaling by elements of $\K$ need not actually define a linear-transformation.  Thus, it is implicit in this condition that $\lambda \id _W\colon W\rightarrow W$ is actually a linear-transformation.  If $V$ is a vector space, this will actually force $\lambda$ to commute with everything in $\K$ (in which case scaling by $\lambda$ defines a linear-transformation on all of $V$)---see \cref{thm4.2.17}.
	\end{rmk}
	\begin{rmk}
		One reason to see we need the maximality condition is to get uniqueness of the eigenspace.  For example, consider the linear-transformation $\R ^2\rightarrow \R ^2$ defined by the matrix
		\begin{equation}
			\begin{bmatrix}2 & 0 \\ 0 & 2\end{bmatrix}.
		\end{equation}
		Without the maximality condition, every nonzero subspace would be considered a $2$-eigenspace.  But that's silly.  We want our definition to be so that, in this example, there is only one $2$-eigenspace, namely all of $\R ^2$.
	\end{rmk}
	\begin{rmk}
		If we allowed $W=0$, then we might have stupid things happening like every scalar being an eigenvalue of $T$.  Thus, unless you're a fan of stupid things, you're going to want $W=0$ here.
	\end{rmk}
	\begin{rmk}
		The \term{eigenspaces}, \term{eigenvalues}, and \term{eigenvectors} of a matrix are respectively the eigenspaces, eigenvalues, and eigenvectors of the associated linear-transformation.
	\end{rmk}
\end{dfn}
Before moving on, there is a common mistake that students make that we should clear up.
\begin{displayquote}
	Eigen\emph{vectors} cannot be $0$.  On the other hand, eigen\emph{values} can be.
	
	This is of course true just by definition.  The reason we exclude the zero vector from being an eigenvector however is because we would always have $T(0)=\lambda \cdot 0$ for all $\lambda \in \K$, not a particularly interesting condition.
\end{displayquote}

Our first order of business is to establish the uniqueness of eigenspaces, which will allow us in particular to utilize unambiguous notation to denote them.
\begin{prp}{Eigenspaces are unique}{prp4.2.3}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $\lambda \in \K$, and let $U,W\subseteq V$ be $\lambda$-eigenspaces of $T$.  Then, $U=W$.
	\begin{rmk}
		We denote the unique $\lambda$-eigenspace of $V$ by $\Eig _{\lambda ,T}$\index[notation]{$\Eig _{\lambda ,T}$}.  If $T$ is clear from context, we may simply write $\Eig _{\lambda}\ceqq \Eig _{\lambda ,T}$\index[notation]{$\Eig _{\lambda ,T}$}.
	\end{rmk}
	\begin{rmk}
		If ever we write ``$\Eig _{\lambda ,T}$'' without having explicitly said so, it should be assumed that $\lambda$ is an eigenvalue of $T$.
	\end{rmk}
	\begin{rmk}
		Warning:  Though eigenvalues themselves may still not be unique---we may still have $\Eig _{\lambda ,T}=\Eig _{\mu ,T}$ for $\lambda \neq \mu$.
	\end{rmk}
	\begin{rmk}
		Of course, $\lambda$-eigenspaces need not exist---this only says that, \emph{if} they do exist, they must be unique.
	\end{rmk}
	\begin{proof}
		$U+W$ is a subspace of $V$.  Furthermore, for $u+w\in U+W$, with $u\in U$ and $w\in W$, we have
		\begin{equation}
			T(u+w)=T(u)+T(w)=\lambda u+\lambda w=\lambda (u+w),
		\end{equation}
		and so $\restr{T}{U+W}=\lambda \id _{U+W}$.  As $U+W\supseteq U,W$, by maximality of eigenspaces, we have $U=U+W=W$.
	\end{proof}
\end{prp}
According to the definition, in order to show that $\lambda$ is an eigenvalue, you have to find a nonzero subspace on which $T$ acts by $\lambda$, and furthermore, is maximal with respect to this property.  Having to check maximality by hand would be obnoxious.  Fortunately, we have the following result that gives a relatively simple sufficient condition for guaranteeing that $\lambda$ be an eigenvalue.
\begin{prp}{Sufficient condition to be an eigenvalue}{}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \K$.  Then, if there is some nonzero subspace $W\subseteq V$ such that $\restr{T}{W}=\lambda \id _W$, then $\lambda$ is an eigenvalue.
	\begin{proof}
		Suppose that there is some nonzero subspace $W\subseteq V$ such that $\restr{T}{W}=\lambda \id _W$.  Define
		\begin{equation}
			\collection{W}\ceqq \left\{ W\subseteq V:W\text{ a nonzero subspace such that }\restr{T}{W}=\lambda \id _W\text{.}\right\} .
		\end{equation}
		\begin{exr}[breakable=false]{}{}
			Check that $\collection{W}$ is a partially-ordered set with respect to inclusion and satisfies the hypotheses of \nameref{ZornsLemma}.
		\end{exr}
		By \nameref{ZornsLemma}, $\collection{W}$ has a maximal element, and so $\lambda$ is an eigenvalue.
	\end{proof}
\end{prp}
In fact, eigenspaces aren't just maximal subspaces on which $T$ acts by $\lambda$, but in fact they are \emph{maximum} with this property.
\begin{prp}{Eigenspaces are maximum with their defining property}{}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $W\subseteq V$ be a subspace, and let $\lambda \in \K$ be an eigenvalue of $V$.  Then, if $\restr{T}{W}=\lambda \id _W$, then $W\subseteq \Eig _{\lambda}$.
	\begin{proof}
		Suppose that $\restr{T}{W}=\lambda \id _W$.  Define
		\begin{equation}
			\collection{U}\ceqq \{ U\subseteq V\text{ a subspace}:\restr{T}{U}=\lambda \id _U\} 
		\end{equation}
		and
		\begin{equation}
			E\ceqq \sum _{U\in \collection{U}}U.
		\end{equation}
		Every element in $E$ can be written as a finite sum $u_1+\cdots +u_m$ where $T(u_k)=\lambda u_k$ for each $u_k$, and so
		\begin{equation}
			T(u_1+\cdots +u_m)=\lambda (u_1+\cdots +u_m).
		\end{equation}
		That is, $\restr{T}{E}=\lambda \id _E$.  $E$ is maximal with this property, and hence $E=\Eig _{\lambda}$.  Finally, note that $W\in \collection{U}$, and hence $W\subseteq E=\Eig _{\lambda}$. 
	\end{proof}
\end{prp}
\begin{exm}{An eigenspace with infinitely many distinct eigenvalues}{exm4.2.12}
	Define $V\ceqq \Z /2\Z$, regard it as a $\K \ceqq \Z$-module, and define $T\ceqq 0$.  Then, of course $\restr{T}{V}=0\id _V$, but also $\restr{T}{V}=2\id _V$.  (It is obviously maximal as this is the entire space!)  Thus, $V\subseteq V$ is an eigenspace of $T$ with eigenvalues $0,2\in \K$.  Of course, $2$ is not special---any even integer would have worked just as well, and so there are in fact infinitely many distinct eigenvalues!
\end{exm}
\begin{prp}{Eigenvalues are unique (in vector spaces)}{prp4.2.13}
	Let $V$ be a vector space and let $T\colon V\rightarrow V$ be linear.  Then, if $\Eig _{\lambda ,T}=\Eig _{\mu ,T}$, it follows that $\lambda =\mu$.
	\begin{proof}
		Suppose that $\Eig _{\lambda ,T}=\Eig _{\mu ,T}$.  There must be some nonzero $v\in \Eig _{\lambda ,T}=\Eig _{\mu ,T}$.  It follows that
		\begin{equation}
			\lambda v=T(v)=\mu v,
		\end{equation}
		and so $(\lambda -\mu )v=0$.  As $v\neq 0$ and the ground ring is a division ring, we must have that $\lambda -\mu =0$, that is, $\lambda =\mu$.
	\end{proof}
\end{prp}

If you've seen eigenvalues and eigenvectors before, you'll note that this is similar, but not identical to how they are usually defined.  The motivation for this slight deviation has to do with the fact that, even for vector spaces, the ground division ring $\F$ need not be commutative.  In this case, we would like eigenvalues to define linear-transformations by scaling (for reasons we'll see in a moment), even if $\K$ itself is not commutative.  The definition as stated above \emph{implies} that eigenvalues have to be central, and hence define linear-transformations by scaling.  Thus, we don't have to put this condition into the definition by hand---in a sense, we get it for free.

Another advantage it has is that it places the emphasis on eigen\emph{spaces} instead of eigen\emph{vectors}.  Eigenspaces are the more fundamental of the two concepts simply because they are unique---in general, there will be infinitely many eigenvectors with the same eigenvalue, and no one of them is more `correct' than the other, whereas for eigen\emph{spaces} there is no arbitrary choice to be made---there's only one thing to pick from.  In any case, it would be good to know that our presentation is equivalent to the usual one.
\begin{thm}{}{thm4.2.17}
	$V$ be a vector space over a division ring $\F$, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \F$.  Then,
	\begin{enumerate}
		\item \label{thm4.2.17(i)}$\lambda$ is an eigenvalue of $T$ iff $\lambda$ is central and there is some nonzero $v\in V$ such that $T(v)=\lambda v$, in which case $v$ is an eigenvector of $T$ with eigenvalue $\lambda$; and
		\item \label{thm4.2.17(ii)}if $\lambda$ is an eigenvalue, then
		\begin{equation}
			\Eig _{\lambda ,T}=\{ v\in V:T(v)=\lambda v\} =\Ker (T-\lambda ).
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		Recall (\cref{Rg}) that for $\lambda$ to be \emph{central} means that $\lambda$ commutes with everything.  For example, in the quaternions $\H$, $\im ,\jm ,\km \in \H$ are \emph{not} central, though $1\in \H$ is.
		
		Of course, if $\F$ is commutative (that is, if $\F$ is a field), then everything is central, and may ignore this part of the result.  Upon doing so, we obtain exactly the condition you are likely familiar with (assuming you've seen eigenvalues before, that is).
	\end{rmk}
	\begin{rmk}
		Perhaps the most relevant fact for us about central elements is that scaling by them defines linear-transformations.  That is, if $\alpha \in \F$ is central, then $V\ni v\mapsto \alpha \cdot v\in V$ is a linear-transformation.
	\end{rmk}
	\begin{rmk}
		In particular, note that, if $0$ is an eigenvalue, then its eigenspace is precisely $\Ker (T)$.  Thus, $T$ is injective iff $0$ is \emph{not} an eigenvalue.
	\end{rmk}
	\begin{proof}
		\cref{thm4.2.17(i)} $(\Rightarrow )$ Suppose that $\lambda$ is an eigenvalue of $T$.  As $\lambda$ is an eigenvalue, there is a nonzero subspace $\Eig _{\lambda}\subseteq V$, such that $T(v)=\lambda v$ for all $v\in \Eig _{\lambda}$.  In particular, there is \emph{some} nonzero $v\in V$ such that $T(v)=\lambda v$.  By definition, $v$ is an eigenvector with eigenvalue $\lambda$.
		
		We now check that $\lambda$ is central.  Let $\alpha \in \F$.  Then,
		\begin{equation}
			(\alpha \lambda )\cdot v=\alpha (\lambda v)=\alpha T(v)=T(\alpha v)=\footnote{If $v\in \Eig _{\lambda}$, $\alpha v\in \Eig _{\lambda}$ as well because this is a subspace.}\lambda (\alpha v)=(\lambda \alpha )\cdot v.
		\end{equation}
		As $v\neq 0$ and we are working over a division ring, it follows that $\alpha \lambda =\lambda \alpha$, that is, $\lambda$ is central.
		
		\blni
		$(\Leftarrow )$ Suppose that $\lambda$ is central and that there is some nonzero $v\in V$ such that $T(v)=\lambda v$.  Define
		\begin{equation}
			W\ceqq \{ v\in V:T(v)=\lambda v\} .
		\end{equation}
		As $\lambda$ is central, this is a subspace.  It is furthermore a nonzero subspace by hypothesis.  By definition, $\restr{T}{W}=\lambda \id _W$.
		
		To show maximality, let $U\supseteq W$ be a subspace such that $\restr{T}{U}=\lambda \id _U$.  We then of course have that $T(u)=\lambda u$ for all $u\in U$, that is, $U\subseteq W$, showing maximality.  Hence,
		\begin{equation}\label{eqn4.2.22}
			W\ceqq \{ v\in V:T(v)=\lambda v\} =\Eig _{\lambda ,T}.
		\end{equation}
		In particular, $\lambda$ is an eigenvalue of $T$.
		
		\blni
		\cref{thm4.2.17(ii)} Note that this has already been proven in \eqref{eqn4.2.22}.
	\end{proof}
\end{thm}

\begin{exm}{}{exm4.2.17}
	Define $D\colon C^{\infty}(\R )\rightarrow C^{\infty}(\R )$ by $D(f)\ceqq f'$.  Then, $\lambda \in \C$ is an eigenvalue of $f$ iff $f$ is nonzero and $f'\eqqc D(f)=\lambda f$.  If this is true, it follows that $f(x)=C\exp (\lambda x)$ for some constant $C\in \C$, and hence that the function $x\mapsto \exp (\lambda x)$ is an eigenvector of $D$ with eigenvalue $\lambda$.  Thus, every $\lambda \in \C$ is an eigenvalue and
	\begin{equation}
		\Eig _{\lambda}=\Span (\exp (\lambda x)).
	\end{equation}
\end{exm}
\begin{exm}{}{}
	Let $T\colon \R ^2\rightarrow \R ^2$ be the linear-transformation that is counter-clockwise rotation about the origin by $\uppi /2$.  If $v\in \R ^2$ and $T(v)=\lambda v$ for some $\lambda \in \R$, then the rotate of $v$ would have to be parallel to $v$.  This is impossible unless $v=0$.  Thus, this linear-transformation has no eigenvalues.
\end{exm}
\begin{exm}{}{}
	Let $A$ be an upper-triangular matrix with entries in a field.  Then, the eigenvalues of $A$ are the scalars appearing on the diagonal of $A$.
	\begin{exr}[breakable=false]{}{}
		Check this.
	\end{exr}
\end{exm}

\begin{prp}{Eigenvectors with distinct eigenvalues are linearly-independent}{prp4.2.22}
	Let $V$ be a vector space, let $T\colon V\rightarrow V$ be linear, let $\lambda _1,\ldots ,\lambda _m$ be distinct eigenvalues of $T$, and let $v_k\in \Eig _{\lambda _k}$ be nonzero $1\leq k\leq m$.  Then, $\{ v_1,\ldots ,v_m\}$ is linearly-independent.
	\begin{rmk}
		When we investigate generalized-eigenvectors, we will encounter a strict generalization of this---see \cref{prp4.4.157}.
	\end{rmk}
	\begin{proof}
		Denote the ground division ring by $\F$.  We proceed by induction on $m$.  The $m=0$ case is immediate as eigenvectors are nonzero.  So, let $m\in \Z ^+$ and suppose the result is true $m-1$.  To prove the result for $m$, we proceed by contradiction:  suppose that $\{ v_1,\ldots ,v_m\}$ is linearly-dependent.  By \cref{prp1.2.28}, there is some $v_k$ such that
		\begin{equation}
			v_k\in \Span (v_1,\ldots ,v_{k-1}).
		\end{equation}
		Without loss of generality, we may assume that $k$ is the smallest such integer.  Thus, there are $\alpha _1,\ldots ,\alpha _{k-1}\in \F$ such that
		\begin{equation}\label{eqn4.2.25}
			v_k=\alpha _1\cdot v_1+\cdots +\alpha _{k-1}\cdot v_{k-1}.
		\end{equation}
		Applying $T$ to this equation yields
		\begin{equation}\label{eqn4.2.26}
			\lambda _kv_k=\alpha _1\lambda _1v_1+\cdots +\alpha _{k-1}\lambda _{k-1}v_{k-1}.
		\end{equation}
		Multiplying \eqref{eqn4.2.25} by $\lambda _k$ and subtracting the result from \eqref{eqn4.2.26} yields\footnote{Note that I am allowed to commute $\lambda _k$ past the $\alpha _i$s as $\lambda _k$ is central.}
		\begin{equation}
			0=\alpha _1(\lambda _1-\lambda _k)v_1+\cdots +\alpha _{k-1}(\lambda _{k-1}-\lambda _k).
		\end{equation}
		Now, by the induction hypothesis, it follows that $\alpha _i(\lambda _i-\lambda _k)=0$ for all $1\leq i\leq k-1$.  If some $\alpha _i\neq 0$, then it would follows that $\lambda _i=\lambda _k$:  a contradiction of the fact that the eigenvalues are distinct.  Thus, we must have that $\alpha _i=0$ for all $1\leq i\leq k-1$, whence it follows from \eqref{eqn4.2.25} that $v_k=0$, which itself is a contradiction (eigenvectors can't be $0$).  Thus, it must have been the case that $\{ v_1,\ldots ,v_m\}$ was linearly-independent, as desired.
	\end{proof}
\end{prp}
\begin{crl}{}{crl4.2.28}
	Let $V$ be a vector space, let $T\colon V\rightarrow V$ be linear, and let $\mcal{E}$ denote the set of eigenvalues of $T$.  Then, $\abs{\mcal{E}}\leq \dim (V)$.
	\begin{rmk}
		In words, the number of eigenvalues is at most the dimension.
	\end{rmk}
	\begin{proof}
		For every $\lambda \in \mcal{E}$, let $v_{\lambda}\in \Eig _{\lambda}$ be an eigenvector.  By the previous result, $\{ v_{\lambda}:\lambda \in \mcal{E}\}$ is linearly-independent, and so\footnote{By \cref{prp1.2.66}---this is the result that says linearly-independent sets have cardinality at most the cardinality of any spanning set.}
		\begin{equation}
			\abs{\mcal{E}}=\abs*{\{ v_{\lambda}:\lambda \in \mcal{E}\}}\leq \dim (V).
		\end{equation}
	\end{proof}
\end{crl}

Let $V$ be a vector space over a division ring $\F$ and let $T\colon V\rightarrow V$ be a linear-transformation.  In \cref{exm1.1.34}, we saw that we could make $V$ into an $\F [x]$-module, by having a polynomial $p$ act on $V$ by the $\F$-linear-transformation $p(T)$.  In the following result, we examine how such operators act on eigenvectors.
\begin{prp}{}{prp4.2.33}
	Let $V$ be a vector space over a division ring $\F$, let $T\colon V\rightarrow V$ be linear, let $p\in \F [x]$ be a polynomial, and let $\lambda \in \F$ be an eigenvalue of $T$ with eigenvector $v\in V$.  Then,
	\begin{equation}
		[p(T)](v)=p(\lambda )v.
	\end{equation}
	\begin{rmk}
		Indeed, note that you can view the equation $T(v)=\lambda v$ as the special case of this result for the polynomial $p(x)\ceqq x$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{crl}{}{crl4.2.36}
	Let $V$ be a vector space over a division ring $\F$, let $T\colon V\rightarrow V$ be linear, and let $p\in \F [x]$ be a polynomial.  Then, if $p(T)=0$, then $p(\lambda )=0$ for every eigenvalue $\lambda \in \F$ of $T$.
	\begin{rmk}
		This can have practical applications for actually \emph{computing} eigenvalues.  For example, if for whatever reason you know that $T^2-1=0$, then you know that $\lambda ^2-1=0$ for any eigenvalue $\lambda \in \F$ of $T$.  Thus, in this case, one could deduce that the only possible eigenvalues for $T$ were $\pm 1$.
	\end{rmk}
	\begin{proof}
		Suppose that $p(T)=0$.  Let $\lambda \in \F$ be an eigenvalue of $T$ with eigenvector $v\in V$.  By the previous result, we have that
		\begin{equation}
			0=[p(T)](v)=p(\lambda )v.
		\end{equation}
		As $v$ is nonzero, it follows that $p(\lambda )=0$.
	\end{proof}
\end{crl}
\begin{exm}{}{}
	For example, suppose that $3\in \C$ is an eigenvalue of $T$ with eigenvector $v\in V$, so that $T(v)=3v$.  If $p(x)\ceqq -5x^2+2x+1$, then we would have
	\begin{equation}
		[-5T^2+2T+\id ](v)\eqqc [p(T)](v)=p(3)v=[-5\cdot 9+2\cdot 3+1]v=-38v.
	\end{equation}
\end{exm}

\section{Diagonalization}

We started this chapter with the objective of finding a basis for which the coordinates of a given linear-transformation was a diagonal matrix.  We now have the tools to discuss this.
\begin{dfn}{Diagonalizable}{Diagonalizable}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, $T$ is \term{diagonalizable}\index{Diagonalizable} iff there is a basis $\basis{B}$ of $T$ such that $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ is a diagonal matrix.
	\begin{rmk}
		In this case, $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ is the \term{diagonalization}\index{Diagonalization} of $T$.
	\end{rmk}
	\begin{rmk}
		If $A$ is a matrix, we say that $A$ is \term{diagonalizable} iff the associated linear-transformation is diagonalizable.
	\end{rmk}
\end{dfn}
The whole point of discussing the ``eigenstuff'' was, as we saw in \crefnameref{sct4.1}, that the existence of eigenvalues was related to the diagonalizability of the linear-transformation.  We now make this precise.
\begin{thm}{Fundamental Theorem of Diagonalizability}{FundamentalTheoremOfDiagonalizability}
	Let $V$ be a finite-dimensional vector space, let $T\colon V\rightarrow V$ be linear, and let $\lambda _1,\ldots ,\lambda _m$ denote the eigenvalues of $T$.\footnote{Note that there are finitely many eigenvalues by \cref{crl4.2.28}.}  Then, the following are equivalent.
	\begin{enumerate}
		\item $T$ is diagonalizable.
		\item There is a basis of $V$ consisting of eigenvectors of $T$.
		\item
		\begin{equation}
			\dim (V)=\dim (\Eig _{\lambda _1})+\cdots +\dim (\Eig _{\lambda _m}).
		\end{equation}
	\end{enumerate}
	In this case, if $\basis{B}$ is a basis of eigenvectors of $V$ consisting of eigenvectors of $T$, then $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ is a diagonal matrix.
	\begin{rmk}
		There is another important condition equivalent to these three which we can't quite state yet as we don't know what direct-sums are.  In brief, it says that a linear operator is diagonalizable iff $V$ is the direct-sum of its eigenspaces---see the remark in \cref{prp4.4.13}.
	\end{rmk}
	\begin{rmk}
		In particular, as eigenspaces are of course at least one-dimensional, if $T$ has $\dim (V)$ distinct eigenvalues, then $T$ must be diagonalizable.  That said, the converse is not true---see \cref{exm4.3.4}.
	\end{rmk}
	\begin{rmk}
		Warning:  This name is nonstandard---there is no standard name for this result.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
\begin{exm}{}{}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & 6\end{bmatrix}.
	\end{equation}
	As $A$ is upper-triangular, the eigenvalues are the elements on the diagonal  $1$, $4$, and $6$.  Thus, as $A$ has $3$ distinct eigenvalues, it must be diagonalizable.
\end{exm}
Diagonalizability is yet another context where the ground division ring matters.
\begin{exm}{A matrix diagonalizable over $\C$ but not over $\R$}{exm4.3.7}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}.
	\end{equation}
	This matrix defines a linear-transformation $\R ^2\rightarrow \R ^2$ and also $\C ^2\rightarrow \C ^2$.  In either case, if $\lambda$ is an eigenvalue, then it must be the case that
	\begin{equation}
		\lambda ^2+1=0.
	\end{equation}
	\begin{exr}[breakable=false]{}{}
		Check this, that is, show that in both case $F=\R ,\C$, $\lambda \in \F$ is an eigenvalue iff $\lambda ^2+1=0$.
	\end{exr}

	Thus, if the ground field is $\R$, there are no eigenvalues.  On the other hand, if the ground field in $\C$, then there are two distinct eigenvalues, $\pm 1$.  Thus, according to \namerefpcref{FundamentalTheoremOfDiagonalizability}, this linear-transformation is diagonalizable over $\C$ but not over $\R$.
\end{exm}
The phenomenon we observed in the previous example is quite general, so it's worth it to take the time to point out its significance.
\begin{displayquote}
	As we will see later, it turns out that to every linear operator, there is an associated polynomial, the \emph{characteristic polynomial}, that has the property that a scalar is an eigenvalue iff it is a root of that polynomial. 
\end{displayquote}
In the previous example, the characteristic polynomial was $\lambda ^2+1$.  Thus, if the characteristic polynomial doesn't have any roots, the linear operator doesn't have any eigenvalues.  Thus, we observe the following.
\begin{displayquote}
	If order to guarantee that all linear operators have an eigenvalue, we're going to want to require all polynomials to have at least one root.
\end{displayquote}
The property that all polynomials have a root is an important one called \emph{algebraically closed}, and we will return to it later.  For now, however, we resume discussion of examples regarding diagonalizability.
\begin{exm}{A diagonalizable linear-transformation with fewer eigenvalues than the dimension}{exm4.3.4}
	The identity $\R ^2\rightarrow \R ^2$ has a single eigenvalue ($1$), but yet it is diagonalizable (with respect to the standard basis).
\end{exm}
\begin{exm}{A matrix over $\C$ that is not diagonalizable}{}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}0 & 1 \\ 0 & 0\end{bmatrix}.
	\end{equation}
	There is only one eigenvalue, $0$.  The eigenspace is the null-space of $A-0=A$, which is just
	\begin{equation}
		\Span \left( \begin{bmatrix}1 \\ 0\end{bmatrix}\right) .
	\end{equation}
	As the sum of the dimensions of the eigenspaces is only $1$, $A$ cannot be diagonalizable.
\end{exm}
\begin{exr}{}{}
	Define $V\ceqq \{ f\in C^{\infty}(\R ):f''+f=0\}$ and define $D\colon V\rightarrow V$ by $D(f)\ceqq f'$.  Show that $D$ is diagonalizable by exhibiting a basis of $V$ with respect to which the coordinates of $D$ is a diagonal matrix.
\end{exr}

\subsection{Diagonalization of matrix linear-transformations}

While the concept of diagonalization makes sense for any linear-transformation, there is an extra question one might ask if the linear-transformation happens to be defined by a matrix:  what is the relationship between the original matrix and its diagonalization?  The answer to this question is given by the following result.
\begin{prp}{}{prp4.3.16}
	Let $A$ be an $m\times m$ diagonalizable matrix with entries in a division ring $\F$ and let $\basis{B}$ be a basis of eigenvectors of $A$.  Then,
	\begin{equation}\label{eqn4.3.12}
		\coordinates{A}{\basis{B}\leftarrow \basis{B}}=\coordinates{\id}{\basis{B}\leftarrow \basis{S}}\coordinates{A}{\basis{S}\leftarrow \basis{S}}\coordinates{\id}{\basis{S}\leftarrow \basis{B}},
	\end{equation}
	where $\basis{S}$ is the standard basis of $\F ^m$.
	\begin{rmk}
		Note that $\coordinates{A}{\basis{S}\leftarrow \basis{S}}=A$ by \cref{prp3.2.100}.\footnote{I wrote \eqref{eqn4.3.12} using $\coordinates{A}{\basis{S}\leftarrow \basis{S}}$ instead of $A$ because I feel as if writing it this way makes it more `obviously' true.}
		
		Furthermore, $\coordinates{A}{\basis{B}\leftarrow \basis{B}}$ is the diagonalization of $A$ (by definition) and $\coordinates{\id}{\basis{B}\leftarrow \basis{S}}=\coordinates{\id}{\basis{S}\leftarrow \basis{B}}^{-1}$.  Thus, writing $D\ceqq \coordinates{A}{\basis{B}\leftarrow \basis{B}}$ and $P\ceqq \coordinates{\id}{\basis{S}\leftarrow \basis{B}}$, this equation is sometimes written more concisely (but perhaps less transparently) as
		\begin{equation}
			D=P^{-1}AP.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		Note that, by \cref{CoordinatesLinearTransformation}, the columns of $\coordinates{\id}{\basis{S}\leftarrow \basis{B}}$ are given by $\coordinates{b_k}{\basis{S}}$ for $b_k\in \basis{B}$.  However, by \cref{prp3.1.17},\footnote{This is the result that says the coordinates of a column vector with respect to the standard basis is just that original column vector.} this is just $b_k$ itself.  Thus,
		\begin{displayquote}
				\emph{$P\ceqq \coordinates{\id}{\basis{S}\leftarrow \basis{B}}$ is the matrix whose columns are the eigenvectors of $A$.}
		\end{displayquote}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result yourself.
		\end{exr}
	\end{proof}
\end{prp}

\section{Jordan Canonical Form}

Diagonalizable linear-transformations are great.  There's just one little itsy-bitsy problem:  not all linear-transformations are diagonalizable!  Not even in the context of finite-dimensional vector space over algebraically closed (\cref{AlgebraicallyClosed}) fields.  Fortunately, there is something that is almost as good, but \emph{always} exist (at least if the ground ring is sufficiently nice):  the \emph{Jordan canonical form} of a linear-transformation.

Before we get there however, we must first take a detour to discuss \emph{direct-sums} and \emph{invariant-subspaces}.  Very roughly speaking, the idea is to decompose a vector space into smaller pieces that are easier to study.  For example, if $b\in V$ is an eigenvector of $T$, then $\Span (b)$ the behavior of $T$ on this subspace is particularly easy to understand.  While we can't quite have the subspaces be that simple in general, we will find that we can still decompose our space into pieces which are not that much more complicated.

\subsection{Direct-sums}

The precise sense in which we are going to decompose vector spaces is called the \emph{direct-sum} and is defined as follows.
\begin{dfn}{Direct-sum}{DirectSum}
	Let $V$ be a $\K$-module, and let $\collection{W}$ be a collection of subspaces of $V$.  Then, $V$ is the \term{direct-sum}\index{Direct-sum} of the elements of $\collection{W}$ iff for every $v\in V$ there are unique $v^W\in W$ such that
	\begin{equation}\label{eqn4.4.2}
		v=\sum _{W\in \collection{W}}v^W.
	\end{equation}
	\begin{rmk}
		In this case, we write
		\begin{equation}
			V=\bigoplus _{W\in \collection{W}}W.
		\end{equation}\index[notation]{$\bigoplus _{W\in \collection{W}}W$}
	\end{rmk}
	\begin{rmk}
		We say that
		\begin{equation}
			V=\bigoplus _{W\in \collection{W}}W
		\end{equation}
		or just $\collection{W}$ is a \term{direct-sum decomposition}\index{Direct-sum decomposition} or \term{decomposition}\index{Decomposition} of $V$.
	\end{rmk}
	\begin{rmk}
		As per usual, the arbitrary (nonfinite) case obfuscates the simplicity of this concept.  If $\collection{W}=\{ W_1,\ldots ,W_m\}$, then this definition reads:  $V$ is the \term{direct-sum} of $W_1,\ldots ,W_m$ iff for every $v\in V$ there are unique $v^1\in W,\ldots ,v^m\in W_m$ such that
		\begin{equation}
			v=v^1+\cdots +v^m,
		\end{equation}
		in which case we write
		\begin{equation}
			V=W_1\oplus \cdots \oplus W_m.
		\end{equation}\index[notation]{$W_1\oplus \cdots \oplus W_m$}
	\end{rmk}
	\begin{rmk}
		Note how this definition is similar in ways to that of a basis---see \cref{dfn4.4.6,exr4.4.9}.
	\end{rmk}
\end{dfn}
We will see that a given direct-sum decomposition is in many ways similar to the specification of a basis.  Indeed, there is a sense in which it is a generalization---see \cref{exr4.4.9}.  The notions of \emph{linear-independence} and \emph{spanning} generalized to the context of subspaces as well, and, as with bases, these concepts will help us understand direct-sum decompositions.
\begin{thm}{Span (of subspaces)}{}
	Let $V$ be a $\K$-module and let $\collection{W}$ be a collection of subspaces of $V$.  Then, there is a unique subspace of $V$, the \term{span}\index{Span (of subspaces)} of $\collection{W}$, $\Span (\collection{W})$\index[notation]{$\Span (\collection{W})$}, such that
	\begin{enumerate}
		\item $W\subseteq \Span (\collection{W})$ for all $S\in \collection{W}$; and
		\item if $U\subseteq V$ is other subspace containing each $W\in \collection{W}$, it follows that $\Span (\collection{W})\subseteq U$.
	\end{enumerate}
	Furthermore, explicitly,
	\begin{equation}
		\Span (\collection{W})=\sum _{W\in \collection{W}}W.
	\end{equation}
	\begin{rmk}
		Thus, if $\collection{W}=\{ W_1,\ldots ,W_m\}$, we have that
		\begin{equation}
			\begin{split}
				\Span (W_1,\ldots ,W_m) & \ceqq \Span (\collection{W}) \\
				& =W_1+\cdots +W_m.
			\end{split}
		\end{equation}
	\end{rmk}
	\begin{rmk}
		It is incredibly uncommon to see people to write $\Span (W_1,\ldots ,W_m)$, and instead, people simply write the more explicit $W_1+\cdots +W_m$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
\begin{dfn}{Spanning (subspace)}{}
	Let $V$ be a $\K$-module and let $\collection{W}$ be a collection of subspaces of $V$.  Then, $\collection{W}$ is \term{spanning}\index{Spanning (subspaces)} iff $\Span (\collection{W})=V$.
	\begin{rmk}
		Synonymously, we also say that $\collection{W}$ \term{spans}\index{Spans (of subspaces)} $V$.
	\end{rmk}
\end{dfn}
\begin{dfn}{Linear-independence (of subspaces)}{}
	Let $V$ be a $\K$-module and let $\collection{W}$  be a collection of subspaces of $V$.  Then, $\collection{W}$ is \term{linearly-independent}\index{Linearly-independent (subspaces)} iff for all $m\in \Z ^+$, $W_1,\ldots ,W_m\in \collection{W}$, $w_k\in W_k$,
	\begin{equation}
		w_1+\cdots +w_m=0\text{ implies }w_1=0,\ldots ,w_m=0.
	\end{equation}
	$\collection{W}$ is \term{linearly-dependent}\index{Linearly-depenent (subspaces)} iff it is not linearly-independent.
\end{dfn}
Now let us see in what sense these concepts are generalizations of the ones we met before.
\begin{prp}{}{exr4.4.9}
	Let $V$ be a $\K$-module and let $\basis{B}\subseteq V$.
	\begin{enumerate}
		\item $\basis{B}$ is spanning iff $\{ \Span (b):b\in \basis{B}\}$ is spanning.
		\item $\basis{B}$ is linearly-independent iff $\{ \Span (b):b\in \basis{B}\}$ is linearly-independent.
		\item $\basis{B}$ is a basis iff
		\begin{equation}\label{eqn4.4.35}
			V=\bigoplus _{b\in \basis{B}}\Span (b).
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		In finite dimensions, with $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$, \eqref{eqn4.4.35} reads
		\begin{equation}\label{eqn4.4.23}
			V=\Span (b_1)\oplus \cdots \oplus \Span (b_d).
		\end{equation}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
This suggests the following criterion for checking whether a vector space can be written as a direct-sum.
\begin{prp}{Criterion for direct-sums of an arbitrary collection of subspaces}{prp4.4.7}
	Let $V$ be a $\K$-module, let $\collection{W}$ be a collection of subspaces of $V$.  Then,
	\begin{equation}
		V=\bigoplus _{W\in \collection{W}}W
	\end{equation}
	iff $\collection{W}$ is linearly-independent and spans $V$.
	\begin{rmk}
		Again, the case where $\collection{W}\ceqq \{ W_1,\ldots ,W_M\}$ is clearer.  It says:
		\begin{equation}
			V=W_1\oplus \cdots \oplus W_m
		\end{equation}
		iff (i) $V=W_1+\cdots +W_m$ and (ii) the equation $0=w_1+\cdots +w_m$ with $w_k\in W_k$ implies every $w_k=0$.
	\end{rmk}
	\begin{proof}
		$(\Rightarrow )$ Suppose that
		\begin{equation}
			V=\bigoplus _{W\in \collection{W}}W.
		\end{equation}
		By definition, this means that for every $v\in V$ there are unique $v^W\in W$ such that
		\begin{equation}
			v=\sum _{W\in \collection{W}}v^W.
		\end{equation}
		In particular, there are \emph{some} sum $v^W$s, and so $V=\sum _{W\in \collection{W}}W$.  Thus, $\collection{W}$ spans $V$.  For linear-independence, suppose that
		\begin{equation}
			0=\sum _{W\in \collection{W}}v^W.
		\end{equation}
		As we also have
		\begin{equation}
			0=\sum _{v\in \collection{W}}0,
		\end{equation}
		uniqueness implies that $v^W$, and so $\collection{W}$ is linearly-independent, as desired.
		
		\blni
		$(\Leftarrow )$ Suppose that $\collection{W}$ is linearly-independent and spans $V$.  Let $v\in V$.  As $\collection{W}$ spans $V$, there are $v^W\in W$ such that
		\begin{equation}
			v=\sum _{W\in \collection{W}}v^W.
		\end{equation}
		To show uniqueness, suppose that there are other $u^W\in W$ such that
		\begin{equation}
			v=\sum _{W\in \collection{W}}u^W.
		\end{equation}
		Subtracting these two expressions for $v$, we find
		\begin{equation}
			0=\sum _{W\in \collection{W}}[v^W-u^W],
		\end{equation}
		whence $v^W=u^W$ by linear-independence, giving us uniqueness, as desired.
	\end{proof}
\end{prp}
The case of two subspaces is particularly important, and in this case the criterion simplifies, and so we state it separately.
\begin{crl}{Criterion for direct-sums of two spaces}{crl4.4.11}
	Let $V$ be a $\K$-module, and let $U,W\subseteq V$ be subspaces.  Then,
	\begin{equation}
		V=U\oplus W
	\end{equation}
	iff (i) $V=U+W$ and (ii) $U\cap W=0$.
	\begin{rmk}
		Warning:  The naive generalization of this criterion to more than two subspaces is not true.  That is, if $W_1,\ldots ,W_m\subseteq V$ are subspaces such that $V=W_1+\cdots +W_m$ and $W_k\cap W_l=0$ for $k\neq l$, it is \emph{not} necessarily the case that $V=W_1\oplus \cdots \oplus W_m$---see \cref{exm4.4.13}.
	\end{rmk}
	\begin{proof}
		From the previous result, it suffices to show that $U\cap W=0$ is true iff it is true that the equation $0=u+w$ with $u\in U$ and $w\in W$ implies $u=0=w$.
		
		\blni
		$(\Rightarrow )$ Suppose that $U\cap W=0$.  Let $u\in U$, $w\in W$, and suppose that $0=u+w$.  Then, $w=-u\in U$, and so $w\in U\cap W$, and so $w=0$.  We then have $u=-w=0$.
		
		\blni
		$(\Leftarrow )$ Suppose that the equation $0=u+w$ with $u\in U$ and $w\in W$ implies $u=0=w$.  Let $v\in U\cap W$.  We have that $0=v+(-v)$, and as $v\in U$ and $-v\in W$, it follows in particular that $v=0$, and hence $U\cap W=0$.
	\end{proof}
\end{crl}
There are also more practical criterion specific to the finite-dimensional case.
\begin{prp}{}{prp4.4.13}
	Let $V$ be a finite-dimensional vector space and let $W_1,\ldots ,W_m\subseteq V$ be subspaces such that
	\begin{equation}
		\dim (V)=\dim (W_1)+\cdots +\dim (W_m).
	\end{equation}
	\begin{enumerate}
		\item If $\{ W_1,\ldots ,W_m\}$ is spanning, then $V=W_1\oplus \cdots \oplus W_m$.
		\item If $\{ W_1,\ldots ,W_m\}$ are linearly-independent, then $V=W_1\oplus \cdots \oplus W_m$.
	\end{enumerate}
	\begin{rmk}
		Thus, if the sum of the dimensions of the $W_k$s add up to that of $V$, then you only need to check one of the two usual conditions, instead of both.
	\end{rmk}
	\begin{rmk}
		Note that a corollary of this (together with \cref{FundamentalTheoremOfDiagonalizability}) is that a linear operator $V\rightarrow V$ is diagonalizable iff $V$ is the direct-sum of its eigenspaces:
		\begin{equation}
			V=\Eig _{\lambda _1}\oplus \cdots \oplus \Eig _{\lambda _m}.
		\end{equation}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exm}{$W_1,W_2,W_3\subseteq V$ subspaces with trivial pairwise intersections and $V=W_1+W_2+W_3$ but $V\neq W_1\oplus W_2\oplus W_3$}{exm4.4.13}
	Define $V\ceqq \R ^2$, $W_1\ceqq \Span (\coord{1,0})$, $W_2\ceqq \Span (\coord{0,1})$, and $W_3\ceqq \Span (\coord{1,1})$.  Then, $W_1\cap W_2=0$, $W_1\cap W_3=0$, $W_2\cap W_3=0$, $W_1+W_2+W_3=V$, but the sum is not direct:  we can write both
	\begin{equation}
		\coord{1,1}=0+0+\coord{1,1}
	\end{equation}
	and
	\begin{equation}
		\coord{1,1}=\coord{1,0}+\coord{0,1}+0.
	\end{equation}
\end{exm}
\begin{exm}{}{}
	Define
	\begin{equation}
	U=\Span \left( \begin{bmatrix}1 \\ 0 \\ 0\end{bmatrix},\begin{bmatrix}0 \\ 1 \\ 0\end{bmatrix}\right) \text{ and }W=\Span \left( \begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}\right) .
	\end{equation}
	Then, $\R ^3=U\oplus V$.
	\begin{exr}[breakable=false]{}{}
		Check this.
	\end{exr}
\end{exm}
\begin{exm}{}{}
	Define
	\begin{equation}
	V\ceqq \left\{ f\in \Mor _{\Set}(\R ^2,\R ):\lim _{\abs{x}\to \infty}f(x)\text{ exists.}\right\} .
	\end{equation}
	Then,
	\begin{equation}
	V=\left\{ f\in V:f\text{ is constant.}\right\} \oplus \left\{ f\in V:\lim _{\abs{x}\to \infty}f(x)=0\right\} .
	\end{equation}
	To see this, note that we may write
	\begin{equation}
	f=\lim _{\abs{x}\to \infty}f(x)+(f-\lim _{\abs{x}\to \infty}f(x))
	\end{equation}
	for any $f\in V$.
	\begin{exr}[breakable=false]{}{}
		Check this in more detail.
	\end{exr}
\end{exm}

In a way roughly analogous to how we can extend linearly-independent subsets to bases, we can `extend' subspaces into direct-sum decompositions, at least for vector spaces
\begin{prp}{Subspaces of vector spaces have complements}{prp4.4.24}
	Let $V$ be a vector space and let $U\subseteq V$ be a subspace.  Then, there is a subspace $W\subseteq V$ such that $V=U\oplus W$.
	\begin{rmk}
		If $V$ is a $\K$-module and $U\subseteq V$ is a subspace, then a subspace $W\subseteq V$ such that $V=U\oplus W$ is a \term{complement}\index{Complement} of $U$.
	\end{rmk}
	\begin{rmk}
		Warning:  Complements are not unique, even for vector spaces---see \cref{exm4.4.6}.
	\end{rmk}
	\begin{rmk}
		Warning:  This will fail in general for $\K$-modules---see \cref{exm4.5.15}.
	\end{rmk}
	\begin{proof}
		Let $\basis{A}$ be a basis of $U$ and extend it to a basis $\basis{B}$ of $V$.  Define $\basis{C}\ceqq \basis{B}\setminus \basis{A}$ and $W\ceqq \Span (\basis{C})$.  We certainly have
		\begin{equation}
			V=\Span (\basis{B})=\Span (\basis{A}\cup \basis{C})=\Span (\basis{A})+\Span (\basis{C})=U+W.
		\end{equation}
		On the other hand, an element in the intersection $V\cap W$ must be able to be written as a linear-combination of both elements of $\basis{A}$ and elements of $\basis{C}$.  This would then yield a nontrivial linear-dependence relation among elements of $\basis{A}\cup \basis{C}=\basis{B}$ unless the element in the intersection were $0$.  Thus, we must have $U\cap W=0$, and hence $V=U\oplus W$ by \cref{crl4.4.11}.
	\end{proof}
\end{prp}

Direct-sums allow us to define \emph{projections}.
\begin{prp}{Projection}{Projection}
	Let $V$ be a $\K$-module, and let $\collection{W}$ be a collection of subspaces of $V$ such that $V=\bigoplus _{W\in \collection{W}}W$.  Then, for every $W_0\in \collection{W}$, there is a unique linear-transformation $\proj _{W_0}\colon V\rightarrow W_0$\index[notation]{$\proj _{W_0}$}, the \term{projection}\index{Projection} onto $W$ with respect to the decomposition $V=\bigoplus _{W\in \collection{W}}W$, such that $\proj _{W_0}(v)=v^{W_0}$, where $v=\sum _{W\in \collection{W}}v^W$ for unique $v^W\in W$.
	\begin{rmk}
		For example, if $V=U\oplus W$, for $v\in V$, we can write $v=u+w$ for unique $u\in U$ and $w\in U$.  In this case, $\proj _U(v)=u$.  (Similarly, $\proj _W(v)=w$.)
	\end{rmk}
	\begin{rmk}
		Warning:  $\proj _{W_0}$ is \emph{not} uniquely determined by $W_0$ itself---it depends on the other $W$s, on the entire decomposition---see \cref{exm4.4.6} for a concrete example of this.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result yourself.
		\end{exr}
	\end{proof} 
\end{prp}
\begin{exm}{$V=U\oplus W_1=U\oplus W_2$ for $W_1\neq W_2$}{exm4.4.6}
	Define $V\ceqq \R ^2$, $U\ceqq \Span (\coord{1,0})$, $W_1\ceqq \Span (\coord{0,1})$, and $W_2\ceqq \Span (\coord{1,1})$.
	\begin{exr}[breakable=false]{}{}
		Check that $V=U\oplus W_1$ and $V=U\oplus W_2$.
	\end{exr}

	Define $v\ceqq \coord{a,b}\in \R ^2$.  The decomposition of $v$ with respect to the decomposition $V=U\oplus W_1$ is given by
	\begin{equation}
		v\ceqq \begin{bmatrix}a \\ b\end{bmatrix}=\begin{bmatrix}a \\ 0\end{bmatrix}+\begin{bmatrix}0 \\ b\end{bmatrix}.
	\end{equation}
	On the other hand, the decomposition of $v$ with respect to the decomposition $V=U\oplus W_2$ is given by
	\begin{equation}
		v\ceqq \begin{bmatrix}a \\ b\end{bmatrix}=\begin{bmatrix}a-b \\ 0\end{bmatrix}+\begin{bmatrix}b \\ b\end{bmatrix}
	\end{equation}
	Hence,
	\begin{equation}
		\proj _U(w)=\begin{bmatrix}a \\ 0\end{bmatrix}
	\end{equation}
	with respect to the first decomposition, but
	\begin{equation}
		\proj _U(w)=\begin{bmatrix}a-b \\ 0\end{bmatrix}
	\end{equation}
	with respect to the second decomposition.
\end{exm}

\subsubsection{Coordinates with respect to direct-sum decompositions}

We saw in \cref{CoordinatesVector} that, given a basis of a $\K$-module $V$, we can define the \emph{coordinates} of elements of $v$ with respect to that basis.  In a similar way, we can also talk about the coordinates of elements of $V$ with respect to a direct-sum decomposition.   In fact, the definition of coordinates we learned before is a special case of this---see \cref{exr4.4.9}.

We saw before that the coordinates of an `abstract' vector was an element of $\K ^d$, $\K$ the ground ring.  Given a direct-sum decomposition $V=W_1\oplus \cdots \oplus W_m$, the first question we must address is ``What plays the role of $\K ^d$ in this context?''.  To see what the answer should be, recall (\cref{exr4.4.9}) that, given a basis $\basis{B}$ of $V$, $V$ can be written as the direct-sum of the subspaces $\{ \Span (b):b\in \basis{B}\}$.

Over a division ring $\F$, $\Span (b)\cong _{\Vect _{\F}}\F$ via the map $\F \ni \alpha \mapsto \alpha \cdot b\in \Span (b)$.  Thus, if $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ is a basis for the vector space $V$, after identifying each $\Span (b_k)$ with $\F$, \eqref{eqn4.4.23} reads
\begin{equation}
	V\cong \underbrace{\F \oplus \cdots \oplus \F}_d.
\end{equation}
On the other hand, we know from \cref{prp3.1.7} that the map that sends an ``abstract'' vector to its coordinates yields an isomorphism
\begin{equation}
	V\cong \F ^d\ceqq \underbrace{\F \times \cdots \times \F}_d.
\end{equation}
These two facts together suggest that in the general case $V=W_1\oplus \cdots \oplus W_d$, $W_1\times \cdots \times W_d$ should play the role that $\F ^d$ did with coordinates before.  That is, the map that sends an ``abstract'' vector to its coordinates should be a map $V\rightarrow W_1\times \cdots \times W_d$ (and with any luck, it will be an isomorphism).  This is exactly how we're doing to define things.  But first, we want to give $W_1\times \cdots \times W_d$ the structure of a vector space.
\begin{prp}{Product of modules}{}
	Let $\collection{V}$ be an indexed collection of $\K$-modules, and define $\prod _{V\in \collection{V}}\times \prod _{V\in \collection{V}}\rightarrow \prod _{V\in \collection{V}}$ and $\K \times \prod _{V\in \collection{V}}V\rightarrow \prod _{V\in \collection{V}}V$ respectively by
	\begin{subequations}
		\begin{align}
			[v_1+v_2]^V & \ceqq v_1^V+v_2^V \\
			[\alpha \cdot v]^V & \ceqq \alpha \cdot v^V.
		\end{align}
	\end{subequations}
	Then, $\prod _{V\in \collection{V}}V$ is a $\K$-module, the \term{product}\index{Product (of $\K$-modules)}, with this addition and scaling.
	\begin{rmk}
		$v\in \prod _{V\in \collection{V}}V$, that is, it is a function $\collection{V}\rightarrow \bigsqcup _{V\in \collection{V}}V$ whose value at $V\in \collection{V}$ is an element of $V$---see \cref{CartesianProductCollection}.  $\alpha \cdot v$ is a new function, one we are defining, and $[\alpha \cdot v]^V$ is its value at $v\in \collection{V}$.  Similarly for the definition of addition.
	\end{rmk}
	\begin{rmk}
		In words, addition and scaling are defined \emph{componentwise}.
	\end{rmk}
	\begin{rmk}
		In the finite case $\collection{V}\eqqc \{ V_1,\ldots ,V_m\}$, these definitions (in more suggestive notation) look like
		\begin{subequations}
			\begin{align}
				\coord{v_1,\ldots ,v_m}+\coord{w_1,\ldots ,w_m} & \ceqq \coord{v_1+w_1,\ldots ,v_m+w_m} \\
				\alpha \cdot \coord{v_1,\ldots ,v_m} & \ceqq \coord{\alpha \cdot v_1,\ldots ,\alpha \cdot v_m}.
			\end{align}
		\end{subequations}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  The proof is as easy as verifying the axioms of a $\K$-module---see \cref{RModule}.
			\end{rmk}
		\end{exr}
	\end{proof} 
\end{prp}

We are now ready define coordinates with respect to a direct-sum decomposition.
\begin{dfn}{Coordinates (of a vector with respect to a decomposition)}{dfn4.4.6}
	Let $V$ be a $\K$-module, let $V=\bigoplus _{W\in \collection{W}}W$ be a direct-sum decomposition of $V$, let $v\in V$, and write
	\begin{equation}
	v=\sum _{W\in \collection{W}}v^W
	\end{equation}
	for unique $v^W\in \collection{W}$.  Then, the \term{coordinates}\index{Coordinates (of a vector with respect to a decomposition)} of $v$ with respect to the decomposition $\collection{W}$, $\coordinates{v}{\collection{W}}$\index[notation]{$\coordinates{v}{\collection{W}}$}, is defined by
	\begin{equation}
	\coordinates{v}{\collection{W}}\ceqq \langle v^W:W\in \collection{W}\rangle \in \prod _{W\in \collection{W}}W.
	\end{equation}
	\begin{rmk}
		If $\collection{W}\eqqc \{ W_1,\ldots ,W_m\}$ is finite, where are unique $v^1\in W^1,\ldots ,v^m\in W^m$ such that
		\begin{equation}
		v=v^1+\cdots +v^m.
		\end{equation}
		Then, the coordinates of $v$ are given by the column vector
		\begin{equation}
		\coordinates{v}{\collection{W}}\ceqq \begin{bmatrix}v^1 \\ \vdots \\ v^m\end{bmatrix}\in \prod _{k=1}^mW_k.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		Note that this generalizes coordinates with respect to a basis (\cref{CoordinatesVector}), at least for vector spaces.  Indeed, if $\basis{B}$ is a basis for $V$, then we obtain a corresponding direct-sum decomposition
		\begin{equation}
		V=\bigoplus _{b\in \basis{B}}\Span (b)
		\end{equation}
		by the previous result.  If the ground ring $\F$ is a division ring, then $\Span (b)$ is isomorphic to the one-dimensional vector space $\F$, and after this identification, we can view the component $v^b\in \Span (B)$ in the sense of this definition as a scalar, which gives us $\coordinates{v}{\basis{B}}$.
	\end{rmk}
\end{dfn}
As before, this is an isomorphism if the direct-sum decomposition is finite.
\begin{prp}{$\coordinates{\blankdot}{\collection{W}}$ is linear and injective (and surjective in finite dimensions)}{}
	Let $V$ be a $\K$-module, and let $V=\bigoplus _{W\in \collection{W}}W$ be a direct-sum decomposition of $V$.  Then,
	\begin{equation}
		V\ni v\mapsto \coordinates{v}{\collection{W}}\in \prod _{W\in \collection{W}}W
	\end{equation}
	is linear and injective with image
	\begin{equation}
		\left\{ \coord{v^W:W\in \collection{W}}:v^W=0\text{ for cofinitely many }W\in \collection{W}\text{.}\right\} .
	\end{equation}
	\begin{rmk}
		In particular, if $\collection{W}\eqqc \{ W^1,\ldots ,W^m\}$ is finite this gives an isomorphism $V\rightarrow W^1\times \cdots \times W^m$.  In particular,
		\begin{equation}
			W^1\oplus \cdots \oplus W^m\cong W^1\times \cdots \times W^m.
		\end{equation}
		For this reason, people are not always careful to distinguish between the two, and quite often people will write $V\oplus W$ when technically they mean $V\times W$.  Indeed, I have even heard $V\times W$ called the ``external direct sum'' (in which case they referred to what we have been calling ``direct sum'' as ``internal direct sum'').
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove this yourself.
		\end{exr}
	\end{proof}
\end{prp}

As you might now expect, we obtain an analogous notion of coordinates of linear-transformations with respect to (finite) direct-sum decompositions.  As with coordinates of vectors, our first order of business is to determine what the analogue of matrices is in this context (just as we had to determine what the analogue of $\K ^d$ should be).  We will see that the answer is still essentially ``matrices'', but now matrices whose entries themselves are linear-transformations (similar to how one might think of an element of $W_1\times \cdots \times W_m$ as a ``column vector'' whose entries are themselves vectors).
\begin{dfn}{Matrix (of linear-transformations)}{}
	Let $m$ and $n$ be collections of $\K$-modules.  Then, an \term{$m\times n$ matrix}\index{Matrix (of linear-transformations)} is a function
	\begin{equation}
		m\times n\ni \coord{i,j}\mapsto A\indices{^i_j}\in \bigsqcup _{\substack{W\in m \\ V\in n}}\Mor _{\Mod{\K}}(V,W)
	\end{equation}
	such that $A\indices{^i_j}\in \Mor _{\Mod{\K}}(i,j)$.
	\begin{rmk}
		We make use of all of the suggestive notation and language as we did with matrices (of scalars) before.  One thing to note, however, is that in the equation defining matrix multiplication (\eqref{eqn1.1.47}), upon generalizing to this context, the multiplication denoted by juxtaposition should be interpreted there as composition in \emph{postfix notation}, that is, $B\indices{^k_j}A\indices{^i_k}\ceqq A\indices{^i_k}\circ B\indices{^k_j}$ (otherwise the composition wouldn't make sense in general).
	\end{rmk}
	\begin{rmk}
		We write $\Matrix _{m\times n}$\index[notation]{$\Matrix _{m\times n}$} for the set of all $m\times n$ matrices.  Note that we need not specify the ground ring as we did before (e.g.~as $\Matrix _{m\times n}(\K )$) because this is implicitly contained in the data given in $m$ and $n$---they are collections of \emph{$\K$}-modules.  In case $m=n$, we may write $\Matrix _m\ceqq \Matrix _{m\times m}$\index[notation]{$\Matrix _m$}.  As before, $\Matrix _{m\times n}$ has the structure of a (commutative) group always, it will additionally have the structure of a $\K$-module if $\K$ is commutative, and $\Matrix _m$ has the structure of a ring.
	\end{rmk}
	\begin{rmk}
		If the two collection of $\K$-modules are finite, say $\{ W^1,\ldots ,W^m\}$ and $\{ V^1,\ldots ,V^n\}$, then a $\{ W^1,\ldots ,W^m\} \times \{ V^1,\ldots ,V^n\}$ matrix should be thought of as a double-indexed array $A\indices{^i_j}$, $1\leq i\leq m$ and $1\leq j\leq n$, where $A\indices{^i_j}\colon V^j\rightarrow W^i$ is a linear-transformation from $V^j$ to $W^i$.  As before, this will be written
		\begin{equation}
			A=\begin{bmatrix}A\indices{^1_1} & \cdots & A\indices{^1_n} \\ \vdots & \ddots & \vdots \\ A\indices{^m_1} & \dots & A\indices{^m_n}\end{bmatrix}.
		\end{equation}
		We will see later that this will define a linear-transformation $V^1\times \cdots \times V^n\rightarrow W^1\times \cdots \times W^m$.  Indeed, that is somehow the point of matrices of linear-transformations---they make it easier to think about linear-transformations between products.
	\end{rmk}
	\begin{rmk}
		If $A\indices{^i_j}=0$ for $i\neq j$, we write
		\begin{equation}
		A\indices{^1_1}\oplus \cdots \oplus A\indices{^m_m}\ceqq \begin{bmatrix}A\indices{^1_1} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \dots & A\indices{^m_n}\end{bmatrix}.
		\end{equation}\index[notation]{$A\indices{^1_1}\oplus \cdots \oplus A\indices{^m_m}$}
		This matrix of linear-transformations is the \term{direct-sum}\index{Direct-sum (of linear-transformations)}.  Matrices of this form are referred to as \term{Block-diagonal}\index{Block-diagonal matrix}.\footnote{It is of course just a diagonal matrix in our old terminology, but in this context it is far more common to say ``block-diagonal'' to emphasize that the entries on the diagonal are not scalars but linear-transformations.}
	\end{rmk}
\end{dfn}
Before moving on, we introduce a convention that will prove quite convenient.

\paragraph{Identifying matrices with the linear-transformations they define}

Consider a linear-transformation $T\colon V\rightarrow W$.  In case $V=\K ^n$ and $W=\K ^m$, $V$ and $W$ have canonical bases (the standard bases), and so, in a sense, vectors and linear-transformations really are `the same as' column-vectors and matrices respectively.  Ordinarily this identification is arbitrary because it depends on a choice of basis, but not so in this case.  Thus, one can be sloppy and fail to make a distinction between a matrix and the linear-transformation it defines.  Previously, we didn't do this because (i) pedagogy and (ii) it wasn't really necessary.  Of course, it's still not \emph{strictly} necessary, but now it will prove to rather useful.

For example, it is quite cumbersome to say ``Consider the linear-transformation $\C ^3\rightarrow \C ^3$ defined by the follow matrix of linear-transformations
\begin{equation}
	\begin{bmatrix}T_A & 0 \\ 0 & T_B\end{bmatrix},
\end{equation}
where $T_A\colon \C \rightarrow \C$ is the linear-transformation defined by the matrix $A\ceqq \begin{bmatrix}5\end{bmatrix}$ and $T_B\colon \C ^2\rightarrow \C ^2$ is the linear-transformation defined by the matrix
\begin{equation}
	B\ceqq \begin{bmatrix}-3 & 1 \\ 0 & -3\end{bmatrix}.\text{''},
\end{equation}
especially when you consider that I could be sloppy and alternatively say ``Consider the following linear-transformation $\C ^3\rightarrow \C ^3$
\begin{equation}
	\begin{bmatrix}5 & 0 & 0 \\ 0 & -3 & 1 \\ 0 & 0 & -3\end{bmatrix}.\text{''}.
\end{equation}

Thus, hereafter, we do not guarantee to be careful about making distinctions between matrices and the linear-transformations they define---we trust that by this point in the notes the conceptual difference between the two is firmly cemented in your mind---though we will still make the distinction if it's convenient.  Related to this is that you should now consider definitions made for linear-transformations as also being `officially' made for matrices.  For example, we defined above the term ``block-diagonal matrix of linear-transformations''.  A \term{block-diagonal matrix} (of scalars) is one such that the matrix of linear-transformations with respect to the given direct-sum decomposition of the linear-transformation defined by the matrix is block-diagonal.  For example,
\begin{equation}
	\begin{bmatrix}1 & 2 & 3 & 0 & 0 \\ 4 & 5 & 7 & 0 & 0 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1\end{bmatrix}
\end{equation}
is a block-diagonal matrix of scalars, whereas
\begin{equation}
	\begin{bmatrix}1 & 2 & 3 & 0 & -13 \\ 4 & 5 & 7 & 0 & 0 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1\end{bmatrix}
\end{equation}
is not.  Similarly, we may speak of direct-sums of matrices and so on.
\begin{exm}{Direct-sum of matrices}{}
	For example,
	\begin{equation}
		\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix}\oplus \begin{bmatrix}-1 & 1 & -2 & 2 \\ -3 & 3 & -4 & 4 \\ 1 & 2 & -5 & -6\end{bmatrix}\coloneqq \begin{bmatrix}1 & 2 & 3 & 0 & 0 & 0 & 0 \\ 4 & 5 & 6 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & -1 & 1 & -2 & 2 \\ 0 & 0 & 0 & -3 & 3 & -4 & 4 \\ 0 & 0 & 0 & 1 & 2 & -5 & -6\end{bmatrix}
	\end{equation}
\end{exm}

\horizontalrule

We now turn to defining the coordinates of a linear-transformation with respect to direct-sum decompositions.  Our ability to do this will follow from what are in their own right important properties of direct-sum decompositions.  The first of these properties is that direct-sum decompositions of a vector space allow one to define a linear-transformation by defining what it does on all the ``summands''.  The second of these properties is the dual property involving mapping \emph{into} summands.  We state both results here.
\begin{thm}{Finite direct-sums are biproducts}{thm4.4.14}
	Let $U$ and $V$ be $\K$-modules, and let $V_1,\ldots ,V_m\subseteq V$ be subspaces such that $V=V_1\oplus \cdots \oplus V_m$.
	\begin{enumerate}
		\item \label{thm4.4.14(i)}Let $S_k\colon U\rightarrow V_k$ be linear.  Then, there is a unique linear-transformation $S\colon U\rightarrow V$ such that $S_k=\proj _{V_k}\circ S$ for $1\leq k\leq m$.
		\item \label{thm4.4.14(ii)}Let $T_k\colon V_k\rightarrow U$ be linear.  Then, there is a unique linear-transformation $T\colon V\rightarrow U$ such that $\restr{T}{V_k}=T_k$ for $1\leq k\leq m$.
	\end{enumerate}
	\begin{rmk}
		More explicitly, the first implies that
		\begin{equation}
			S(u)=S_1(u)+\cdots +S_m(u)
		\end{equation}
		for all $u\in U$.  Likewise, the second implies that
		\begin{equation}
			T(v_1+\cdots +v_m)=T_1(v_1)+\cdots +T_m(v_m)
		\end{equation}
		for $v_k\in V_k$.
	\end{rmk}
	\begin{rmk}
		You should draw an analogy between the property stated in \cref{thm4.4.14(i)} and the corresponding property for \emph{Cartesian products} of sets, namely, that you can define a function $Z\rightarrow X\times Y$ by specifying its ``components'' $Z\rightarrow X$ and $Z\rightarrow Y$.  You will learn when you study category theory in more depth that these both serve examples of \emph{limits}, in fact a special type of limit called a \emph{product} (one in the category of $\K$-modules and the other in the category of sets).
	\end{rmk}
	\begin{rmk}
		Dually, you should draw an analogy between the property stated in \cref{thm4.4.14(ii)} and the corresponding property for \emph{disjoint-unions} of sets, namely, that you can define a function $X\coprod Y\rightarrow Z$ by specifying its ``restrictions'' $X\rightarrow Z$ and $Y\rightarrow Z$.  You will learn when you study category theory in more depth that these both serve examples of \emph{colimits}, in fact a special type of colimit called a \emph{coproduct} (one in the category of $\K$-modules and the other in the category of sets).
	\end{rmk}
	\begin{rmk}
		The term \emph{biproduct} comes from the fact that $W_1\oplus \cdots \oplus W_m$ is both a product and a coproduct (though the definition is not quite that simple).
	\end{rmk}
	\begin{rmk}
		Warning:  While \cref{thm4.4.14(ii)} generalizes to arbitrary direct-sums, \cref{thm4.4.14(i)} \emph{fails} in this case---this is why we stuck to the case of finitely-many subspaces in this result.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			We leave this as an exercise.
		\end{exr}
	\end{proof}
\end{thm}
Combining these two properties, with both the domain and the codomain as a direct-sum, we obtain the following.
\begin{thm}{Coordinates of a linear-transformation (with respect to a decomposition)}{}
	Let $V$ and $W$ be $\K$-modules, let $\collection{V}\eqqc \{V_1,\ldots ,V_n\}$ and $\collection{W}\eqqc \{ W_1,\ldots ,W_m\}$ be collections of subspaces of $V$ and $W$ respectively such that $V=V_1\oplus \cdots \oplus V_n$ and $W=W_1\oplus \cdots \oplus W_m$, and let $T\colon V\rightarrow W$ be a linear-transformation.  Then, there is a unique $m\times n$ matrix, $\coordinates{T}{\collection{W}\leftarrow \collection{V}}$, the \term{coordinates}\index{Coordinates (of a linear-transformation with respect to a decomposition)} with respect to the decompositions $\collection{V}$ and $\collection{W}$, such that
	\begin{equation}
		\coordinates{T(v)}{\collection{W}}=\coordinates{T}{\collection{W}\leftarrow \collection{V}}\coordinates{v}{\collection{V}}
	\end{equation}
	for all $v\in V$.
	
	Furthermore, explicitly
	\begin{equation}
		\coordinates{T}{\collection{W}\leftarrow \collection{V}}=\begin{bmatrix}T\indices{^1_1} & \cdots & T\indices{^1_n} \\ \vdots & \ddots & \vdots \\ T\indices{^m_1} & \dots & T\indices{^m_n}\end{bmatrix},
	\end{equation}
	where we have defined
	\begin{equation}
		\proj _{W_i}\circ \restr{T}{V_j}\eqqc T\indices{^i_j}\ceqq \restr{\proj _{W_i}\circ T}{V_j}.
	\end{equation}
	\begin{rmk}
		Perhaps the most important thing for you to remember about this is an expression for $T$ in terms of the $T\indices{^i_j}$s:
		
		For $v\in V$, write $v=v^1+\cdots +v^n$ for unique $v^k\in V^k$.  Then,
		\begin{equation}
			T(v)=\left( \sum _{j=1}^nT\indices{^1_j}(v^j)\right) +\cdots +\left( \sum _{j=1}^nT\indices{^m_j}(v^j)\right) .
		\end{equation}
		That is, the $i^{\text{th}}$ coordinate of $T(v)$ with respect to the decomposition $W=W^1\oplus \cdots \oplus W^m$ is given by
		\begin{equation}
			T(v)^i=\sum _{j=1}^nT\indices{^i_j}(v^j).
		\end{equation}
	\end{rmk}
	\begin{rmk}
		To be clear, $\coordinates{v}{\collection{V}}\in V^1\times \cdots \times V^n$, $\coordinates{T(v)}{\collection{W}}\in W^1\times \cdots \times W^m$, and $\coordinates{T}{\collection{W}\leftarrow \collection{V}}\colon V^1\times \cdots \times V^n\rightarrow W^1\times \cdots \times W^m$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
As before, this yields an isomorphism from the space of linear-transformations to the space of matrices (of linear-transformations).
\begin{prp}{$\coordinates{\blankdot}{\basis{W}\leftarrow \basis{V}}$ is an isomorphism}{}
	Let $\K$ be a cring, let $V$ and $W$ be $\K$-modules, and let $V=V^1\oplus \cdots \oplus V^n$ and $W=W^m\oplus \cdots \oplus W^m$ be direct-sum decompositions of $V$ and $W$ respectively.  Then,
	\begin{equation}
		\Mor _{\Mod{\K}}(V,W)\ni T\mapsto \coordinates{T}{\collection{W}\leftarrow \collection{V}}\in \Matrix _{\collection{W}\times \collection{V}}
	\end{equation}
	is an isomorphism of $\K$-modules, where $\collection{V}\ceqq \{ V^1,\ldots ,V^n\}$ and $\collection{W}\ceqq \{ W^1,\ldots ,W^m\}$.
	\begin{rmk}
		In particular, we have that
		\begin{equation}
			\coordinates{T_1+T_2}{\collection{W}\leftarrow \collection{V}}=\coordinates{T_1}{\collection{W}\leftarrow \collection{V}}+\coordinates{T_2}{\collection{W}\leftarrow \collection{V}}
		\end{equation}
		and
		\begin{equation}
			\coordinates{\alpha \cdot T}{\collection{W}\leftarrow \collection{V}}=\alpha \cdot \coordinates{T}{\collection{W}\leftarrow \collection{V}}.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		If $\K$ weren't commutative, this instead would only be an isomorphism of groups.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

\subsection{Invariant subspaces}

At the beginning of the previous subsection, we explained that we sought to decompose $V$ into simpler pieces, with the hopes that this would enable us to associate a relatively simple matrix to a linear-transformation.  In this subsection, we investigate the \emph{types} of subspaces we would like to decompose $V$ into in order to achieve this:  \emph{invariant subspaces}.\footnote{In an ideal world, we could choose these smaller pieces to not just be $T$-invariant but in fact be eigenspaces, but of course, if we could do that, everything would be diagonalizable, and we wouldn't be discussing this in the first place.}  If we can decompose $V$ as a direct-sum of smaller $T$-invariant subspaces, then the form that the matrix of $T$ takes will simplify into a \emph{block-diagonal} matrix---see \cref{thm4.5.2}.

Before turning to the the definition of invariant subspace itself, we first investigate a new example of an $R$-module that will prove moderately useful in the things to come.
\begin{exm}{The $\K [x]$-module defined by a linear-trans-\\formation}{exm1.1.34}
	Let $\K$ be a ring.  Before we get to the module itself, note that, while we have previously only thought of $\K [x]$ as a $\K$-module, it also has the canonical structure of a ring:  the addition is the same and the multiplication now is just `ordinary' multiplication of polynomials.\footnote{This works just like you think it would, with the caveat that the coefficients are assumed to \emph{commute} with $x$, even if $\K$ is noncommutative.  For example, $(\alpha x)\cdot (\beta +x^2)=\alpha \beta x+\alpha x^2$.  This assumption is a reasonable one because, as you will see shortly, $x$ is going to act as if it were a linear-transformation, and linear-transformations commute with all scalars, no matter how noncommutative $\K$ might be.}  This is exactly analogous to how we can consider $\R$ to be both a vector space and a ring.
	
	Having noted that $\K [x]$ has the structure of a ring (so that ``$\K [x]$-module'' actually makes sense), let us turn to examining the $\K [x]$-module itself.  To define this $\K [x]$-module, we must first start with a $\K$-module $V$ and a $\K$-linear operator $T\colon V\rightarrow V$.
	
	We now define the $\K [x]$-module as follows:  The underlying set of vectors is the same as before, $V$; the addition is the same as it was before; but now scaling $\K [x]\times V\rightarrow V$ is defined by
	\begin{equation}
	(\alpha _0+\alpha _1x+\cdots +\alpha _mx^m)\cdot v\ceqq \alpha _0\cdot v+\alpha _1\cdot T(v)+\cdots +\alpha _m\cdot T^m(v).
	\end{equation}
	Intuitively, we require that $x\cdot v\ceqq T(v)$, and then the ``action'' of every other polynomial is determined in the obvious way (e.g.~$x^2\cdot v\ceqq T^2(v)$).
	\begin{exr}[breakable=false]{}{}
		Check that this actually satisfies the axioms of a $\K [x]$-module.
	\end{exr}
	\begin{rmk}
		If ever $V$ is a $\K$-module and $T$ is a $\K$-linear operator on $V$, if we say ``$V$ is a $\K [x]$-module'', it should be understood that the $\K [x]$-module structure we are referring to is the one defined in this example.  While I don't want to make the notation `official',\footnote{Because the notation is not unique enough to make it effectively unambiguous.} if I want to regard $V$ as a $\K [x]$-module instead of a $\K$-module, I will instead write $V_T$.  Of course, the set of ``vectors'' itself hasn't change, but this notation indicates to you that I am thinking of the same set of vectors \emph{with different ``scalars''}.
	\end{rmk}
\end{exm}
Regarding $V$ as a $\K [x]$-module in this way is just a fancy way of saying that we have defined what $p(T)$ means, $T\colon V\rightarrow V$ a $\K$-linear operator and $p$ a polynomial.  For example, if $p(x)\ceqq 3x^3-5x^2+2$, then $p(T)\colon V\rightarrow V$ is the $\K$-linear transformation
\begin{equation}
v\mapsto [p(T)](v)\ceqq [3T^3-5T^2+2](v)\ceqq 3T(T(T(v)))-5T(T(v))+2v.
\end{equation}
After having discussed eigenvalues, we can say a little more about these operators---see \cref{prp4.2.33}.

This example allows us to make the following definition.
\begin{dfn}{Invariant subspace}{InvariantSubspace}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be a $\K$-linear operator, and let $W\subseteq V$ be a subspace.  Then, $W$ is \term{$T$-invariant}\index{Invariant subspace} iff $W$ is a $\K [x]$-submodule of $V$.
	\begin{rmk}
		If $T$ is clear from context, we may simply say \term{invariant}.
	\end{rmk}
	\begin{rmk}
		This is succinct but obtuse---see the following proposition for a more down-to-earth characterization of what this means.
	\end{rmk}
	\begin{rmk}
		Recall that (\cref{Subspace}) ``submodule'' is synonymous with ``subspace''.  We use the term ``submodule'' here to emphasize the distinction between thinking of $V$ as a $\K$-module versus a $\K [x]$-module.
	\end{rmk}
	\begin{rmk}
		Note that subspaces which satisfy \cref{Eigen}\cref{Eigen(i)} are automatically $T$-invariant.  In particular, eigenspaces are invariant subspaces.
	\end{rmk}
	\begin{rmk}
		Strictly speaking, we didn't really \emph{need} the $\K [x]$-module defined in \cref{exm1.1.34} to state this definition, and it is admittedly easier to understand characterization in the following proposition.  Nonetheless, I decided to present it this was (i) to give you practice thinking about nonartificial examples of $R$-modules that are not vector spaces, (ii) I personally find it more elegant, and (iii) it is more systematic in the sense that the term ``invariant subspace'' is used in more general contexts where it can analogously be defined as a particular submodule.
	\end{rmk}
\end{dfn}
\begin{prp}{}{}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be a $\K$-linear operator, and let $W\subseteq V$ be a subspace.  Then, $W$ is $T$-invariant iff $T(w)\in W$ for all $w\in W$.
	\begin{rmk}
		This makes it clear one reason why invariant subspaces might be useful:  if $W$ is invariant, then $\restr{T}{W}\colon W\rightarrow W$, that is, $T$ can be considered as a linear operator on $W$.
	\end{rmk}
	\begin{rmk}
		Of course, said another way, this is the same as the statement $T(W)\subseteq W$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove this yourself.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exm}{}{}
	Define $D\colon \R [x]\rightarrow \R [x]$ by $D(p)\ceqq p'$.  Then, $\R [x]_m\subseteq \R [x]$ is a $D$-invariant subspace for all $m\in \N$.\footnote{Recall (\cref{Polynomials}) that $\R [x]_m$ is the subspace of all polynomials of degree at most $m$.}
\end{exm}

One fact we will find useful is that projections onto invariant subspaces are ``compatible'' with the linear operator.
\begin{prp}{}{prp4.5.9}
	Let $V$ be a $\K$-module, let $V=U\oplus W$ be a direct-sum decomposition, and let $T\colon V\rightarrow V$ be a linear operator.  Then, if $U$ and $W$ are $T$-invariant, then
	\begin{equation}
		\proj _U\circ T=T\circ \proj _U\text{ and }\proj _W\circ T=T\circ \proj _W.
	\end{equation}
	\begin{proof}
		Suppose that $U$ and $W$ are $T$-invariant.  Let $v\in V$ and write $v=u+w$ for unique $u\in U$ and $w\in W$.  Then, $T(v)=T(u)+T(w)$.  By invariance, $T(u)\in U$ and $T(w)\in W$, so by definition of projections, $\proj _U(T(v))=T(u)\ceqq T(\proj _U(v))$.  Hence, $\proj _U\circ T=T\circ \proj _U$.  Similarly for $W$.
	\end{proof}
\end{prp}

We know now that we can't always find a basis in which the matrix of a linear operator is diagonal.  Our knowledge of direct-sums from the previous subsection suggests that perhaps we can get what is in a sense the `next best thing':  a \emph{block-diagonal} matrix.  Before, we saw that the matrix our our linear-transformation will be diagonal iff the basis consisted of eigenvectors.  Our goal now then is to determine when the matrix of our linear-transformation with respect to a \emph{direct-sum decomposition} is block-diagonal.  As eigenspaces were special types of invariant subspaces, we might guess that the more general invariant subspaces will work.  Indeed, this is the case.
\begin{dfn}{Block-diagonalizable}{BlockDiagonalizble}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, $T$ is \term{block-diagonalizable}\index{Block-diagonalizable} iff there is a direct-sum decomposition $V=V^1\oplus \cdots \oplus V^m$ such that $\coordinates{T}{\collection{V}\leftarrow \collection{V}}$ is a block-diagonal matrix, where $\collection{V}\ceqq \{ V^1,\ldots ,V^m\}$.
	\begin{rmk}
		In this case, $\coordinates{T}{\collection{V}\leftarrow \collection{V}}$ is the \term{block-diagonalization}\index{Block-diagonalization} of $T$.
	\end{rmk}
\end{dfn}
\begin{thm}{Fundamental Theorem of Block-diagonaliz\-ability}{thm4.5.2}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, the following are equivalent.
	\begin{enumerate}
		\item $T$ is block-diagonalizable.
		\item There is a direct-sum decomposition of $V$ consisting of $T$-invariant subspaces.
		\item There are $T$-invariant subspaces $V^1,\ldots ,V^m$ such that
		\begin{equation}
		\dim (V)=\dim (V^1)+\cdots +\dim (V^m).
		\end{equation}
	\end{enumerate}
	In this case, $\coordinates{T}{\collection{V}\leftarrow \collection{V}}$ is a block-diagonal matrix, where $\collection{V}\ceqq \{ V^1,\ldots ,V^m\}$.
	\begin{rmk}
		Let $V=V^1\oplus \cdots \oplus V^m$ be a direct-sum decomposition of $V$ consisting of $T$-invariant subspaces.  As each $V^k$ is invariant, $T$ restricts to a linear-transformation $T^k\ceqq \restr{T}{V^k}\colon V^k\rightarrow V^k$.  Using this notation, we have
		\begin{equation}
		\coordinates{T}{\collection{V}\leftarrow \collection{V}}=\begin{bmatrix}T^1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & T^m\end{bmatrix}\eqqc T^1\oplus \cdots \oplus T^m.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		Warning:  This name is nonstandard---there is no standard name for this result.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

\subsubsection{Indecomposability}

Of course, $V=V$ is itself a direct-sum decomposition, and one that generally won't be very useful.  In order to simplify the block-diagonal form as much as possible, we wish to break up $V$ into the `smallest' invariant subspaces we can.  The precise sense in which we want these subspaces to the the ``smallest'' possible is called \emph{indecomposable}.
\begin{dfn}{Indecomposable}{Indecomposable}
	Let $V$ be a $R$-module.  Then, $V$ is \term{indecomposable}\index{Indecomposable module} iff whenever $V=U\oplus W$, it follows that either $U=0$ or $W=0$.
	\begin{rmk}
		If $V$ is a $\K$-module and $T\colon V\rightarrow V$ is a linear operator, we say that $V$ is \term{$T$-indecomposable} iff $V$ is indecomposable when regarded as a $\K [x]$-module (\cref{exm1.1.34}).
	\end{rmk}
	\begin{rmk}
		If we say that a subspace $W\subseteq V$ is $T$-indecomposable, it is implicit that it is also $T$-invariant (otherwise it wouldn't actually possess the structure of a $\K [x]$-module, and so the definition given in the previous remark wouldn't make sense).
	\end{rmk}
	\begin{rmk}
		You can view the requirement that $V$ be nonzero as roughly analogous to how we disallow $1\in \Z$ from being considered prime.
	\end{rmk}
	\begin{rmk}
		Warning:  The term ``irreducible'' (\cref{SimpleModule}) is \emph{not} the same as ``indecomposable'' (\cref{exmC.1.2}), though they are certainly related---irreducible implies indecomposable (\cref{prpC.2.2}).
	\end{rmk}
	\begin{rmk}
		Warning:  Some authors do not consider the $0$ module to be indecomposable.  (Presumably their reason for doing so is similar to the reason why $1\in \Z ^+$ is not considered prime.)
	\end{rmk}
\end{dfn}
We now consider whether we can, and if so, how can we, decompose $V$ into indecomposable submodules.  The basic idea is as follows.  If $V$ is indecomposable, we're done; otherwise, there are proper nonzero submodules $U,W\subseteq V$ such that $V=U\oplus W$.  If $U$ and $W$ are indecomposable, again, we're done; otherwise, we can break up $U$ and/or $W$ as a direct-sum of proper nonzero submodules, and so on.  In general, there is no need for this process to terminate.  However, the next result says that it does in the primary case of interest.
\begin{prp}{}{prp4.5.17}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, there are finitely-many $T$-indecomposable subspaces $V^1,\ldots ,V^m\subseteq V$ such that
	\begin{equation}
		V=V^1\oplus \cdots \oplus V^m.
	\end{equation}
	\begin{rmk}
		For what it's worth, modules that can be written as direct-sums of indecomposable modules are known as \term{completely-decomposable}\index{Completely-decomposable module} modules.  Here, we would be regarding $V$ as a $\F [x]$-module, $\F$ the ground field,\footnote{All vector spaces are `trivially' completely-decomposable---every basis gives a direct-sum decomposition into $\F$-invariant subspaces.} in which case we would say that $V$ is \term{$T$-completely-decomposable} as short-hand for the statement ``$V$ is a completely-decomposable $\F [x]$-module.''.
	\end{rmk}
	\begin{rmk}
		Warning:  This will not be true in general---see \cref{exr4.5.17}.
	\end{rmk}
	\begin{proof}
		If $V$ is indecomposable, we are done.  Otherwise, there are proper nonzero invariant subspaces $U_1,U_2\subseteq V$ such that $V=U_1\oplus U_2$.  Note that as $V$ is finite-dimensional and these are proper subspaces, we have that $\dim (U_1),\dim (U_2)<\dim (V)$.  Again, if each $U_1$ and $U_2$ are indecomposable, we are done; otherwise, we can write these as the direct-sum of invariant subspaces whose dimensions are strictly less than $\dim (U_1)$ and $\dim (U_2)$ respectively.
		
		Repeating this process inductively, the process either stops because everything is indecomposable, or we have reached that point where there are no proper nonzero subspaces, in which case the subspace has to have dimension $1$, and so must be indecomposable.  Thus, we eventually find that $V=V_1\oplus \cdots \oplus V_M$ for $V_k\subseteq V$ indecomposable subspaces, as desired.
	\end{proof}
\end{prp}
\begin{exr}{}{exr4.5.17}
	What is an example of a $\K$-module that is not completely-decomposable?  Can you find one that is an $\F [x]$-module?
\end{exr}

Now seems an appropriate time to return to a counter-example we mentioned previously, namely, a subspace of a $\K$-module with no complement.
\begin{exm}{A subspace of a $\K$-module with no complement}{exm4.5.15}
	Define $\K \ceqq \C$, $V\ceqq \C ^2$, $U\ceqq \Span (\coord{1,0})$, and let $T_A\colon V\rightarrow V$ be the $\C$-linear-transformation defined by the matrix
	\begin{equation}
		A\ceqq \begin{bmatrix}0 & 1 \\ 0 & 0\end{bmatrix}.
	\end{equation}
	Note that $U\subseteq V$ is a $\C [x]$-submodule, that is, it is $T_A$-invariant.  We claim that it has no complement.
	
	We proceed by contradiction:  let $W\subseteq V$ be a $\K [x]$-submodule such that $V=U\oplus W$ (as $\C [x]$-modules).  $W$ is then in particular a $\C$-subspace, and so, by a dimension count, we must have $W=\Span (\coord{a,b})$ for $a,b\in \C$, $b\neq 0$.  However, $N(\coord{a,b})=\coord{b,0}\notin W$, and so $W$ is not $N$-invariant, no matter what $a$ and $b$ are.
	\begin{rmk}
		Essentially this same argument shows that $V$ is an indecomposable $\C [x]$-module.  In particular, this serves as an example of an indecomposable module with proper nonzero submodules.\footnote{One might naively think that if $U\subseteq V$ is proper and nonzero, we can write $V=U\oplus W$ for $W\subseteq V$ also proper nonzero, as we can with vector spaces, and so the existence of such a $U$ implies decomposability.  This example shows that this naive thinking is wrong.}
	\end{rmk}
\end{exm}

\subsection{Generalized ``Eigenstuff''}

After realizing that we won't always be able to find a basis in which the matrix of a given linear-transformation was diagonal, we set out to find the `next best thing'.  In the previous sections, we decided that instead we should only try to find a \emph{block}-diagonal matrix, and in order to do that, we should decompose $V$ into invariant subspaces.  Furthermore, in order to simplify the description as much as possible, we would like these subspaces not to just be invariant but to be \emph{indecomposable}.

\subsubsection{A glimpse of generalized-eigenspaces}

Our first hint at how we might do all this is given by the following result \cref{prp4.6.1}.
\begin{prp}{}{lma4.6.2}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then,
	\begin{equation}
	0\subseteq \Ker (T)\subseteq \cdots \subseteq \Ker (T^{\dim (V)})=\Ker (T^{\dim (V)+1})=\cdots .
	\end{equation}
	\begin{proof}
		Let $v\in \Ker (T^k)$.  Then, $T^k(v)=0$, and so $T^{k+1}(v)\ceqq T(T^k(v))=0$, and so $v\in \Ker (T^{k+1})$.  Hence, $\Ker (T^k)\subseteq \Ker (T^{k+1})$.
		
		If $\Ker (T^m)=\Ker (T^{m+1})$, then in fact $\Ker (T^m)=\Ker (T^n)$ for all $n\geq m$.  To see this, write $n=m+k$ for $k\in \N$ and let $v\in \Ker (T^{m+k})$, so that $T^{m+1}(T^{k-1}(v))=T^{m+k}(v)=0$.  Then, $T^{k-1}(v)\in \Ker (T^{m+1})=\Ker (T^m)$, and so in fact $T^m(T^{k-1}(v))=T^{m+k-1}(v)=0$.  Proceeding inductively, we eventually find that $T^m(v)=0$.
		
		Thus, if there is some $m\leq \dim (V)$ such that $\Ker (T^m)=\Ker (T^{m+1})$, we are done.  Otherwise, $0$ is properly contained in $\Ker (T)$, which is properly contained in $\Ker (T^2)$, etc..  This means that $\dim (\Ker (T^k))\geq \dim (\Ker (T^{k-1}))+1$ for $1\leq k\leq \dim (V)$, and hence $\dim (\Ker (T^{\dim (V)}))\geq \dim (V)$,\footnote{As you go from $0$, to $\Ker (T)$, to $\Ker (T^2)$, etc., you must increase the dimension by $1$ at each step, and so by the time you get to $\Ker (T^{\dim (V)})$, the dimension must be at least $\dim (V)$.} in which case we must have $\Ker (T^{\dim (V)})=\Ker (T^m)$ for all $m\geq \Ker (T)$, as desired.
	\end{proof}
\end{prp}
\begin{prp}{}{prp4.6.1}
	Let $V$ be a vector space over a division ring $\F$, let $T\colon V\rightarrow V$ be linear, let $\lambda \in \F$ be central, and let $m\in \N$.  Then, if $\Ker ([T-\lambda ]^m)=\Ker ([T-\lambda ]^{m+1})$, then,
	\begin{equation}
		V=\Ker ([T-\lambda ]^m)\oplus \Ima ([T-\lambda ]^m)
	\end{equation}
	is a direct-sum decomposition of $V$ into $T$-invariant subspaces.
	\begin{rmk}
		Note by the previous result that, if $V$ is finite-dimensional, we always have
		\begin{equation}
			V=\Ker ([T-\lambda ]^{\dim (V)})\oplus \Ima ([T-\lambda ]^{\dim (V)}).
		\end{equation}
	\end{rmk}
	\begin{rmk}
		In fact, they are invariant subspaces for all $m\in \N$---we needn't assume anything about $m$ for this part of the result to hold.
	\end{rmk}
	\begin{rmk}
		Note that $\Eig _{\lambda}=\Ker (T-\lambda )\subseteq \Ker ([T-\lambda ]^{\dim (V)})$, that is, the first invariant subspace here contains the $\lambda$-eigenspace of $T$.\footnote{Strictly speaking, we shouldn't be using this notation or language if $\lambda$ is not an eigenvalue.  In any case, the inclusion of the kernels stated remains valid.}
	\end{rmk}
	\begin{rmk}
		We will see later that, if $\lambda \in \F$ is an eigenvalue of $T$, then $\Ker ([T-\lambda ]^{\dim (V)})$ is the \emph{$\lambda$-generalized-eigenspace} of $T$.
	\end{rmk}
	\begin{proof}
		We first check that these are invariant subspaces.  Let $v\in \Ker ([T-\lambda ]^{m})$.  As $T$ computes with $T-\lambda$, we have that
		\begin{equation}
			[T-\lambda ]^{m}(T(v))=T\left( [T-\lambda ]^{m}(v)\right) =T(0)=0,
		\end{equation}
		and so $T(v)\in \Ker ([T-\lambda ]^{m})$.
		
		Let $v=[T-\lambda ]^{m}(w)\in \Ima ([T-\lambda ]^{m})$.  Then,
		\begin{equation}
			T(v)=T\left( [T-\lambda ]^{m}(w)\right) =[T-\lambda ]^{m}\left( T(w)\right) ,
		\end{equation}
		and so $T(v)\in \Ima ([T-\lambda ]^{m})$.
		
		It suffices to prove the rest of the result for $\lambda =0$ (this is the same as proving the result for arbitrary $T$ linear).  By \cref{crl4.4.11}, it suffices to show that (i) $V=\Ker (T^{m})+\Ima (T^{m})$ and that (ii) $\Ker (T^{m})\cap \Ima (T^{m})=0$.
		
		We first check that $\Ker (T^{m})\cap \Ima (T^{m})=0$.  So, let $v=T^{m}(w)\in \Ker (T^{m})\cap \Ima (T^{m})$.  We thus have that
		\begin{equation}
		0=T^{m}(v)=T^{2m}(w),
		\end{equation}
		and hence $w\in \Ker (T^{2m})$.  By \cref{lma4.6.2}, $\Ker (T^{m})=\Ker (T^{2m})$, and so in fact $w\in \Ker (T^{m})$, and hence $v=T^{m}(w)=0$.
		
		It now follows that
		\begin{equation}
			\begin{split}
				\MoveEqLeft
				\dim (\Ker (T^{m})+\Ima (T^{m})) \\
				& =\dim (\Ker (T^{m}))+\dim (\Ima (T^{m})) \\
				& =\footnote{By the \namerefpcref{RankNullityTheorem}.}m,
			\end{split}
		\end{equation}
		and hence $V=\Ker (T^{m})+\Ima (T^{m})$.	
	\end{proof}
\end{prp}

The next idea is to apply this decomposition inductively:  List the distinct eigenvalues of $T$, $\lambda _1,\ldots ,\lambda _m$, write $V=\Ker ([T-\lambda _1]^{\dim (V)})\oplus W_1$, where we have defined $W_1\ceqq \Ima ([T-\lambda _1]^{\dim (V)})$, write $W_1=\Ker ([\restr{T-\lambda _2}{W_1}]^{\dim (W_1)})\oplus W_2$, where we have defined $W_2\ceqq \Ima (\restr{[T-\lambda _2]}{W_1}^{\dim (V)}$, and so on.  With any luck, we will find $W_m=0$, so that $V$ can be written as a finite direct-sum of subspaces of the form $\Ker ([T-\lambda ]^{\dim (V)})$.

That said, the issue or not of whether we can make such a decomposition is going to be irrelevant if we don't understand the behavior of $T$ on subspaces of the form $\Ker ([T-\lambda ])^{\dim (V)})$.  In brief, $T-\lambda$ is \emph{nilpotent} on this subspace, which brings us to the next subsubsection.

\subsubsection{Nilpotent linear operators}

Before we begin a study if nilpotency proper, let us briefly consider an alternative perspective one can take on finding the ``next best thing'' to diagonalization.

Let $T\colon V\rightarrow V$ be a linear operator and let $\basis{B}$ be a basis for $V$.  As we can't always find a $\basis{B}$ such that $\coordinates{T}{\basis{B}}$ is diagonal, we instead try to see if we can find a $\basis{B}$ such that
\begin{equation}
	\coordinates{T}{\basis{B}}=D+N,
\end{equation}
where $D$ is diagonal and $N$ is `small' (i.e.~`close' to $0$) in some sense.

To see what notion of ``small'' we might want, recall that we are going to try to decompose $V$ into subspaces of the form $\Ker ([T-\lambda ]^{\dim (V)})$, and so are also interested in understanding the behavior of the restriction of $T$ to these subspaces.  If this were simply $\Ker (T-\lambda )$, that is, an eigenspace,\footnote{At least for $\lambda$ an eigenvalue.} we would have $T-\lambda =0$ on this subspace---in the notation above, we have $D=\lambda$ and $N=0$.  However, we don't quite have that for the subspace $\Ker ([T-\lambda ]^{\dim (V)})$.  Rather, we only have that the restriction of $T-\lambda$ to this subspace is \emph{nilpotent}.  It is in this sense that $N$ will be ``small''.
\begin{dfn}{Nilpotent}{Nilpotent}
	Let $X$ be a rg and let $x\in X$.  Then, $x$ is \term{nilpotent}\index{Nilpotent} iff there is some $m\in \N$ such that $x^m=0$.
	\begin{rmk}
		The smallest natural number $\Rank (x)\in \N$\index[notation]{$\Rank (x)$} such that $x^{\Rank (x)}=0$ is the \term{rank}\index{Rank (of a nilpotent element)} of $x$.
	\end{rmk}
	\begin{rmk}
		For us, the rg we're going to be interested in is in fact a ring, $\End _{\Mod{\K}}(V)$, $V$ a $\K$-module, and which case for $T\colon V\rightarrow V$ to be nilpotent means that there is some $m\in \N$ such that $T^m=0$, where
		\begin{equation}
			T^m\ceqq \underbrace{T\circ \cdots \circ T}_m,
		\end{equation}
		that is, $T$ composed with itself $m$ times.
	\end{rmk}
	\begin{rmk}
		For $N\colon V\rightarrow V$ nilpotent and $v\in V$, we say that $v$ is \term{$N$-maximal}\index{$N$-maximal} iff it is \emph{not} the case that $v=N(w)$ for some $w\in V$.\footnote{This is just the statement that $v\notin \Ima (N)$, though saying ``$v$ is $N$-maximal'' is more suggestive in this context for reasons that will hopefully become more apparent later on.}  Of course, the definition of nilpotent always guarantees that there is some $N$-maximal element (except for the stupid case $N=0$).
		
		Note that this is the notion of maximal that corresponds to the preorder $\preceq$ defined by $v\preceq w$ iff $v=N^k(m)$ for some $k\in \N$.
	\end{rmk}
\end{dfn}
\begin{exm}{}{}
	Let $N$ be a strictly upper-triangular matrix.  Then, $N$ is nilpotent.
	\begin{exr}[breakable=false]{}{}
		Check this.
	\end{exr}
\end{exm}

One of the first things we note about nilpotent operators is that they have but one possible eigenvalue:  zero.
\begin{prp}{}{}
	Let $V$ be a vector space, let $N\colon V\rightarrow V$ be linear, and let $\lambda \in \F$ be an eigenvalue.  Then, if $N$ is nilpotent, $\lambda =0$.
	\begin{rmk}
		In fact, except for the silly case $V=0$ (in which case no operator can have any eigenvalue), $0$ will be an eigenvalue of $N$.
	\end{rmk}
	\begin{proof}
		Suppose that $N$ is nilpotent.  By definition, there is then some $m\in \N$ such that $N^m=0$.  By \cref{crl4.2.36}, it follows that $\lambda ^m=0$, whence $\lambda =0$ because the ground ring is a division ring.
	\end{proof}
\end{prp}
This has an important, albeit unfortunate, consequence.
\begin{crl}{}{}
	Let $V$ be a finite-dimensional vector space and let $N\colon V\rightarrow V$ be nilpotent linear.  Then, $N$ is diagonalizable iff $N=0$.
	\begin{rmk}
		Moreover, we will see later that, in a sense, having a nonzero `nilpotent part' is the only way in which a linear operator can fail to be diagonalizable.
	\end{rmk}
	\begin{proof}
		$(\Rightarrow )$ Suppose that $N$ is diagonalizable.  Then, there is a basis of $V$ consisting of eigenvectors of $N$.  By the previous result, the corresponding eigenvalues are all $0$, and so $N$ vanishes on each of these basis vectors.  Hence, $N=0$.
		
		\blni
		$(\Leftarrow )$ Suppose that $N=0$.  Then, $\coordinates{N}{\basis{B}}$ will be a diagonal\footnote{To accommodate the silly case in which $V=0$, note that the empty matrix is vacuously diagonal.} matrix for any basis $\basis{B}$ of $V$.
	\end{proof}
\end{crl}

We thus know that nilpotent linear-transformations are essentially never diagonalizable, so if we are to come up with a `next best thing' to diagonalization, we had at the very least know how to do so for nilpotent linear-transformations.  The following result is our solution to this problem.
\begin{thm}{Jordan Canonical Form of Nilpotent Operators}{thm4.6.17}
	Let $V$ be a nonzero finite-dimensional vector space, let $N\colon V\rightarrow V$ be nilpotent linear, let $v_0\in V$ be nonzero, and let $r\in \N$ be the smallest natural number such that $N^r(v_0)=0$.  Then,
	\begin{enumerate}
		\item \label{thm4.6.17(i)}
		\begin{equation}
			N^{\infty}(v_0)\ceqq \{ N^{r-1}(v_0),\ldots ,N(v_0),v_0\}
		\end{equation}
		is linearly-independent;
		\item \label{thm4.6.17(ii)}
		\begin{equation}
			\Span (N^{\infty}(v_0))
		\end{equation}
		is $N$-indecomposable;
		\item \label{thm4.6.17(iii)}
		\begin{equation}
			\Span (N^{\infty}(v_0))
		\end{equation}
		is maximal $N$-indecomposable if $v_0\in V$ is $N$-maximal; and
		\item \label{thm4.6.17(iv)}if $V$ is $N$-indecomposable and $v_0\in V$ is $N$-maximal, then $N^{\infty}(v_0)$ is a basis of $V$ that has the property that
		\begin{equation}\label{eqn4.6.17}
			\coordinates{N}{N^{\infty}(v_0)\leftarrow N^{\infty}(v_0)}=\begin{bmatrix}0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 1 & \cdots & 0 \\ \vdots &\ddots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 1 \\ 0 & 0 & 0 & \cdots & 0\end{bmatrix}.
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		Intuitively, one can think of this as follows.  Starting with $v_0$, if you want something that is $N$-invariant, that something had better contain $N(v_0)$, $N^2(v_0)$, and so on.  This gives you $N^{\infty}(v_0)$.  If you then want that something to be a subspace, you had better taken the span to obtain $\Span (N^{\infty}(v_0))$.  In turns out that this is $N$-indecomposable, is maximal if $v_0$ is $N$-maximal, and furthermore, the matrix of $N$ with respect to this basis is particularly simple---see \eqref{eqn4.6.17}.
	\end{rmk}
	\begin{rmk}
		Note that (\cref{prp4.5.17}) we can always write $V$ as a direct-sum of finitely many $N$-indecomposable submodules, and so in the general case (when $V$ is not necessarily indecomposable), we can use this result to obtain a basis $\basis{B}$ of $V$ that has the property that $\coordinates{N}{\basis{B}\leftarrow \basis{B}}$ will be a direct-sum of matrices of the above form.
	\end{rmk}
	\begin{proof}
		\cref{thm4.6.17(i)} Suppose that
		\begin{equation}
		0=\alpha _0\cdot v_0+\cdots +\alpha _{r-1}\cdot N^{r-1}(v_0).
		\end{equation}
		Applying $N^{r-1}$ to this equation, we obtain $0=\alpha _0\cdot N^{r-1}(v_0)=0$, and hence $\alpha _0$.  Applying $N^{r-2}$ in the same way, we find $\alpha _1$.  Proceeding inductively, we eventually find that every $\alpha _k=0$, establishing linear-independence.
		
		\blni
		\cref{thm4.6.17(ii)} Let $U,W\subseteq \Span (N^{\infty}(v_0))$ be $N$-invariant subspaces such that $\Span (N^{\infty}(v_0))=U\oplus W$.  Either $U$ or $W$ must contain some vector $v$ for which $N^{r-1}(v)\neq 0$.  Without loss of generality, suppose $U$ contains such a vector $v\in U$.  Writing
		\begin{equation}\label{eqn4.4.139}
			v=\alpha _0\cdot v_0+\cdots +\alpha _{r-1}\cdot N^{r-1}(v_0),
		\end{equation}
		we see that $\alpha _0\neq 0$ (otherwise $N^{r-1}(v)=0$).  By $N$-invariance, we must have that
		\begin{equation}
			\alpha _0\cdot N^{r-1}(v_0)=N^{r-1}(v)\in U,
		\end{equation}
		and so as $\alpha _0\neq 0$, $N^{r-1}(v_0)\in U$.  It follows that
		\begin{equation}
			v-\alpha _{r-1}\cdot N^{r-1}(v_0)=\alpha _0\cdot v_0+\cdots +\alpha _{r-2}\cdot N^{r-2}(v_0)\in U.
		\end{equation}
		Continuing this process inductively, eventually we obtain that all terms in \eqref{eqn4.4.139} with nonzero coefficients are elements of $U$, and so in particular $v_0\in U$.  But then $U=\Span (N^{\infty}(v_0))$, and so $\Span (N^{\infty}(v_0))$ is indecomposable.
		
		
		\blni
		\cref{thm4.6.17(iii)} Suppose that $v_0\in V$ is $N$-maximal.  Let $W\supseteq \Span (N^{\infty}(v_0))$ be $N$-indecomposable.  We wish to show that $W=\Span (N^{\infty}(v_0))$.  To do this, we proceed by induction.
		
		\Step{Start the proof of spanning with induction}
		The statement we prove by induction (on $\Rank (N)$) is as follows.
		\begin{displayquote}
			If $N$ is a nilpotent linear operator on a finite-dimensional vector space $V$ and $v_0\in V$ is $N$-maximal, then $\{ N^{\Rank (N)-1}(v_0),\ldots ,v_0\}$ spans $V$.
		\end{displayquote}
		First take $\Rank (N)\eqqc r=1$, so that $N=0$.  In this case, every subspace is $N$-invariant, and so indecomposability forces $\dim (V)=1$, so that indeed $V=\Span (v_0)$.
		
		Now suppose the result is true for $r-1$.  If $r$ is the smallest positive integer such that $N^m=0$ on $V$, then $r-1$ is the smallest positive integer such that $N^{r-1}=0$ on $\Ima (N)$.  Furthermore, $N(v_0)\in \Ima (N)$ is such that $N^{(r-1)-1}(N(v_0))=N^{r-1}(v_0)\neq 0$.  Thus, if we can show that $\Ima (N)$ is indecomposable still, then the induction hypothesis will gives us that $\Ima (N)=\Span (N^{r-1}(v_0),\ldots ,N(v_0))$.
		
		\Step{Show that $\Ima (N)$ is indecomposable}[stp4.6.17.4]
		So, let $U,W\subseteq \Ima (N)$ be subspaces and suppose that $\Ima (N)=U\oplus W$ with $U$ and $W$ $N$-invariant.  Then write $V=U\oplus W\oplus V'$ for some subspace $V'\subseteq V$ (by \cref{prp4.4.24}).  For every $v\in V'$, we may write
		\begin{equation}\label{eqn4.4.132}
		\begin{split}
		N(v) & =\proj _U(N(v))+\proj _W(N(v)) \\
		& =\footnote{$N$ commutes with the projections because the subspaces are invariant---see \cref{prp4.5.9}.}N(\proj _U(v))+N(\proj _W(v))\eqqc N(v_U)+N(v_W),
		\end{split}
		\end{equation}
		where we have written $v_U\ceqq \proj _U(v)$ and $v_W\ceqq \proj _W(v)$.  Let $\basis{B}'$ be a basis for $V'$, and define
		\begin{equation}
		\breve{\basis{B}}\ceqq \{ b-b_U:b\in \basis{B}'\} 
		\end{equation}
		and $\breve{V}\ceqq \Span (\breve{\basis{B}})$.  From \eqref{eqn4.4.132}, we see that $N(b-b_U)=N(b_W)\in W$, and so $W+\breve{V}$ is an invariant subspace.  Thus, if we can show $V=U\oplus W\oplus \breve{V}$, indecomposability will imply that either $U=0$ or $W\oplus \breve{V}=0$, so that either $U=0$ or $W=0$, as desired.
		
		So, suppose that $0=u+w+\breve{v}$ for $u\in U$, $w\in W$, and $\breve{v}\in \breve{V}$.  Write $\breve{v}=\alpha _1(b_1-[b_1]_U)+\cdots +\alpha _m(b_m-[b_m]_U)$, so that
		\begin{equation}
		0=\left( u-\sum _{k=1}^m\alpha _k[b_k]_U\right) +w+\sum _{k=1}^r\alpha _kb_k.
		\end{equation}
		As $V=U\oplus W\oplus W'$, it follows that $\sum _{k=1}^r\alpha _kb_k=0$, whence each $\alpha _k=0$ by linear-independence, that $w=0$, and that $u=\sum _{k=1}^r\alpha _k[b_k]_U=0$.  By \cref{prp4.4.7}, it just remains to check that $V=U+W+\breve{V}$.
		
		So, let $v\in V$.  We may then write $v=u+w+v'$ for $u\in U$, $w\in W$, and $v'\in V'$.  Write $v'=\alpha _1b_1+\cdots +\alpha _rb_r$.  Then,
		\begin{equation}
		v'=\left( u+\sum _{k=1}^r\alpha _k[b_k]_U\right) +w+\sum _{k=1}^r\alpha _k(b_k-[b_k]_U)\in U+W+\breve{V},
		\end{equation}
		as desired.  Thus, $\Ima (T)$ is indecomposable, and hence $\Ima (N)=\Span (N^{r-1}(v_0),\ldots ,N(v_0))$.
		
		\Step{Finish the proof of spanning}
		Extend the linear-independent set $\basis{B}\ceqq \{ N^{r-1}(v_0),\ldots ,v_0\}$ to a basis $\basis{C}$ of $V$ such that $\basis{C}\supseteq \basis{B}$.  We wish to show that $\basis{C}=\basis{B}$.  So, let $c\in \basis{C}\setminus \basis{B}$.  As $N(c)\in \Ima (N)$, we can write
		\begin{equation}\label{eqn4.6.21}
		\begin{split}
		N(c)& =\alpha _1N(v_0)+\cdots +\alpha _{r-1}N^{r-1}(v_0) \\
		& =N\left( \alpha _1v_0+\cdots +\alpha _{r-1}N^{r-2}(v_0)\right) .
		\end{split}
		\end{equation}
		Now define
		\begin{equation}\label{eqn4.6.22}
		\breve{c}\ceqq c-\sum _{k=1}^{r-1}\alpha _kN^{k-1}(v_0)
		\end{equation}
		as well as $\breve{\basis{A}}\ceqq \{ \breve{c}:c\in \basis{C}\setminus \basis{B}\}$.  \eqref{eqn4.6.21} implies that $N(\breve{c})=0$.  In particular, $\breve{V}\ceqq \Span (\breve{\basis{C}})$ is invariant.  Thus, if we can show that $V=\Span (\basis{B})\oplus \Span (\breve{\basis{A}})$, indecomposability will force $\Span (\breve{\basis{A}})=0$, hence $\breve{\basis{A}}=\emptyset$, hence $\basis{C}\setminus \basis{B}=\emptyset$, hence $\basis{C}=\basis{B}$, completing the proof.
		
		Thus, it remains only to check that $V=\Span (\basis{B})\oplus \Span (\breve{\basis{A}})$.  So, let $v\in \Span (\basis{B})\cap \Span (\breve{\basis{A}})$.  We could then write this vector as a linear-combination of both elements of $\basis{B}$ and elements of $\breve{\basis{A}}$.  Using the definition of the $\breve{c}$s in \eqref{eqn4.6.22}, we see that this would yield a nontrivial linear-dependence relation about elements of $\basis{B}$ and elements of $\basis{C}\setminus \basis{B}$ unless $v=0$.  By linear-dependence of $\basis{C}$ then, we have that $\Span (\basis{B})\cap \Span (\breve{\basis{A}})=0$.
		\begin{exr}[breakable=false]{}{}
			Finish the proof by showing that $V=\Span (\basis{B})+\Span (\basis{C})$.
			\begin{rmk}
				Hint:  You might try an argument similar to the one we used to show that $V=U+W+\breve{V}$ in \cref{stp4.6.17.4}.
			\end{rmk}
		\end{exr}
		
		\blni
		\cref{thm4.6.17(iv)} Suppose that $V$ is $N$-indecomposable and $v_0\in V$ is $N$-maximal.  By the previous part, $\Span (N^{\infty}(v_0))$ is maximal indecomposable, and so by maximality we have $V=\Span (N^{\infty}(v_0))$.  As we already knew it was linearly-independent (by \cref{thm4.6.17(i)}), it follows that $\Span (N^{\infty}(v_0))$ is a basis of $V$.  Finally, \eqref{eqn4.6.17} follows from the fact that $N(N^{r-1}(v_0))=0$ and $N(N^k(v_0))=N^{k+1}(v_0)$ for $0\leq k<r-1$.
	\end{proof}
\end{thm}

The use of nilpotent operators is far more common, and in our primary case of interest, it will work just fine.  However, for the purposes of dealing with generalized-eigenspaces in general, a related concept is more natural, that of \emph{local-nilpotency}.
\begin{dfn}{Locally-nilpotent}{LocallyNilpotent}
	Let $V$ be an $R$-module and let $T\in R$.  Then, $T$ is \term{locally-nilpotent}\index{Locally-nilpotent} iff for every $v\in V$ there is some $m_v\in \N$ such that $T^{m_v}\cdot v=0$.
	\begin{rmk}
		Of course, we're interested in the case $V$ is a $\K [x]$-module with module structure defined as usual by a given linear-transformation $T\colon V\rightarrow V$, in which case we say that $T$ is \term{locally-nilpotent} iff $x\in \K[x]$ is.  Explicitly, $T$ is locally-nilpotent iff for every $v\in V$ there is some $m_v\in \N$ such that $T^{m_v}(v)=0$.
	\end{rmk}
	\begin{rmk}
		Intuitively, the difference between nilpotency and local-nilpotency if the depends of $m_v$ on $V$.  For local-nilpotency, you are allowed pick a different $m_v$ for each $v\in V$; for nilpotency on the other hand, you must be able to pick a single $m\in \N$ that `works' ``uniformly'' for all $v\in V$.
		
		Thus, it is clear from the definitions that a nilpotent linear-transformation is locally-nilpotent.
	\end{rmk}
\end{dfn}
\begin{exm}{A locally-nilpotent operator that is not nilpotent}{exm4.4.129}
	Consider the left-shift operator $L\colon \C ^{\infty}\rightarrow \C ^{\infty}$ (\cref{ShiftOperator}).  Let $a\in \C ^{\infty}$.  By definition of $\C ^{\infty}$, $m\mapsto a_m$ is eventually $0$, so that there is some $M$ such that whenever $m\geq M$ it follows that $a_m=0$.  Then $L^M(a_m)=0$, and hence $L$ is locally-nilpotent.
	
	To see that $L$ is not nilpotent, note that for every $m\in \N$, the sequence $a\in C^{\infty}$ that is identically $0$ except for a $1$ at index $m$ is not `killed' by $L^m$:  $L^m(a)\neq 0$.  Hence, $L^m\neq 0$ for any $m\in \N$, and hence $L$ is not nilpotent.
	
	\horizontalrule
	
	For $m\in \N$, let $E_m\subseteq \C ^{\infty}$ be the subspace of sequences which vanish for indices larger than $m$.  Note that $T$ is nilpotent on $E_m$, but not nilpotent on all of $\C ^{\infty}$.  Thus, there is no maximal subspace on which $T$ is nilpotent.
	\begin{rmk}
		This fact is mentioned later when we discuss generalized-eigenspaces, and in particular, why locally-nilpotent, and not nilpotent, is the notion that should be used in the general definition.
	\end{rmk}
\end{exm}
On the other hand, in finite-dimensions, they are equivalent.
\begin{prp}{}{prp4.4.130}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, $T$ is nilpotent iff it is locally-nilpotent.
	\begin{proof}
		$(\Rightarrow )$ This is always true.
		
		\blni
		$(\Leftarrow )$ Suppose that $T$ is locally-nilpotent.  Let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis for $V$.  Let $m_k\in \N$ be such that $T^{m_k}(b_k)=0$.  Define $m\ceqq \max \{ m_1,\ldots ,m_d\}$.  Then, $T^m(b_k)=0$ for all $1\leq k\leq m$.  Let $v\in V$ and write $v=v^1\cdot b_1+\cdots +v^d\cdot b_d$.  Then,
		\begin{equation}
			T^m(v)=v^1\cdot T^m(b_1)+\cdots +v^d\cdot T^m(b_d)=0.
		\end{equation}
		Hence, $T^m=0$, and so $T$ is nilpotent.
	\end{proof}
\end{prp}

\subsubsection{More than a glimpse of generalized-eigenspaces}

Having understood the behavior of $T$ on subspaces of the form $\Ker ([T-\lambda ]^{\dim (V)})$, it is time we actually decompose $V$ into such subspaces.  Before we do so, we give such subspaces a name.
\begin{dfn}{Generalized-eigenspaces}{GeneralizedEigenspaces}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $W\subseteq V$ be a nonzero subspace, and let $\lambda \in \K$.  Then, $W$ is a \term{$\lambda$-generalized-eigenspace}\index{Generalized-eigenspace} iff
	\begin{enumerate}
		\item $\restr{T}{W}=\lambda \id _W+N$ with $N\colon W\rightarrow W$ locally-nilpotent linear; and
		\item $W$ is maximal with this property.
	\end{enumerate}
	\begin{rmk}
		Such a $\lambda$ is referred to a \term{generalized-eigenvalue}\index{Generalized-eigenvalue} of $T$.\footnote{We will see (\cref{prp4.6.25}) that in fact generalized-eigenvalues are the same as the eigenvalues.  So in particular, we don't introduce a separate notation for the set of generalized-eigenvalues.}
	\end{rmk}
	\begin{rmk}
		Nonzero elements of $W$ are \term{generalized-eigenvectors} of $T$ with eigenvalue $\lambda$.
	\end{rmk}
	\begin{rmk}
		Note that it follows immediately from this that scaling by $\lambda$ on $W$ is linear.  Furthermore, we shall see shortly (\cref{prp4.6.25}) that generalized-eigenvalues are just eigenvalues,\footnote{Which is why you probably won't see the term ``generalized-eigenvalue'' in most places---it turns out they're just the same as eigenvalues, and so people just stick with using the single term ``eigenvalue'' for everything.} and so will in particular be central, at least for vector spaces (\cref{thm4.2.17}).
	\end{rmk}
	\begin{rmk}
		See the definition of eigenspaces (\cref{Eigen}) for some other potentially useful remarks that apply equally well here.
	\end{rmk}
\end{dfn}
As before, we start by establishing uniqueness of generalized-eigenspaces.
\begin{prp}{Generalized-eigenspaces are unique}{}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $\lambda \in \K$, and let $U,W\subseteq V$ be $\lambda$-generalized-eigenspaces of $T$.  Then, $U=W$.
	\begin{rmk}
		We denote the unique $\lambda$-generalized-eigenspace of $V$ by $\Eig _{\lambda ,T}^{\infty}$\index[notation]{$\Eig _{\lambda ,T}^{\infty}$} (for reasons that we will understand shortly).\footnote{See \cref{RankOfGeneralizedEigenvectors} to see why the use of ``$\infty$'' is appropriate here.}  If $T$ is clear from context, we may simply write $\Eig _{\lambda}^{\infty}\ceqq \Eig _{\lambda ,T}^{\infty}$\index[notation]{$\Eig _{\lambda}^{\infty}$}.
	\end{rmk}
	\begin{rmk}
		If ever we write ``$\Eig _{\lambda ,T}$'' without having explicitly said so, it should be assumed that $\lambda$ is an eigenvalue of $T$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				See the proof of \cref{prp4.2.3}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{Generalized-eigenspaces are maximum with their defining property}{}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $W\subseteq V$ be a subspace, and let $\lambda \in \K$ be a generalized-eigenvalue of $V$.  Then, if $\restr{T}{W}=\lambda \id _W+N$ with $N\colon W\rightarrow W$ locally-nilpotent linear, then $W\subseteq \Eig _{\lambda}^{\infty}$.
	\begin{rmk}
		A corollary of this is that, to show that $\lambda$ is a generalized-eigenvalue, it suffices to exhibit a single nonzero subspace $W$ on which $T-\lambda$ is locally-nilpotent linear.  For this result implies that $\Eig _{\lambda}^{\infty}\supseteq W$, so that if $W$ is nonzero, so is $\Eig _{\lambda}^{\infty}$, in which case $\lambda$ would be a generalized-eigenvalue.
	\end{rmk}
	\begin{rmk}
		Note that this is one reason why we prefer locally-nilpotent over nilpotent in the definition of generalized-eigenspaces.  Were we to use ``nilpotent'' in place of ``locally-nilpotent'' in the definition, this result would fail.  A counter-example is given by the left-shift operator on $\C ^{\infty}$---see \cref{exm4.4.129}.
	\end{rmk}
	\begin{proof}
		Suppose that $\restr{T}{W}=\lambda \id _W+N$ with $N\colon W\rightarrow N$ locally-nilpotent linear.  Define
		\begin{equation}
			\begin{split}
				\collection{U} & \ceqq \left\{ U\subseteq V\text{ a subspace}:\restr{T}{U}-\lambda \id _U\text{ is}\right. \\ & \qquad \left. \text{locally-nilpotent linear.}\right\} 
			\end{split}
		\end{equation}
		and
		\begin{equation}
		E\ceqq \sum _{U\in \collection{U}}U.
		\end{equation}
		
		Let $e\in E$ and and write $e=u_1+\cdots +u_m$ for some $u_1\in U_1,\ldots ,u_m\in U_m$ with $U_1,\ldots ,U_m\in \collection{U}$.  As $T-\lambda$ is locally-nilpotent on $U_k$, there is some $n_k\in \N$ such that $[T-\lambda ]^{m_k}(u_k)=0$.  Define $n\ceqq \max \{ n_1,\ldots ,n_m\}$.  Then,
		\begin{equation}
			[T-\lambda ]^n(e)=[T-\lambda ]^n(u_1)+\cdots +[T-\lambda ]^n(u_m)=0.
		\end{equation}
		That is, $\restr{T}{E}=\lambda \id _E+N$ with $N\colon E\rightarrow E$ locally-nilpotent linear.  $E$ is maximal with this property, and hence $E=\Eig _{\lambda}^{\infty}$.  Finally, note that $W\in \collection{U}$, and hence $W\subseteq E=\Eig _{\lambda}^{\infty}$. 
	\end{proof}
\end{prp}
\begin{prp}{}{prp4.6.25}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \K$.  Then, $\lambda$ is a generalized-eigenvalue of $T$ iff it is an eigenvalue of $T$.
	\begin{rmk}
		Thus, hereafter, we will likely stick with just ``eigenvalue'' and stop using the term ``generalized-eigenvalue''.
	\end{rmk}
	\begin{proof}
		$(\Rightarrow )$ Suppose that $\lambda$ is a generalized-eigenvalue of $T$.  Then, $\Eig _{\lambda}^{\infty}\subseteq V$ is a subspace of $V$ on which $T-\lambda$ is locally-nilpotent and is maximal with this property.
		
		For succinctness of notation, let us temporarily write $N\ceqq \restr{T-\lambda}{\Eig _{\lambda}^{\infty}}$, so that $N\colon \Eig _{\lambda}^{\infty}\rightarrow \Eig _{\lambda}^{\infty}$ is locally-nilpotent linear.  If $\Ker (N)=0$, then $N$ would be injective, and so $N^m$ would be injective for all $m\in \N$, and hence $N$ couldn't possibly be locally-nilpotent.  Thus, $\Ker (N)\subseteq V$ is a nonzero subspace such that $\restr{T}{\Ker (N)}=\lambda \id _{\Ker (N)}$, and therefore $\lambda$ is an eigenvalue of $T$.
		
		\blni
		$(\Leftarrow )$ Suppose that $\lambda$ is an eigenvalue of $T$.  Then, $\Eig _{\lambda}$ is a nonzero subspace on which $T-\lambda$ is locally-nilpotent, and so $\lambda$ is a generalized-eigenvalue of $T$.
	\end{proof}
\end{prp}
\begin{prp}{Generalized-eigenvalues are unique (in vector spaces)}{}
	Let $V$ be a vector space and let $T\colon V\rightarrow V$ be linear.  Then, if $\Eig _{\lambda ,T}^{\infty}=\Eig _{\mu ,T}^{\infty}$, it follows that $\lambda =\mu$.
	\begin{proof}
		Suppose that $\Eig _{\lambda ,T}^{\infty}=\Eig _{\mu ,T}^{\infty}$.  For convenience, let us write $\Eig _{\lambda ,T}^{\infty}\eqqc E\ceqq \Eig _{\mu ,T}^{\infty}$.  There is then a locally-nilpotent operator $N\colon E\rightarrow E$ such that
		\begin{equation}
			\lambda \id _E+N=\restr{T}{E}=\mu \id _E+N
		\end{equation}
		Let $v\in E$ be nonzero.  Then,
		\begin{equation}
			\lambda v+N(v)=\mu v+N(v),
		\end{equation}
		and hence $(\lambda -\mu )v=0$.  As $v\neq 0$ and the ground ring is a division ring, we must have that $\lambda -\mu =0$, that is, $\lambda =\mu$.
	\end{proof}
\end{prp}
We also have a result analogous to \cref{thm4.2.17} that is useful for actually `calculating' generalized-eigenspaces.
\begin{thm}{}{thm4.4.143}
	Let $V$ be a vector space over a division ring $\F$, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \F$.  Then, if $\lambda$ is an eigenvalue of $T$, then
	\begin{equation}
		\begin{split}
			\Eig _{\lambda ,T}^{\infty} & =\left\{ v\in V:[T-\lambda ]^m(v)=0\text{for some }m\in \N \text{.}\right\} \\
			& =\bigcup _{m\in \N}\Ker ([T-\lambda ]^m).
		\end{split}
	\end{equation}
	In fact, if $V$ is finite-dimensional, then
	\begin{equation}
		\Eig _{\lambda ,T}^{\infty}=\Ker ([T-\lambda ]^{\dim (V)}).
	\end{equation}
	\begin{rmk}
		The analogous result for eigenvalues \cref{thm4.2.17} had two parts whereas this result only has one.  This is because the first part of \cref{thm4.2.17} gives an explicit characterization of eigenvalues, but as eigenvalue and generalized-eigenvalues are the same, there is no additional characterization here.
	\end{rmk}
	\begin{proof}
		Suppose that $\lambda$ is an eigenvalue of $T$.  Define
		\begin{equation}
			\begin{split}
				W & \ceqq \left\{ v\in V:[T-\lambda ]^m(v)=0\text{ for some }m\in \N \text{.}\right\} \\
				& =\bigcup _{m\in \N}\Ker ([T-\lambda ]^m).
			\end{split}
		\end{equation}
		\begin{exr}[breakable=false]{}{}
			Check that $W$ is a subspace.
		\end{exr}
		$W$ contains $\Ker (T-\lambda )$, and so is nonzero as $\lambda$ is an eigenvalue.  By definition, $T-\lambda$ is locally-nilpotent on $W$.
		
		To show maximality, let $U\supseteq W$ be a subspace such that $\restr{T}{U}=\lambda \id _U+N$ for $N\colon U\rightarrow U$ locally-nilpotent.  Then, for every $u\in U$, there is some $m\in \N$ such that $N^m(u)=0$, in which case $[T-\lambda ]^m(u)=N^m(u)=0$.  Thus, $u\in W$, so that $U\subseteq W$, showing maximality.  Hence,
		\begin{equation}
		W\ceqq \left\{ v\in V:[T-\lambda ]^m(v)=0\text{for some }m\in \N \text{.}\right\} =\Eig _{\lambda ,T}^{\infty}.
		\end{equation}
		
		Now suppose that $V$ is finite-dimensional.  It follows from \cref{lma4.6.2} that
		\begin{equation}
			\Eig _{T,\lambda}^{\infty}=\bigcup _{m\in \N}\Ker ([T-\lambda ]^m)=\Ker ([T-\lambda ]^{\dim (V)}).
		\end{equation}
	\end{proof}
\end{thm}
Related to the fact that we can write the generalized-eigenspace as a union in this way is that we now have a notion of \emph{rank}.\footnote{Well, I suppose we had a notion of rank before with just eigenvalues, but it's a bit silly because all eigenvalues have rank $1$, as you'll see in the following definition.}
\begin{dfn}{Rank (of generalized-eigenvectors)}{RankOfGeneralizedEigenvectors}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $\lambda \in \K$ be an eigenvalue of $T$, and let $v\in \Eig _{\lambda}^{\infty}$.  Then, the \term{rank}\index{Rank (of a generalized-eigenvector)} of $v$ is the smallest natural number $\Rank (v)$\index[notation]{$\Rank (v)$} such that $[T-\lambda ]^{\Rank (v)}(v)=0$.
	\begin{rmk}
		For $m\in \N$, the \term{rank $m$ $\lambda$-eigenspace}\index{Rank $m$ eigenspace}, $\Eig _{T,\lambda}^m$, is defined by
		\begin{equation}
			\Eig _{T,\lambda}^m\ceqq \left\{ v\in \Eig _{T,\lambda}^{\infty}:\Rank (v)\leq m\right\} .
		\end{equation}\index[notation]{$\Eig _{T,\lambda}^m$}
		As before, we will often write $\Eig _{\lambda}^m\ceqq \Eig _{\lambda ,T}^m$\index[notation]{$\Eig _{\lambda}^m$}.
	\end{rmk}
	\begin{rmk}
		We may say ``rank $m$ eigenvector'' instead of the more verbose ``rank $m$ generalized-eigenvector''---the fact that we are mentioning the rank at all implies that we do not mean eigenvectors is the `usual' sense (unless of course $m$ happens to be $1$).
	\end{rmk}
	\begin{rmk}
		Warning:  Note that $\Eig _{\lambda ,T}^m$ is \emph{not} the space of rank $m$ eigenvectors, but rather, the space of generalized-eigenvectors with rank \emph{at most} $m$.  The set of generalized-eigenvectors with rank exactly equal to $m$ is not a subspace in general.
	\end{rmk}
	\begin{rmk}
		As $\restr{T-\lambda}{\Eig _{\lambda}^{\infty}}$ is locally-nilpotent, there is by definition some such natural number.
	\end{rmk}
	\begin{rmk}
		Note that the rank $1$ generalized-eigenvectors are precisely the (`ordinary') eigenvectors.  Also note that $0\in \Eig _{\lambda ,T}^{\infty}$ has rank $0$, though it is technically not a generalized-eigenvector.
	\end{rmk}
	\begin{rmk}
		Hence,
		\begin{equation}
			\Eig _{\lambda ,T}^1=\Eig _{\lambda ,T}
		\end{equation}
		and
		\begin{equation}
			\Eig _{\lambda ,T}^0=0.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		Note that $\Rank (v)\leq \dim (V)$ if $V$ is finite-dimensional by the previous result \cref{thm4.4.143}.
	\end{rmk}
\end{dfn}
\begin{exm}{}{}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0\end{bmatrix}.
	\end{equation}
	$A$ has a single eigenvalue, $0$.  $e_1\ceqq \coord{1,0,0}$ is a generalized-eigenvector of rank $1$ (so just an eigenvector), $e_2\ceqq \coord{0,1,0}$ is a generalized-eigenvector of rank $2$, and $e_3\ceqq \coord{0,0,1}$ is a generalize-eigenvector of rank $3$.  $e_1$ should be clear.  As for $e_2$, we can see that it has rank $2$ because
	\begin{equation}
		Ae_2\neq 0\text{ but }A^2e_2=0.
	\end{equation}
	Similarly, we see that $e_3$ has rank $3$ because
	\begin{equation}
		A^2e_3\neq 0\text{ but }A^3e_3=0.
	\end{equation}
	Thus, $\{ e_1,e_2,e_3\}$ is a basis of $\R ^3$ consisting of generalized-eigenvectors of $A$, whose elements have rank $1$, $2$, and $3$ respectively.
	
	Note that this matrix is \emph{not} diagonalizable, so there is \emph{not} a basis of (regular) eigenvectors.  Thus, generalizing to, well, generalized-eigenvectors has solved this problem.
	
	Additionally, note that $A$ is nilpotent and $e_3$ is $A$-maximal.  According to \cref{thm4.6.17}, $\{ e_3,Ae_3,A^2e_3\}$ will be linearly-independent and indecomposable.  Indeed, $Ae_3=e_2$, and $A^2e_3=e_1$, so this is certainly linearly-independent.
	
	When it comes to actually computing the Jordan canonical form of linear-transformations, you can't just use any basis of generalized-eigenvectors, but rather you have to pick your generalized-eigenvectors in a certain way.  In general, you want to find the ``maximal'' generalized-eigenvectors and `generate' a basis by applying the nilpotent operators as we have done here.  We'll explain this in more detail later, so don't worry if this doesn't all make sense now.
\end{exm}
\begin{exm}{}{}
	Define $D\colon C^{\infty}(\R )\rightarrow C^{\infty}(\R )$.  From \cref{exm4.2.17}, we know that every $\lambda \in \C$ is an eigenvalue with eigenspace given by $\Eig _{\lambda}=\Span (\e ^{\lambda x})$.  Let us now investigate the generalized-eigenspaces.
	
	Let $m\in \N$, $\lambda \in \C$, and $f\in C^{\infty}(\R )$.  Then, $f\in \Eig _{\lambda}^m$ iff
	\begin{equation}
		[D-\lambda ]^m(f)=0.
	\end{equation}
	A basis for the set of solutions to this differential equation is given by
	\begin{equation}
		\left\{ \e ^{\lambda x},x\e ^{\lambda x},\ldots ,x^{m-2}\e ^{\lambda x},x^{m-1}\e ^{\lambda x}\right\} ,\footnote{The existence and uniqueness theorem from ordinary differential equations implies that the set of solutions is an $m$-dimensional vector space.  To find a basis, it thus suffices to exhibit $m$ linearly-independent solutions.  After staring at it for awhile, you realize that functions of the form $x^k\e ^{\lambda x}$ do the job.  (Seriously, while you might have forgotten after it's become so familiar, ultimately even the solution $\e ^{\lambda x}$ is originally found by making an educated guess.)}
	\end{equation}
	and hence
	\begin{equation}
		\Eig _{\lambda}^m=\Span \left( \e ^{\lambda x},x\e ^{\lambda x},\ldots ,x^{m-2}\e ^{\lambda x},x^{m-1}\e ^{\lambda x}\right) .
	\end{equation}
\end{exm}
\begin{prp}{Properties of higher rank eigenspaces}{prp4.4.154}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \K$ be an eigenvalue of $T$.  Then,
	\begin{enumerate}
		\item \label{prp4.4.154(i)}
		\begin{equation}
			\Eig _{\lambda}^m=\Ker ([T-\lambda ]^m);
		\end{equation}
		\item \label{prp4.4.154(ii)}$\Eig _{\lambda}^m\subseteq V$ is a $T$-invariant subspace; and
		\item
		\begin{equation}
			\Eig _{\lambda}^0\subseteq \Eig _{\lambda}^1\subseteq \Eig _{\lambda}^2\subseteq \cdots ;
		\end{equation}
		and
		\item \label{prp4.4.154(iii)}if $v\in \Eig _{\lambda}$ has rank $m\in Z^+$, then $[T-\lambda ](v)$ has rank $m-1$.
	\end{enumerate}
	\begin{rmk}
		In particular, $\Eig _{\lambda}^0=0$ and $\Eig _{\lambda}^1=\Eig _{\lambda}$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

Recall (\cref{prp4.2.22}) that eigenvectors with distinct eigenvalues are linearly-independent.  The same is true for generalized-eigenvectors (and in fact this is a strict generalization of the previous result).
\begin{prp}{Generalized-eigenvectors with distinct ei\-genvalues are linearly-independent}{prp4.4.157}
	Let $V$ be a vector space, let $T\colon V\rightarrow V$ be linear, let $\lambda _1,\ldots ,\lambda _m$ be distinct eigenvalues of $T$, and let $v_k\in \Eig _{\lambda _k}^{\infty}$ be nonzero $1\leq k\leq m$.  Then, $\{ v_1,\ldots ,v_m\}$ is linearly-independent.
	\begin{proof}
		Denote the ground division ring by $\F$.  Suppose that
		\begin{equation}
			0=\alpha _1\cdot v_1+\cdots +\alpha _m\cdot v_m.
		\end{equation}
		Let $n\in \Z$ be the smallest positive integer such that $w_1\ceqq [T-\lambda _1]^n(v_1)\neq 0$, so that $[T-\lambda _1](w_1)=0$, that is, $T(w_1)=\lambda _1w_1$.  It follows that $[T-\lambda ](w_1)=(\lambda _1-\lambda )w$ for any $\lambda \in \F$.  Apply $[T-\lambda _1]^n$ to this equation to obtain
		\begin{equation}
			0=\alpha _1\cdot w_1+\cdots +\alpha _m\cdot w_m,
		\end{equation}
		where we have defined $w_k\ceqq [T-\lambda _1]^n(v_k)$.\footnote{The basic strategy for doing this is to hopefully reduce the argument to a similar one we used for the analogous result for eigenvectors (note of course that $w_1\ceqq [T-\lambda _1]^n(v)_1$ is an eigenvectors of $T$ with eigenvalue $\lambda _1$ by construction).}  Note that, as generalized-eigenspaces are $T$-invariant subspaces and $\lambda _1\in \F$ is central, $w_k\ceqq [T-\lambda _1]^n(v_k)\in \Eig _{\lambda _k}^{\infty}$ is still in the $\lambda _k$-generalized-eigenspace.  Hence, for each $2\leq k\leq m$, there is some $n_k\in \Z ^+$ such that $[T-\lambda _k]^{n_k}(w_k)=0$.  Applying $[T-\lambda _2]^{n_2}\cdots [T-\lambda _m]^{n_m}$ to this equation, we thus find
		\begin{equation}
			0=\alpha _1(\lambda _1-\lambda _2)^{n_2}\cdots (\lambda _1-\lambda _m)^{n_m}.
		\end{equation}
		As the eigenvalues are distinct and we're working over a division ring, this implies that $\alpha _1=0$.
		
		Proceeding inductively and doing the same thing for $\alpha _2$, $\alpha _3,$ and so on, we eventually obtain $0=\alpha _1=\cdots =\alpha _m$, and so $\{ v_1,\ldots ,v_m\}$ is linearly-independent.
	\end{proof}
\end{prp}

We are now essentially ready to put all the pieces together.

\subsection{The Jordan Canonical Form Theorem}

Okay, so that was a lie.  There is still one minor detail we need to address:  how do we know we have any eigenvalues at all!?  Indeed, we don't know that.  Because it's just not true.  We saw in \cref{exm4.3.7} an example of a linear-transformation that didn't have any eigenvalues.  Fortunately, there is a natural hypothesis would can impose on the ground division ring in order to guarantee we have eigenvalues, in which case (\cref{GeneralizedEigenspaceDecomposition}), not only do we have a single eigenvalue, but we have `enough' generalized-eigenvectors to generate the entire space.  The hypothesis we speak of is that of being \emph{algebraically closed}, a discussion of which, to a void yet another large interruption before we reach our goal (among other reasons), has been placed in the appendix (\cref{AlgebraicallyClosed}).

\subsubsection{The theorem.  Finally.}

We're finally ready to state the Jordan Canonical Form Theorem, which, among other things, details the near-diagonal form our matrices will take in a suitable basis.  To state it, however, it will be convenient to have a name for a special type of matrix appearing in the description.
\begin{dfn}{Jordan block}{JordanBlock}
	Let $\K$ be a ring, let $\lambda \in \K$, and let $m\in \Z ^+$.  Then, the \term{Jordan block}\index{Jordan block}, $\Jord _{\lambda ,m}$\index[notation]{$\Jord _{\lambda ,m}$}, of size $m$ with eigenvalue $\lambda$ is the $m\times m$ matrix
	\begin{equation}
	\Jord _{\lambda ,m}\ceqq \begin{bmatrix}\lambda & 1 & 0 & \cdots & 0& 0 \\ 0 & \lambda & 1 & \cdots & 0 & 0 \\ \vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & \ddots & 1 & 0 \\ 0 & 0 & 0 & \cdots & \lambda & 1 \\ 0 & 0 & 0 & \cdots & 0 & \lambda\end{bmatrix}.
	\end{equation}
	\begin{rmk}
		Note that this can be written as
		\begin{equation}
		\Jord _{\lambda ,m}=\lambda \id +N_m,
		\end{equation}
	\end{rmk}
	where we have used the notation
	\begin{equation}
	N_m\ceqq \begin{bmatrix}0 & 1 & 0 & \cdots & 0 & 0 \\ 0 & 0 & 1 & \cdots & 0 & 0 \\ \vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & \ddots & 1 & 0 \\ 0 & 0 & 0 & \cdots & 0 & 1 \\ 0 & 0 & 0 & \cdots & 0 & 0\end{bmatrix}.
	\end{equation}
\end{dfn}
\begin{exm}{}{}
	\begin{equation}
		\Jord _{-2,3}\coloneqq \begin{bmatrix}-2 & 1 & 0 \\ 0 & -2 & 1 \\ 0 & 0 & -1\end{bmatrix}\text{ and } \Jord _{1,1}\coloneqq \begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix}.
	\end{equation}
\end{exm}
\begin{exm}{Jordan blocks and ranks of generalized-eigenvectors}{JordanRank}
	Consider for the sake of example the Jordan block
	\begin{equation}
		\Jord _{5,4}\ceqq \begin{bmatrix}5 & 1 & 0 & 0 \\ 0 & 5 & 1 & 0 \\ 0 & 0 & 5 & 1 \\ 0 & 0 & 0 & 5\end{bmatrix}.
	\end{equation}
	
	Note that the standard basis for $\C ^4$ $\basis{S}\eqqc \{ e_1,e_2,e_3,e_4\}$ is a basis consisting of generalized-eigenvectors of $\Jord _{5,4}$.  Furthermore, $e_1$ has rank $1$, $e_2$ has rank $2$, $e_3$ has rank $3$, and $e_4$ has rank $4$.
	\begin{exr}[breakable=false]{}{}
		Check all this.
	\end{exr}

	Of course, none of this is specific to $\Jord _{5,4}$, and in general we have the following.
	\begin{displayquote}
		For a Jordan block of size $m$, the standard basis is a basis of generalized-eigenvectors.  The first has rank $1$, the second has rank $2$, and so on.
		
		In particular, \emph{Jordan blocks have a single eigenvector} (up scaling).
	\end{displayquote}
\end{exm}
And now, what is arguably the most important theorem in the entirety of the notes.\footnote{After reading the statement, if things are still unclear, you might consider jumping to the end of the proof, after which comes some more explanation.}
\begin{thm}{Jordan Canonical Form Theorem}{GeneralizedEigenspaceDecomposition}
	Let $V$ be a finite-dimensional vector space over an algebraically closed field $\F$, let $T\colon V\rightarrow V$ be linear, and denote the eigenvalues of $T$ by $\lambda _1,\ldots ,\lambda _m$.  Then,
	\begin{enumerate}
		\item \label{GeneralizedEigenspaceDecomposition(i)}
		\begin{equation}\label{eqn4.4.173}
			V=\Eig _{\lambda _1}^{\infty}\oplus \cdots \oplus \Eig _{\lambda _m}^{\infty};
		\end{equation}
		\item \label{GeneralizedEigenspaceDecomposition(ii)}the coordinates of $T$ with respect to this direct-sum decomposition is
		\begin{equation}
			\begin{bmatrix}\lambda _1\id _{\Eig _{\lambda _1}^{\infty}}+N_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \lambda _m\id _{\Eig _{\lambda _m}^{\infty}}+N_m\end{bmatrix},
		\end{equation}
		where $N_k\colon \Eig _{\lambda _k}^{\infty}\rightarrow \Eig _{\lambda _k}^{\infty}$ is nilpotent linear;
		\item \label{GeneralizedEigenspaceDecomposition(iii)}for every $1\leq k\leq m$, there is a basis $\basis{B}_k$ for $\Eig _{\lambda _k}^{\infty}$ such that the coordinates
		\begin{equation}\label{eqn4.4.175}
			\coordinates{\lambda _k\id _{\Eig _{\lambda _k}^{\infty}}+N_k}{\basis{B}_k\leftarrow \basis{B}_k}
		\end{equation}
		is a direct-sum of Jordan blocks with eigenvalue $\lambda _k$, in fact, $\basis{B}_k$ is the union of $N_k^{\infty}(v_0)\ceqq \{ v_0,N(v_0),\ldots ,N^{\Rank (N_k)-1}(v_0)\}$ as $v_0$ ranges over a chosen $N_k$-maximal from each $N$-indecomposable summand of $\Eig _{\lambda _k}^{\infty}$;
		\item \label{GeneralizedEigenspaceDecomposition(iv)}$\basis{B}\ceqq \basis{B}_1\cup \cdots \cup \basis{B}_m$ is a basis for $V$ such that the coordinates
		\begin{equation}\label{eqn4.4.176}
			\coordinates{T}{\basis{B}\leftarrow \basis{B}}
		\end{equation}
		is the direct-sum of the matrices in \eqref{eqn4.4.175}; and
		\item \label{GeneralizedEigenspaceDecomposition(v)}if $\basis{C}$ is another basis of $V$ that has the property that $\coordinates{T}{\basis{C}\leftarrow \basis{C}}$ is a direct-sum of Jordan blocks, then the set of Jordan blocks appearing in this decomposition is the same as the set of Jordan blocks appearing in the decomposition of $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$.
	\end{enumerate}
	\begin{rmk}
		\eqref{eqn4.4.173} is the \term{generalized-eigenspace decomposition}\index{Generalized-eigenspace decomposition} of $V$.
	\end{rmk}
	\begin{rmk}
		The matrix in \eqref{eqn4.4.176} is the\footnote{``The'' is justified by the fact the the Jordan canonical form is essentially unique by \cref{GeneralizedEigenspaceDecomposition(iv)}.} \term{Jordan canonical form}\index{Jordan canonical form} of $T$.  It is also called the \term{Jordan normal form}\index{Jordan normal form} of $T$, but I prefer ``Jordan canonical form'' for the simple reason that the word ``normal'' tends to be overused in mathematics.  A basis in which the matrix of $T$ is in Jordan canonical form is called a \term{Jordan basis}\index{Jordan basis}.
	\end{rmk}
	\begin{rmk}
		In short:
		
		\cref{GeneralizedEigenspaceDecomposition(i)} says that $V$ is a direct-sum of the generalized-eigenspaces.
		
		\cref{GeneralizedEigenspaceDecomposition(ii)} says that the matrix of $T$ with respect to this decomposition is block-diagonal with $\lambda _k\id _{\Eig _{\lambda _k}^{\infty}}+N_k$ on the diagonal.
		 
		\cref{GeneralizedEigenspaceDecomposition(iii)} says that we can choose bases for each of the generalized-eigenspaces in which the matrix associated to the linear operators $\lambda _k\id _{\Eig _{\lambda _k}^{\infty}}+N_k$ is a direct-sum of Jordan blocks.
		 
		\cref{GeneralizedEigenspaceDecomposition(iv)} says that these bases assemble together to form a basis for all of $V$ for which the matrix of $T$ is a direct-sum of the matrices of the previous part, and so in particular a direct-sum of Jordan blocks.
		
		\cref{GeneralizedEigenspaceDecomposition(v)} says that the Jordan canonical form of $T$ is essentially unique in the sense that, if you write $T$ as a direct-sum of Jordan blocks, the Jordan blocks appearing in that decomposition must be exactly the same as the ones you found before.
	\end{rmk}
	\begin{rmk}
		Note that it follows from this (and the comment made at the end of \cref{JordanRank}) that \emph{the dimension of the $\lambda$ eigenspace is equal to the number of Jordan blocks with eigenvalue $\lambda$ appearing in the Jordan canonical form}.
	\end{rmk}
	\begin{rmk}
		Note that a direct-sum of Jordan blocks of size $1$ is a diagonal matrix, and so, by uniqueness, a matrix is diagonalizable iff all the the Jordan blocks appearing it its Jordan canonical form are of size $1$, in which case the diagonalization is the same as its Jordan canonical form.  Thus, Jordan canonical form is indeed a strict generalization of diagonalization.
	\end{rmk}
	\begin{rmk}
		Warning:  For diagonalizable linear operators, any basis of eigenvectors will serve as a diagonalizing basis, the analogous result is \emph{not} true for Jordan canonical form.  That is, it is \emph{not} the case that $\coordinates{T}{\basis{B}}$ will be in Jordan canonical form for any basis of generalized-eigenvectors $\basis{B}$.  One has to be a bit more careful---this is explained in more detail in \cref{GeneralizedEigenspaceDecomposition(iii)} and again after the proof of this result.
	\end{rmk}
	\begin{rmk}
		In particular, $T$ has at least one eigenvalue.
	\end{rmk}
	\begin{rmk}
		In particular, generalized-eigenvectors with distinct eigenvalues are linearly-independent.
	\end{rmk}
	\begin{rmk}
		This is one of the few times, and most likely the most significant time, where I was not able to remove the assumption of commutativity.  The reason for this is that eigenvalues are central, and so you would need to assume that all nonconstant polynomials have \emph{central} roots.  But if you do that, then you automatically have a field!\footnote{For every $\alpha \in \F$, $x-\alpha$ would have a central root, implying that $\alpha$ is central.}
	\end{rmk}
	\begin{proof}
		\cref{GeneralizedEigenspaceDecomposition(i)} We proceed by (strong induction) on $\dim (V)$.  If $\dim (V)=0$, the statement is vacuously true.
		
		So, let $d\ceqq \dim (V)\in \Z ^+$ and suppose that the result is true whenever the dimension of the vector space is less than $d$.
		\begin{lma}[break at=19cm/0pt]{}{}
			Let $V$ be a finite-dimensional vector space over an algebraically closed field and let $T\colon V\rightarrow V$ be linear.  Then, $T$ has an eigenvalue.
			\begin{proof}
				Let $v\in V$ be nonzero.
				\begin{equation}
				\{ v,T(v),T^2(v),\ldots ,T^d(v)\}
				\end{equation}
				is a set of $d+1$ vectors in a vector space of dimension $d$, and so it must be linearly-depend.  Thus, there are $\alpha _0,\ldots ,\alpha _d\in \F$, not all zero, such that
				\begin{equation}\label{eqn4.4.180}
				0=\alpha _0\cdot v+\cdots +\alpha _d\cdot T^d(v).
				\end{equation}
				Note that it cannot be the case that $0=\alpha _1=\cdots =\alpha _d$, for then the above equation would in turn imply $\alpha _0=0$.  Hence,
				\begin{equation}
					p(x)\ceqq \alpha _0+\alpha _1x+\cdots +\alpha _dx^d\in \F [x]
				\end{equation}
				is a nonconstant polynomial.  As $\F$ is right algebraically closed, there is a root $\lambda _1\in \F$ of $p$.  Hence, by \cref{prp4.4.174}, there is a polynomial $p_2\in \F [x]$ such that
				\begin{equation}
					p(x)=p_2(x)(x-\lambda _1).
				\end{equation}
				If $p_2$ is constant, stop.  Otherwise, $p_2$ has another root $\lambda _2\in \F$, in which case we can write
				\begin{equation}
					p(x)=p_3(x)(x-\lambda _2)(x-\lambda _1)
				\end{equation}
				for some $p_3\in \F [x]$.  Continuing inductively, we eventually find
				\begin{equation}
					p(x)=\alpha (x-\lambda _d)\cdots (x-\lambda _1)
				\end{equation}
				for some nonzero $\alpha \in \F$.
				
				By \eqref{eqn4.4.180}, we have that $[p(T)](v)=0$, and hence
				\begin{equation}
					0=\alpha [T-\lambda _d]\left( [T-\lambda _{d-1}]\left( \cdots ([T-\lambda _1](v))\cdots \right) \right) .
				\end{equation}
				If $[T-\lambda _{d-1}](\cdots )$ is nonzero, then $T-\lambda _d$ has nonzero kernel.  Otherwise, in fact we have $[T-\lambda _{d-1}](\cdots )=0$.  Now, if $[T-\lambda _{d-2}](\cdots )$ is nonzero, $T-\lambda _{d-1}$ has nonzero kernel.  Continuing inductively, we eventually find some $k$ such that $\Ker (T-\lambda _k)\neq 0$.  Thus, $\lambda _k$ is an eigenvalue of $T$.
			\end{proof}
		\end{lma}
	
		So, let $\lambda _1$ be an eigenvalue of $T$.  By \cref{thm4.4.143}, we have that $\Eig _{\lambda}^{\infty ,T}=\Ker ([T-\lambda ]^{\dim (V)})$.  Then, by \cref{prp4.6.1}, we have
		\begin{equation}\label{eqn4.4.178}
			V=\Eig _{\lambda ,T}^{\infty}\oplus U,
		\end{equation}
		where $U\ceqq \Ima ([T-\lambda ]^{\dim (V)})$ is $T$-invariant.
	
		Enumerate all the eigenvalues of $T$ by $\lambda _1,\ldots ,\lambda _m$.
		\begin{exr}[breakable=false]{}{}
			Show that the eigenvalues of $\restr{T}{U}\colon U\rightarrow U$ are $\lambda _2,\ldots ,\lambda _m$.
		\end{exr}
		By the induction hypothesis, we now have
		\begin{equation}\label{eqn4.4.180x}
			U=\Eig _{\lambda _2,\restr{T}{U}}^{\infty}\oplus \cdots \oplus \Eig _{\lambda _m,\restr{T}{U}}^{\infty}.
		\end{equation}
		Combining this with \eqref{eqn4.4.178} will complete the proof if we can show that $\Eig _{\lambda _k,\restr{T}{U}}^{\infty}=\Eig _{\lambda _k,T}^{\infty}$.  As the proof is the same for all $k$, it suffices to prove this for $k=2$.
	
		So, let $u\in \Eig _{\lambda _2,\restr{T}{U}}^{\infty}$.  There is then some $m\in \N$ such that $[T-\lambda _2]^m(u)=0$, and so $u\in \Eig _{\lambda _2,T}^{\infty}$.  In the other direction, let $v\in \Eig _{\lambda _2,T}^{\infty}$.  As before, we know that there is some $m\in \N$ such that $[T-\lambda _2]^m(v)=0$, but now we need to show additionally that $v\in U$.
	
		By \eqref{eqn4.4.178}, we can write
		\begin{equation}\label{eqn4.4.181}
			v=v_1+u
		\end{equation}
		for unique $v_1\in \Eig _{\lambda _1,T}^{\infty}$ and $u\in U$.  We wish to show that $v_1=0$.
	
		By \eqref{eqn4.4.180x}, we may write
		\begin{equation}
			u=u_2+\cdots +u_m
		\end{equation} 
		for unique $u_k\in \Eig _{\lambda _k,\restr{T}{U}}^{\infty}$.  Hence,
		\begin{equation}
			v=v_1+u_2+\cdots +u_m.
		\end{equation}
		As generalized-eigenvectors with distinct eigenvalues are linearly-independent (\cref{prp4.4.157}), this implies that $v_1=0$, $u_2=v$, and $u_3=\cdots =u_m=0$.  Thus, $v\in U$, as desired.
		
		\blni
		\cref{GeneralizedEigenspaceDecomposition(ii)} As each generalized-eigenspace is invariant (\cref{prp4.4.154}\cref{prp4.4.154(ii)}, the \namerefpcref{thm4.5.2} says that the matrix of $T$ with respect to this direct-sum decomposition is
		\begin{equation}
			\begin{bmatrix}\restr{T}{\Eig _{\lambda _1}^{\infty}} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \restr{T}{\Eig _{\lambda _m}^{\infty}}\end{bmatrix}
		\end{equation}
		By definition of generalized-eigenspaces, $N_k\ceqq \restr{T}{\Eig _{\lambda _k}^{\infty}}-\lambda _k\id _{\Eig _{\lambda _k}}^{\infty}$ is locally-nilpotent, hence nilpotent by finite-dimensionality (\cref{prp4.4.130}).  We thus have that the matrix of $T$ with respect to this direct-sum decomposition is indeed of the form
		\begin{equation}
			\begin{bmatrix}\lambda _1\id _{\Eig _{\lambda _1}^{\infty}}+N_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \lambda _m\id _{\Eig _{\lambda _m}^{\infty}}+N_m\end{bmatrix},
		\end{equation}
		where $N_k\colon \Eig _{\lambda _k}^{\infty}\rightarrow \Eig _{\lambda _k}^{\infty}$ is nilpotent linear.
		
		\blni
		\cref{GeneralizedEigenspaceDecomposition(iii)} By \cref{thm4.6.17}, there is a basis $\basis{B}_k$ of $\Eig _{\lambda _k}^{\infty}$ such that
		\begin{equation}
			\coordinates{\restr{T}{\Eig _{\lambda _k}^{\infty}}-\lambda _k\id _{\Eig _{\lambda _k}^{\infty}}}{\basis{B}_k\leftarrow \basis{B}_k}
		\end{equation}
		is a direct-sum of Jordan blocks with eigenvalue $0$, and hence
		\begin{equation}
			\coordinates{\restr{T}{\Eig _{\lambda _k}^{\infty}}}{\basis{B}_k\leftarrow \basis{B}_k}
		\end{equation}
		is a direct-sum of Jordan blocks with eigenvalue $\lambda _k$.
		
		\blni
		\cref{GeneralizedEigenspaceDecomposition(iv)} As $\basis{B}_k$ is a basis for $\Eig _{\lambda _k}^{\infty}$ and $V=\Eig _{\lambda _1}^{\infty}\oplus \cdots \oplus \Eig _{\lambda _m}^{\infty}$, $\basis{B}\ceqq \basis{B}_1\cup \cdots \cup \basis{B}_m$ is a basis for $V$.  Furthermore, $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ is a block-diagonal matrix whose $k^{\text{th}}$ block is $\coordinates{\lambda _k\id _{\Eig _{\lambda _k}^{\infty}}+N_k}{\basis{B}_k\leftarrow \basis{B}_k}$.
		\begin{exr}[breakable=false]{}{}
			Check this.
		\end{exr}
		
		\blni
		\cref{GeneralizedEigenspaceDecomposition(v)} Let $\basis{C}$ is another basis of $V$ that has the property that $\coordinates{T}{\basis{C}\leftarrow \basis{C}}$ is a direct-sum of Jordan blocks.  The eigenvalues of such a direct-sum are the same as the eigenvalues of $T$, and so the eigenvalues appearing among the Jordan blocks are the same for both the bases $\basis{B}$ and $\basis{C}$.  So, fix $\lambda \in \F$ an eigenvalue of $T$, and for the moment consider only the restriction $S\ceqq \restr{T}{\Eig _{\lambda}^{\infty}}\colon \Eig _{\lambda}^{\infty}$.  As $\coordinates{T}{\basis{C}\leftarrow \basis{C}}$ is a direct-sum of Jordan blocks, the elements of $\basis{C}$ are all generalized-eigenvectors, and so $\basis{C}_{\lambda}\ceqq \basis{C}\cap \Eig _{\lambda}^{\infty}$ is a basis for $\Eig _{\lambda}^{\infty}$.  Similarly for $\basis{B}_{\lambda}\ceqq \basis{B}\cap \Eig _{\lambda}^{\infty}$.  Thus, $\coordinates{S}{\basis{B}}$ and $\coordinates{S}{\basis{C}}$ are both direct-sums of Jordan blocks and we would like to prove that the size of the Jordan blocks appearing in this decomposition are the same (the eigenvalues are of course all equal to $\lambda$).  Let $m\in \Z ^+$ be the smallest positive integer such that $[S-\lambda ]^m=0$ (there is some such $m$ as we know $S-\lambda$ is nilpotent).  It follows that $m$ is the smallest positive integer such that $\coordinates{[S-\lambda ]}{\basis{B}}^m=0$ and also the smallest positive integer such that $\coordinates{[S-\lambda ]}{\basis{C}}^m=0$.  It follows that the largest Jordan block appearing in both of these matrices is size $m$.  Now, replace $S$ with $S-\Jord _{\lambda ,m}$ and apply the same logic again.  We find again that the largest Jordan blocks appearing in these decompositions have the same size.  Proceeding inductively, we see that they have the same Jordan blocks.
	\end{proof}
\end{thm}
This theorem says a lot, all of which is important and useful, but the short and sweet version can be summed up as saying
\begin{displayquote}
	Every linear operator on a finite-dimensional vector space over an algebraically closed field has a basis in which the associated matrix is a direct-sum of Jordan blocks.
\end{displayquote}

How this works can be understood in three key steps.  First, $V$ is a direct-sum of the generalized-eigenspaces:
\begin{equation}
	V=\Eig _{\lambda _1}^{\infty}\oplus \cdots \oplus \Eig _{\lambda _m}^{\infty}.
\end{equation}
Second, we break up each generalized-eigenspace into indecomposable summands (\cref{thm4.6.17}).  Finally, as $T-\lambda _k$ is nilpotent on the corresponding generalized-eigenspace, \cref{prp4.5.17}, whose statement (or part of it) we reproduce below for convenience, explains how $T-\lambda _k$ behaves on these indecomposable components.
\begin{thm}{}{thm4.4.221}
	Let $V$ be a finite-dimensional vector space and let $N\colon V\rightarrow V$ be nilpotent linear.  Then, if $V$ is $N$-indecomposable,
	\begin{equation}
		\basis{B}\ceqq \{ v_0,N(v_0),\ldots ,N^{\dim (V)-1}(v_0)\}
	\end{equation}
	is a basis of $V$ for any $N$-maximal $v_0\in V$, that has the property that
	\begin{equation}
		\coordinates{N}{\basis{B}\leftarrow \basis{B}}=\begin{bmatrix}0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 1 & \cdots & 0 \\ \vdots &\ddots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 1 \\ 0 & 0 & 0 & \cdots & 0\end{bmatrix}.
	\end{equation}
	\begin{proof}
		This is a part of the conclusion of \cref{thm4.6.17}.
	\end{proof}
\end{thm}
Of course, there is still the issue of how to actually \emph{calculate} the damn thing, but we postpone that until the next section until after having discussed the \emph{minimal polynomial}.

To give you an idea of what this means (don't worry---it's not hard), let's take a look at a couple examples of matrices in Jordan canonical form.
\begin{exm}{}{}
	\begin{equation}
		\Jord _{3,2}\oplus \Jord _{4,1}\oplus \Jord _{-2,2}\ceqq \begin{bmatrix}3 & 1 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 & 0 \\ 0 & 0 & 4 & 0 & 0 \\ 0 & 0 & 0 & -2 & 1 \\ 0 & 0 & 0 & 0 & -2\end{bmatrix}
	\end{equation}
	is in Jordan canonical form.
	
	Similarly,
	\begin{equation}
		\Jord _{5,3}\oplus \Jord _{-3,2}=\begin{bmatrix}5 & 1 & 0 & 0 & 0 \\ 0 & 5 & 1 & 0 & 0 \\ 0 & 0 & 5 & 0 & 0 \\ 0 & 0 & 0 & -3 & 1 \\ 0 & 0 & 0 & 0 & -3\end{bmatrix}
	\end{equation}
	is in Jordan canonical form.
	
	Finally,
	\begin{equation}
		\Jord _{4,2}\oplus \Jord _{4,2}\oplus \Jord _{4,1}\ceqq \begin{bmatrix}4 & 1 & 0 & 0 & 0 \\ 0 & 4 & 0 & 0 & 0 \\ 0 & 0 & 4 & 1 & 0 \\ 0 & 0 & 0 & 4 & 0 \\ 0 & 0 & 0 & 0 & 4\end{bmatrix}.
	\end{equation}
	is also in Jordan canonical form.
	\begin{rmk}
		Note in particular that you have have \emph{multiple Jordan blocks} corresponding to the \emph{same eigenvalue}.
	\end{rmk}
\end{exm}
\begin{exm}{}{}
	Suppose I give you a linear operator $T$ on $\C ^4$ and I tell you that $T$ has a single eigenvalue $3$.  This theorem tells us that the Jordan canonical form of $T$ is going to be a direct-sum of Jordan blocks with eigenvalue $3$, and furthermore, the size of these Jordan blocks must sum to $4$.  Thus, this theorem tells you that the possible Jordan canonical forms of $T$ are as follows.
	\begin{equation}
	\begin{split}
		& \begin{bmatrix}3 & 1 & 0 & 0 \\ 0 & 3 & 1 & 0 \\ 0 & 0 & 3 & 1 \\ 0 & 0 & 0 & 3\end{bmatrix},\ \begin{bmatrix}3 & 1 & 0 & 0 \\ 0 & 3 & 1 & 0 \\ 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 3\end{bmatrix} \\ & \begin{bmatrix}3 & 1 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0 & 3 & 1 \\ 0 & 0 & 0 & 3\end{bmatrix},\ \begin{bmatrix}3 & 1 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 3\end{bmatrix} \\ & \begin{bmatrix}3 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 3\end{bmatrix}
	\end{split}
	\end{equation}
	(These respectively, going across first, correspond to Jordan block sizes of $4=4$, $4=3+1$, $4=2+2$, $4=2+1+1$, and $4=1+1+1+1$.)
	
	With just a bit of extra information, you can determine which of these five possibilities it is.  For example, if $T-3=0$, then of course it must the last choice.  If $T-3\neq 0$ but $(T-3)^2=0$, then it must be the third or fourth choice, and you can distinguish between those two by looking at $\dim (\Eig _3)$.\footnote{While the dimension of the generalized-eigenspace is of course always $4$, we have $\dim (\Eig _3)=2$ for the third matrix and $\dim (\Eig _3)=3$ for the third matrix.}  If $(T-3)^2\neq 0$ but $(T-3)^3=0$, then it must be the second choice.  Finally, if $(T-3)^3\neq 0$, then it must be the first choice.
\end{exm}

\subsubsection{Jordan canonical form of matrix linear-transformations}

When studying diagonalization, it wasn't just of interest to know what the diagonalization itself was, but also what the relationship was between the original linear-transformation and its diagonalization.  In general, this relationship is just ``The diagonalization is the matrix with respect to a basis of eigenvectors.'', but if the linear-transformation is defined by a matrix, we can be a bit more explicit.  This was done in \cref{prp4.3.16}, and we now present the analogous result for Jordan canonical form.  Conceptually, it is essentially identical in nature, and if understand the previous result, you understand this one.
\begin{prp}{}{MatrixJordan}
	Let $A$ be an $m\times m$ matrix with entries in an algebraically-closed field $\F$ and let $\basis{B}$ be a Jordan basis of $A$.  Then,
	\begin{equation}
		\coordinates{A}{\basis{B}\leftarrow \basis{B}}=\coordinates{\id}{\basis{B}\leftarrow \basis{S}}\coordinates{A}{\basis{S}\leftarrow \basis{S}}\coordinates{\id}{\basis{S}\leftarrow \basis{B}},
	\end{equation}
	where $\basis{S}$ is the standard basis of $\F ^m$.
	\begin{rmk}
		Note that $\coordinates{A}{\basis{S}\leftarrow \basis{S}}=A$ by \cref{prp3.2.100}.\footnote{I wrote \eqref{eqn4.3.12} using $\coordinates{A}{\basis{S}\leftarrow \basis{S}}$ instead of $A$ because I feel as if writing it this way makes it more `obviously' true.}
		
		Furthermore, $\coordinates{A}{\basis{B}\leftarrow \basis{B}}$ is the Jordan canonical form of $A$ (by the definition of ``Jordan basis'') and $\coordinates{\id}{\basis{B}\leftarrow \basis{S}}=\coordinates{\id}{\basis{S}\leftarrow \basis{B}}^{-1}$.  Thus, writing $J\ceqq \coordinates{A}{\basis{B}\leftarrow \basis{B}}$ and $P\ceqq \coordinates{\id}{\basis{S}\leftarrow \basis{B}}$, this equation is sometimes written more concisely (but perhaps less transparently) as
		\begin{equation}
			J=P^{-1}AP.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		Note that, by \cref{CoordinatesLinearTransformation}, the columns of $\coordinates{\id}{\basis{S}\leftarrow \basis{B}}$ are given by $\coordinates{b_k}{\basis{S}}$ for $b_k\in \basis{B}$.  However, by \cref{prp3.1.17},\footnote{This is the result that says the coordinates of a column vector with respect to the standard basis is just that original column vector.} this is just $b_k$ itself.  Thus,
		\begin{displayquote}
			\emph{$P\ceqq \coordinates{\id}{\basis{S}\leftarrow \basis{B}}$ is the matrix whose columns form a Jordan basis for $A$.}
		\end{displayquote}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result yourself.
		\end{exr}
	\end{proof}
\end{prp}

\subsubsection{Similarity of linear operators}

While we shall not really make use of the concept ourselves, as you continue in mathematics you will want to be able to determine when two linear operators are \emph{conjugate} or \emph{similar} to one another.\footnote{For example, if you want to determine the conjugacy classes in certain matrix groups.}  Jordan canonical form gives a characterization of when this is the case.
\begin{dfn}{Similar (linear operators)}{Similar}
	Let $V$ be a $\K$-module and let $S,T\colon V\rightarrow V$ be linear operators.  Then, $S$ and $T$ are \term{similar}\index{Simlar (linear operators)} iff the two $\K [x]$-module structures defined by $S$ and $T$ respectively are isomorphic.
\end{dfn}
Concretely, this means the following.
\begin{prp}{}{}
	Let $V$ be a $\K$-module and let $S,T\colon V\rightarrow V$ be linear operators.  Then, $S$ and $T$ are similar iff there is an invertible linear operator $P\colon V\rightarrow V$ such that
	\begin{equation}
		T=P\circ S\circ P^{-1}.
	\end{equation}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result yourself.
		\end{exr}
	\end{proof}
\end{prp}
Another characterization of similarity that makes it more apparent what this concept have to do with Jordan canonical form is given by the following.
\begin{prp}{}{}
	Let $V$ be a finite-dimensional vector space and let $S,T\colon V\rightarrow V$ be linear operators.  Then, $S$ and $T$ are similar iff there are bases $\basis{B}$ and $\basis{C}$ of $V$ such that
	\begin{equation}
		\coordinates{S}{\basis{B}}=\coordinates{T}{\basis{C}}.
	\end{equation}
	\begin{proof}
		$(\Rightarrow )$ Suppose that $S$ and $T$ are similar.  Then, there is an invertible linear operator $P\colon V\rightarrow V$ such that
		\begin{equation}
			T=P\circ S\circ P^{-1}.
		\end{equation}
		Let $\basis{B}$ be any basis of $V$ and define
		\begin{equation}
			\basis{C}\ceqq \{ P(b):b\in \basis{B}\} .
		\end{equation}
		\begin{exr}[breakable=false]{}{}
			Check that $\basis{C}$ is a basis of $V$.
		\end{exr}
		Write $\basis{B}=\{ b_1,\ldots ,b_d\}$, so that $\basis{C}=\{ P(b_1),\ldots ,P(b_d)\}$.
		
		From the explicit expression given in the defining result of coordinates of linear-transformations (\cref{CoordinatesLinearTransformation}), we see that
		\begin{equation}
			\coordinates{P}{\basis{C}\leftarrow \basis{B}}=\begin{bmatrix}\coordinates{P(b_1)}{\basis{C}} & \cdots & \coordinates{P(b_d)}{\basis{C}}\end{bmatrix}=\id _{d\times d}.
		\end{equation}
		Hence,
		\begin{equation}
			\coordinates{T}{\basis{C}}\ceqq \coordinates{T}{\basis{C}\leftarrow \basis{C}}=\coordinates{P}{\basis{C}\leftarrow \basis{B}}\coordinates{S}{\basis{B}\leftarrow \basis{B}}\coordinates{P^{-1}}{\basis{B}\leftarrow \basis{C}}=\coordinates{S}{\basis{B}}.
		\end{equation}
		
		\blni
		$(\Leftarrow )$ Suppose there are bases $\basis{B}$ and $\basis{C}$ of $V$ such that
		\begin{equation}
		\coordinates{S}{\basis{B}}=\coordinates{T}{\basis{C}}.
		\end{equation}
		Write $\basis{B}=\{ b_1,\ldots ,b_d\}$ and $\basis{C}=\{ c_1,\ldots ,c_d\}$ (indexing in such a way that the above equality is true) and let $P\colon V\rightarrow V$ be the unique linear operator such that $P(b_k)=c_k$ for all $1\leq k\leq m$.
		\begin{exr}[breakable=false]{}{}
			Show that $P$ is invertible.
		\end{exr}
		By construction, we have that $\coordinates{P}{\basis{C}\leftarrow \basis{B}}=\id _{d\times d}$, and so
		\begin{equation}
			\coordinates{T}{\basis{C}\leftarrow \basis{C}}=\coordinates{P}{\basis{C}\leftarrow \basis{B}}\coordinates{S}{\basis{B}\leftarrow \basis{B}}\coordinates{P^{-1}}{\basis{B}\leftarrow \basis{C}}=\coordinates{P\circ S\circ P^{-1}}{\basis{C}\leftarrow \basis{C}},
		\end{equation}
		and hence $T=P\circ S\circ P^{-1}$.
	\end{proof}
\end{prp}
As mentioned before, Jordan canonical form gives a complete characterization of when linear operators are similar.
\begin{thm}{}{}
	Let $V$ be a finite-dimensional vector space over an algebraically closed field and let $S,T\colon V\rightarrow V$ be linear-operators.  Then, $S$ and $T$ are similar iff they have the same Jordan canonical form.
	\begin{proof}
		$(\Rightarrow )$ Suppose that $S$ and $T$ are similar.  Then, there is an invertible linear operator $P\colon V\rightarrow V$ such that $T=P\circ S\circ P^{-1}$.  Let $\basis{B}$ be a basis for which $\coordinates{S}{\basis{B}}$ is in Jordan canonical form.  Define $\basis{C}\ceqq \{ P(b):b\in \basis{B}\}$.  Then,
		\begin{equation}
			\coordinates{T}{\basis{C}}=\coordinates{P\circ S\circ P^{-1}}{\basis{C}\leftarrow \basis{C}}=\coordinates{P}{\basis{C}\leftarrow \basis{B}}\coordinates{S}{\basis{B}\leftarrow \basis{B}}\coordinates{P^{-1}}{\basis{B}\leftarrow \basis{C}}=\coordinates{S}{\basis{B}}.
		\end{equation}
		Thus, $S$ and $T$ have the same Jordan canonical form.
		
		\blni
		$(\Leftarrow )$ Suppose that $S$ and $T$ have the same Jordan canonical form.  If $\basis{B}$ and $\basis{C}$ are bases of $S$ and $T$ respectively such that $\coordinates{S}{\basis{B}}$ and $\coordinates{T}{\basis{C}}$ are the Jordan canonical forms of $S$ and $T$ respectively, then by hypothesis we have that $\coordinates{S}{\basis{B}}=\coordinates{T}{\basis{C}}$, and so by the previous result $S$ and $T$ are similar.
	\end{proof}
\end{thm}

\subsection{Summary}

We've already summarized the key elements of this section, so we shall not reproduce them here.  (I thought it might still be helpful to have a ``Summary'' subsection that was easy to find.)  If it's an actual summary you're looking for, check the paragraphs immediately preceding and immediately after \cref{thm4.4.221}.

\section{The minimal polynomial}

We've finally concluded that there is a basis for any linear-transformation on a finite-dimensional vector space over an algebraically closed field for which the corresponding matrix is ``nice'', that is, a direct-sum of Jordan blocks.  That's great to know, but what we would like to be able to do is actually compute these things, that is, compute bases of the generalized-eigenspaces so that we may compute the corresponding matrix.

The first step in this process is to compute the eigenvalues, which we still have not really addressed how to do systematically yet.  One (rather poor) answer to this is the \emph{minimal polynomial} of a linear operator.  The minimal polynomial of a linear operator is a polynomial that, among other things, has the property that its roots are precisely the eigenvalues of $T$.  Thus, if you can compute the minimal polynomial, you can compute its roots,\footnote{Numerically anyways.  I'm sure you've heard of the fact that you can't solve degree $5$ polynomial equations or higher in general.} and hence the eigenvalues.  This isn't really where the minimal polynomial shines in its use, however.  In addition to the the eigenvalues themselves, the minimal polynomial also gives you information about the Jordan canonical form.

We begin with the theorem that allows us to define the minimal polynomial.
\begin{thm}{Minimal polynomial}{MinimalPolynomial}
	Let $V$ be a finite-dimensional vector space over a field $\F$ and let $T\colon V\rightarrow V$ be linear.  Then, there is a unique monic polynomial $p_{\min ,T}$\index[notation]{$p_{\min ,T}$}, the \term{minimal polynomial}\index{Minimal polynomial} of $T$, such that
	\begin{enumerate}
		\item \label{MinimalPolynomial(i)}$p_{\min ,T}(T)=0$; and
		\item \label{MinimalPolynomial(ii)}if $p(T)=0$ for $p\in \F [x]$ nonzero, then $\deg (p_{\min ,T})\leq \deg (p)$.
	\end{enumerate}
	Furthermore, for $p\in \F [x]$, $p(T)=0$ iff there is some $q\in \F [x]$ such that
	\begin{equation}\label{eqn4.6.2}
		p=qp_{\min ,T}.
	\end{equation}
	\begin{rmk}
		Recall that (\cref{PolynomialAlgebra}) a monic polynomial is a polynomial whose leading coefficient is $1$.
	\end{rmk}
	\begin{rmk}
		The proof makes use of the dimension of $\End _{\Vect _{\F}}(V)$, and in order for that to be a vector space, we require $\F$ to be a field---see \cref{sss1.1.2,prp3.2.144}.
	\end{rmk}
	\begin{rmk}
		For the minority that care, I believe this can be generalized to $\F$ a centrally-finite division ring.  In this case, the result would be true verbatim, except that in \eqref{eqn4.6.2} the left and right quotients might be distinct.
	\end{rmk}
	\begin{proof}
		\Step{Establish existence}
		Regard $V$ as a $\K [x]$-module as in \cref{exm1.1.34}.  By \cref{prpD.1.3}, this is equivalent to the specification of a ring homomorphism $\rho \colon \K [x]\rightarrow \End _{\Grp}(V)$, precisely,
		\begin{equation}
			\rho (p)\ceqq p(T),
		\end{equation}
		that is, $\rho (p)$ is the linear operator $p(T)\colon V\rightarrow V$.  By \cref{prpC.1.2},
		\begin{equation}\label{eqn4.6.3}
			\Ker (\rho )\ceqq \left\{ p\in \F [x]:p(T)=0\right\}
		\end{equation}
		is an ideal in $\F [x]$.  In other words, $\Ker (\rho )$ is exactly the set of polynomials which satisfy our desired property \cref{MinimalPolynomial(i)}.
		
		We first check that $\Ker (\rho )$ is nonzero.  To see this, it suffices to prove that there is some polynomial $p$ such that $p(T)=0$.  However, consider the following set in $\End _{\Vect _{\F}}(V)$.
		\begin{equation}
			\left\{ 1,T,\ldots ,T^{\dim (V)^2-1},T^{\dim (V)^2}\right\}
		\end{equation}
		This is a set of $\dim (V)^2+1$ elements in $\End _{\Vect _{\F}}(V)$.  However, by \cref{prp3.2.144}, we know that $\dim (\End _{\Vect _{\F}}(V))=\dim (V)^2$, and so we must have a linear-dependence relation:
		\begin{equation}
			\alpha _0\cdot 1+\alpha _1\cdot T+\cdots +\alpha _{\dim (V)^2-1}\cdot T^{\dim (V)^2-1}+\alpha _{\dim (V)^2}\cdot T^{\dim (V)^2}=0.
		\end{equation}
		This of course gives us a polynomial $p$ such that $p(T)=0$.
		
		Now, by \cref{PolynomialsArePIDs}, $\F [x]$ is both a left and right PIR, and so as $\Ker (\rho )\neq 0$, there is some $p_0\in \F [x]$ such that
		\begin{equation}
			\Ker (\rho )=\F [x]p_0.
		\end{equation}
		Scaling $p_0$ if necessary, we may without loss of generality assume that it is monic---call the resulting polynomial $p_{\min ,T}$, so that $p_{\min ,T}$ is monic and
		\begin{equation}
			\Ker (\rho )=\F [x]p_{\min ,T}.
		\end{equation}
		
		Let $p\in \F [x]$ be another nonzero polynomial such that $p(T)=0$.  Then, by \eqref{eqn4.6.3}, $p\in \Ker (\rho )=\F [x]p_{\min ,T}$.  That is, there is some $q\in \F [x]$ such that $p=qp_{\min ,T}$, which implies that $\deg (p)=\deg (q)+\deg (p_{\min ,T})\geq \deg (p_{\min ,T})$ (as $q\geq 0$).  This establishes \cref{MinimalPolynomial(ii)}.
		
		\Step{Establish uniqueness}
		To see uniqueness, let $q\in \F [x]$ be another monic polynomial of minimum degree such that $q(T)=0$.  Then, $p_{\min ,T}-q$ is a polynomial of degree strictly less than $\deg (p_{\min ,T})=\deg (q)$\footnote{The leading terms cancel as their coefficients are both $1$.} and $[p_{\min ,T}-q](T)\ceqq p_{\min ,T}(T)-q(T)=0$.  According to \cref{MinimalPolynomial(ii)}, the only way this can happen is if $p_{\min ,T}-q=0$, that is $q=p_{\min ,T}$.
		
		\Step{Establish ``Furthermore\textellipsis ''}
		The existence of $q_L$ above follows from the fact that $\Ker (\rho )=\F [x]p_{\min ,T}$.  To obtain this, we used the fact that $\F [x]$ is a \emph{left} PIR, but we haven't yet used the fact that it is a \emph{right} PIR.  Using this, we see that there is some $p_0\in \Ker (\rho )$ such that $\Ker (\rho )=p_0\F [x]$.  Again, by scaling if necessary, we may without loss of generality assume that $p_0$ is monic.  Then, preceding as before, we verify that $p_0$ satisfies \cref{MinimalPolynomial(i),MinimalPolynomial(ii)}.  Thus, by uniqueness, we have $p_0=p_{\min ,T}$, and hence that $\Ker (\rho )=p_{\min ,T}\F [x]$, establishing the existence of $q_R$.
	\end{proof}
\end{thm}
Our first goal is to actually prove that the eigenvalues are precisely the roots of $p_{\min ,T}$.
\begin{thm}{Root of $p_{\min ,T}$ are the eigenvalues}{}
	Let $V$ be a a finite-dimensional vector space over a field $\F$, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \F$.  Then, $\lambda$ is an eigenvalue of $T$ iff $p_{\min ,T}(\lambda )=0$.
	\begin{rmk}
		Warning:  Keep in mind the ground field.  For example, suppose that the ground field is $\F$ and that $p_{\min ,T}(x)=x^2+1$.  Don't think ``Oh, $\im$ and $-\im$ are roots of $x^2+1$, and therefore they are eigenvalues of $T$.''.  Nooooooo.  Over the \emph{reals} they are neither roots of $x^2+1$ nor eigenvalues.
	\end{rmk}
	\begin{proof}
		$(\Rightarrow )$ Suppose that $\lambda$ is an eigenvalue of $T$.  Then (\cref{crl4.2.36}) $p(\lambda )=0$ for every $p\in \F [x]$ such that $p(T)=0$.  As the minimal polynomial certainly satisfies this property, we have that $p_{\min ,T}(\lambda )=0$.
		
		\blni
		$(\Leftarrow )$ Suppose that $p_{\min ,T}(\lambda )=0$.  It follows that (\cref{prp4.4.174}) $p_{\min ,T}(x)=(x-\lambda )q(x)$ for some $q\in \F [x]$.  Plugging in $T$, we find
		\begin{equation}
			0=p_{\min ,T}(T)=[T-\lambda ]\circ q(T).
		\end{equation}
		Note that $\deg (q)=\deg (p_{\min ,T})-1$, and so by the definition of the minimal polynomial, it \emph{cannot} be the case that $q(T)=0$.  Thus, there is some $v\in V$ such that $[q(T)](v)\neq 0$, in which case we have
		\begin{equation}
			0=[T-\lambda ]([q(T)](v)),
		\end{equation}
		that is, $[q(T)](v)$ is an eigenvector of $T$ with eigenvalue $\lambda$.  In particular, $\lambda$ is an eigenvalue of $T$.
	\end{proof}
\end{thm}

Awesome, right?  Just compute $p_{\min ,T}$ and find its roots.  Done.

Uhm, sure.  In principle, yes, in practice no.  To see why this is pretty impractical (at least for computation), let's actually see how one might compute $p_{\min ,T}$.  The basic idea is as follows

Compute $T$.\footnote{By which I mean, do literally nothing.}  Is there a linear-dependence relation between $1$ and $T$?  If so, great!  That linear-dependence relation (upon scaling so that it's monic) gives you your minimal polynomial.  If not, compute $T^2$.  Is there a linear-dependence relation between $1$, $T$, and $T^2$?  If so, great!  That gives you the polynomial.  Now compute $T^3$.  And so on.

Even as I describe this, it probably doesn't sound all that efficient.  In any case, let's see this in action for real.
\begin{exm}{}{exm4.5.17}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}.
	\end{equation}
	Hopefully it should be clear that there is no linear-dependence relation between this an the identity.  Computation shows
	\begin{equation}
		A^2=\begin{bmatrix}7 & 10 \\ 15 & 22\end{bmatrix}.
	\end{equation}
	We how check for a linear-dependence relation among $1,A,A^2$:
	\begin{equation}
		0=\alpha _1+\alpha _2A+\alpha _3A^2=\begin{bmatrix}\alpha _1+\alpha _2+7\alpha _3 & 2\alpha _2+10\alpha _3 \\ 3\alpha _2+15\alpha _3 & \alpha _1+4\alpha _2+22\alpha _3\end{bmatrix}.
	\end{equation}
	This gives us $4$ equations in $\alpha _1,\alpha _2,\alpha _3$.  If it has a solution, we have our linear-dependence relation.  Otherwise, onto $A^3$.\footnote{Pro-tip:  You'll only ever need to go as high as $\dim (V)$, so without doing anything, I know there's going to be a solution.  We'll learn this when we cover the Cayley-Hamilton Theorem.}
	
	Setting up the equations and using our ninja-level row-reduction skills\footnote{Or Mathematica.}, we find that
	\begin{equation}
		\begin{bmatrix}\alpha _1 \\ \alpha _2 \\ \alpha _3\end{bmatrix}=\alpha _3\begin{bmatrix}-2 \\ -5 \\ 1\end{bmatrix}.
	\end{equation}
	Any $\alpha _3$ will give us a linear-dependence relation, but as we want the polynomial to be monic, we take $\alpha _3=1$.  Thus,
	\begin{equation}
		p_{\min ,T}(x)=x^2-5x-2.
	\end{equation}
	
	Using the quadratic formula, you can now compute the eigenvalues of $T$.\footnote{Though they involve icky radicals so I won't reproduce them here.  I trust you can solve quadratic equations on your own, yeah?}
	
	\horizontalrule
	
	Let's compare this with how one would compute the eigenvalues straight from the definition.  $\lambda \in \C$ is going to be an eigenvalue iff $\Null (A-\lambda )\neq 0$.  To calculate the null space of $A-\lambda$, we row-reduce.  A calculation shows that this matrix is row-equivalent to
	\begin{equation}
		\begin{bmatrix}3 & 4-\lambda \\ 0 & \tfrac{1}{3}(2+5\lambda -\lambda ^2)\end{bmatrix}.
	\end{equation}
	This null space is nonzero iff it has a free variable, which we see in this case is true iff $2+5\lambda -\lambda ^2=0$.
	
	Of the two methods, I think the direct row-reduction method is by far the better option.  The strength of the minimal polynomial I think is more theoretical.  For example, its mere existence tells us that in principle calculating eigenvalues is going to come down to solving a polynomial equation, even if in practice it's not usually the best idea to use the minimal polynomial to calculate them.
\end{exm}

So, yes, certainly, one can compute $p_{\min ,T}$ for the purpose of computing the eigenvalues, but if that's all it could do, it wouldn't be of much use.  Of course, it has some theoretical uses,\footnote{See, for example, \cref{semisimple_linear_operator}.} but at the moment I would like to pay particular attention to what information the minimal polynomial gives you about the Jordan canonical form.  To best understand this, I think it's helpful to first see what the minimal polynomial of matrices in Jordan canonical form are.
\begin{prp}{The minimal polynomial of a matrix in Jordan canonical form}{MinimalJordan}
	Define
	\begin{equation}
		A\ceqq \bigoplus _{\lambda \in \Eig (A)}\left[ \Jord _{\lambda ,n_{\lambda ,1}}\oplus \cdots \oplus \Jord _{\lambda ,n_{\lambda ,m_{\lambda}}}\right] ,
	\end{equation}
	entries taken to be in an algebraically closed field, and for each $\lambda \in \Eig (A)$ write
	\begin{equation}
		n_{\lambda}\ceqq \max \{ n_{\lambda ,1},\ldots ,n_{\lambda ,m_{\lambda}}\} .
	\end{equation}
	Then,
	\begin{equation}
		p_{\min ,A}(x)=\prod _{\lambda \in \Eig (T)}(x-\lambda )^{n_{\lambda}}.
	\end{equation}
	\begin{rmk}
		In words, the minimal polynomial of $A$ has a factor $x-\lambda$ for each eigenvalue $\lambda$ of $A$, and the power of the factor is the size of the largest Jordan block with that eigenvalue.
	\end{rmk}
	\begin{rmk}
		In particular, this tells us what the minimal polynomial of single Jordan blocks are.
		\begin{equation}
			p_{\min ,\Jord _{\lambda ,m}}(x)=(x-\lambda )^m.
		\end{equation}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exm}{}{}
	The minimal polynomial of
	\begin{equation}
		A\ceqq \Jord _{3,2}\oplus \Jord _{4,1}\oplus \Jord _{-2,2}=\begin{bmatrix}3 & 1 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 & 0 \\ 0 & 0 & 4 & 0 & 0 \\ 0 & 0 & 0 & -2 & 1 \\ 0 & 0 & 0 & 0 & -2\end{bmatrix}
	\end{equation}
	is
	\begin{equation}
		p_{\min ,A}(x)=(x-3)^2(x-4)(x+2)^2.
	\end{equation}
	
	Similarly, the minimal polynomial of
	\begin{equation}
		B\ceqq \Jord _{5,3}\oplus \Jord _{-3,2}=\begin{bmatrix}5 & 1 & 0 & 0 & 0 \\ 0 & 5 & 1 & 0 & 0 \\ 0 & 0 & 5 & 0 & 0 \\ 0 & 0 & 0 & -3 & 1 \\ 0 & 0 & 0 & 0 & -3\end{bmatrix}
	\end{equation}
	is
	\begin{equation}
		p_{\min ,B}(x)=(x-5)^3(x+3)^2.
	\end{equation}
	
	Finally, the minimal polynomial of
	\begin{equation}
		C\ceqq \Jord _{4,2}\oplus \Jord _{4,2}\oplus \Jord _{4,1}\ceqq \begin{bmatrix}4 & 1 & 0 & 0 & 0 \\ 0 & 4 & 0 & 0 & 0 \\ 0 & 0 & 4 & 1 & 0 \\ 0 & 0 & 0 & 4 & 0 \\ 0 & 0 & 0 & 0 & 4\end{bmatrix}.
	\end{equation}
	is
	\begin{equation}
		p_{\min ,C}(x)=(x-4)^2.
	\end{equation}
	\begin{rmk}
		Note how you only have a power of $2$ here.  This power of $2$ corresponds to the size of the largest Jordan block and nothing else.  You can have a $124398217958732987$ dimensional matrix, all with one eigenvalue $\lambda$, but if all those Jordan blocks are just size $1$, the minimal polynomial is still going to be just $x-\lambda$.
	\end{rmk}
\end{exm}
Having examined the minimal polynomial for matrices in Jordan canonical form, let's turn things around and see what information the minimal polynomial can tell us about a linear operator's as of yet unknown Jordan canonical form.  In fact, let's just discuss how one might compute the Jordan canonical form (and the corresponding Jordan basis) in general.

Of course, we're going to start by computing the eigenvalues.\footnote{When we learn about the characteristic equation later one, we'll see that every eigenvalue has an \emph{algebraic multiplicity}, and that the the dimension of the generalized-eigenspace coincides with that multiplicity.  Thus, the algebraic multiplicity tells you exactly how many linearly-independent generalized-eigenvectors you need to find before you know you're `done'.}  For a fixed eigenvalue $\lambda$, find a basis for $\Ker (T-\lambda )$.  If you have `enough',\footnote{By ``enough'' I mean the number of elements in this basis is already equal to the algebraic multiplicity.} stop.  Otherwise, compute a basis for $\Ker ([T-\lambda ^2])$ by \emph{extending} your basis of $\Ker (T-\lambda )$---don't pick an entirely new basis.  Again, if you have ``enough'', stop.  Otherwise, extend your basis of $\Ker ([T-\lambda ]^2)$ to a basis of $\Ker ([T-\lambda ]^3)$, and so on.  When you finally \emph{do} stop, pick one of the `newest' vectors that you just added to your basis at the last step.  Call that guy $v_0$.  $v_0$ is \emph{$[T-\lambda ]$-maximal}.  So, apply $T-\lambda$ to $v_0$ over and over until you get $0$:  $[T-\lambda ]^m(v_0),\ldots ,[T-\lambda ](v_0),v_0$.  These vectors will form part of your final basis.  Continue finding $[T-\lambda ]$-maximal vectors in this way (for every eigenvalue) until you have a number of vectors equal to the dimension of your vector space.  The matrix of $T$ with respect to this basis will be in Jordan Canonical Form.\footnote{Perhaps its worth mentioning that the matrix in any basis of generalized-eigenvectors will be relatively simple, but if you want the matrix to be exactly in Jordan Canonical Form, you can't just use any basis of generalized-eigenvectors---you have to do what was just explained previously.}
\begin{exm}{A computation of the Jordan Canonical form}{}
	Define
	\begin{equation}
	A\ceqq \begin{bmatrix}5 & 4 & 2 & 1 \\ 0 & 1 & -1 & -1 \\ -1 & -1 & 3 & 0 \\ 1 & 1 & -1 & 2\end{bmatrix}.
	\end{equation}
	Proceeding as in the previous example \cref{exm4.5.17}, we find that the minimal polynomial is
	\begin{equation}\label{eqn4.5.20}
		p_{\min ,A}(x)=x^4-11x^3+42x^2-64x+32=(x-1)(x-2)(x-4)^2.\footnote{Yes, this computation is exceptionally tedious to do by hand.}
	\end{equation}
	
	Thus, we see that the eigenvalues are $\lambda _1\ceqq 1$, $\lambda _2\ceqq 2$, and $\lambda _3\ceqq 4$.  Furthermore, by virtue of \cref{MinimalJordan}, we know that the largest $\lambda =1$ Jordan block has size $1$, the largest $\lambda =2$ Jordan block has size $1$, and the largest $\lambda =4$ Jordan block has size $2$.  For a $4\times 4$ matrix, there is only one way for this to happen:
	\begin{equation}\label{eqn4.5.21}
		\Jord _{4,2}\oplus \Jord _{2,1}\oplus \Jord _{1,1}\ceqq \begin{bmatrix}4 & 1 & 0 & 0 \\ 0 & 4 & 0 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 1\end{bmatrix}.
	\end{equation}
	Thus, from the minimal polynomial alone, we know this \emph{must} be the Jordan canonical form.
	
	Sometimes this will be enough, but often we will not only want to know the Jordan canonical form itself, but also it's relationship to the original linear-transformation.  According to \cref{MatrixJordan}, this means we need to compute a Jordan basis for $A$.  We start by looking at the generalized-eigenspaces.
	
	Row-reducing $A-1$, we obtain the matrix
	\begin{equation}
		\begin{bmatrix}1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0\end{bmatrix}.
	\end{equation}
	Therefore,
	\begin{equation}
		\Null (A-1)=\Span \left( \begin{bmatrix} -1 \\ 1 \\ 0 \\ 0\end{bmatrix}\right) .
	\end{equation}
	We already know from \eqref{eqn4.5.21} that $\dim (\Eig _1^{\infty})=1$, and so we're done for the $\lambda =1$ generalized-eigenspace.
	
	As for $\lambda =2$, row-reducing $A-2$ yields
	\begin{equation}
		\begin{bmatrix}1 & 0 & 0 & -1 \\ 0 & 1 & 0 & 1 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0\end{bmatrix},
	\end{equation}
	and hence
	\begin{equation}
		\Null (A-2)=\Span \left( \begin{bmatrix}1 \\ -1 \\ 0 \\ 1\end{bmatrix}\right) .
	\end{equation}
	As for $\lambda =1$, \eqref{eqn4.5.21} tells us that we're done for $\lambda =2$.
	
	Finally, row-reducing $A-4$ yields
	\begin{equation}
		\begin{bmatrix}1 & 0 & 0 & -1 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0\end{bmatrix},
	\end{equation}
	and hence
	\begin{equation}\label{eqn4.5.27}
		\Null (A-4)=\Span \left( \begin{bmatrix}1 \\ 0 \\ -1 \\ 1\end{bmatrix}\right) .
	\end{equation}
	We're still not done however, as \eqref{eqn4.5.21} tells us that we need to have at least one generalized-eigenvector of rank $2$.\footnote{In fact, it's \emph{important to note} that one could deduce this from the minimal polynomial itself---the existence of the power of $2$ in $(x-4)^2$ tells us that the maximum size Jordan block is size $2$, and in particular, there must be at least one Jordan block of size $2$.}  Thus, in fact we need to compute $\Null ([A-4]^2)$.  Row-reducing $(A-4)^2$ yields
	\begin{equation}
		\begin{bmatrix}0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{bmatrix},
	\end{equation}
	so that
	\begin{equation}\label{eqn4.5.29}
		\Null ([A-4]^2)=\Span \left( \begin{bmatrix}1 \\ 0 \\ 0 \\ 0\end{bmatrix},\begin{bmatrix}0 \\ 0 \\ -1 \\ 1\end{bmatrix}\right) .
	\end{equation}
	Okay, now, \emph{pay attention}!
	
	We might be tempted to take
	\begin{equation}
		\basis{B}\ceqq \left\{ \begin{bmatrix}-1 \\ 1 \\ 0 \\ 0\end{bmatrix},\begin{bmatrix}1 \\ -1 \\ 0 \\ 1\end{bmatrix},\begin{bmatrix}1 \\ 0 \\ 0 \\ 0\end{bmatrix},\begin{bmatrix}0 \\ 0 \\ -1 \\ 1\end{bmatrix}\right\} ,
	\end{equation}
	but this is \emph{wrong}.  If $P$ is the matrix whose columns are these vectors, then we find that\footnote{Recall that (\cref{MatrixJordan}) the resulting matrix should be the Jordan canonical form of $A$, but, of course, it's not.}
	\begin{equation}
		P^{-1}AP=\begin{bmatrix}1 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 5 & -1 \\ 0 & 0 & 1 & 3\end{bmatrix}.
	\end{equation}
	Oopsies.  Something went wrong.
	
	Note, however, that the $\lambda =1$ and $\lambda =2$ parts \emph{did} come out as they should have.  Remember that we can just use any old basis of generalized-eigenvectors.  For the $\lambda =4$ generalized-eigenspace, instead what we want to do is find a rank $2$ generalized-eigenvector $v_0$, in which case $[A-4](v_0)$ will be the other $\lambda =4$ generalized-eigenvector going into our basis.  \eqref{eqn4.5.29} describe the space of generalized-eigenvectors of rank \emph{at most $2$} and we want one with rank \emph{exactly $2$}, so we have to be a bit careful.  Basically, compare the rank $1$ generalized-eigenspace \eqref{eqn4.5.27} and the rank $2$ generalized-eigenspace \eqref{eqn4.5.29} and pick something that is in the latter but not the former.  Inspection reveals that
	\begin{equation}
		\begin{bmatrix}1 \\ 0 \\ 0 \\ 0\end{bmatrix}
	\end{equation}
	works, in which case the other element we want to use for the $\lambda =4$ generalized-eigenspace is
	\begin{equation}
		[A-4](\coord{1,0,0,0})=\begin{bmatrix}1 \\ 0 \\ -1 \\ 1\end{bmatrix}.
	\end{equation}
	
	Now, let's try this again and use the basis\footnote{Note the order of $\coord{1,0,0,0}$ and $[A-4](\coord{1,0,0,0})$.  For practical reasons, when doing the computation, it is important that you place them \emph{in this order} (from lowest rank to highest).}
	\begin{equation}
		\basis{B}\ceqq \left\{ \begin{bmatrix}-1 \\ 1 \\ 0 \\ 0\end{bmatrix},\begin{bmatrix}1 \\ -1 \\ 0 \\ 1\end{bmatrix},\begin{bmatrix}1 \\ 0 \\ -1 \\ 1\end{bmatrix},\begin{bmatrix}1 \\ 0 \\ 0 \\ 0\end{bmatrix}\right\} .
	\end{equation}
	Again, let $P$ be the matrix whose columns are these basis elements.  This time, we find
	\begin{equation}
		P^{-1}AP=\begin{bmatrix}1 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 4 & 1 \\ 0 & 0 & 0 & 4\end{bmatrix},
	\end{equation}
	as desired.
\end{exm}

\section{The Jordan-Chevalley Decomposition}

The Jordan-Chevalley Decomposition is what I would consider a variant of the Jordan Canonical Form decomposition.  Essentially it says that we can write any linear operator $T$ as a sum
\begin{equation}
	T=D+N,
\end{equation}
where $D$ is ``diagonalizable''\footnote{In quotes because this may not literally be true.  Instead, what is true is that, if it does fail to be diagonalizable, it only does so because the ground field doesn't contain all the eigenvalues of $T$.} and $N$ is nilpotent.  For example,
\begin{equation}
	\Jord _{4,3}\ceqq \begin{bmatrix}4 & 1 & 0 \\ 0 & 4 & 1 \\ 0 & 0 & 4\end{bmatrix}=\begin{bmatrix}4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 4\end{bmatrix}+\begin{bmatrix}0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0\end{bmatrix}
\end{equation}
is the Jordan-Chevalley decomposition of $\Jord _{4,3}$.  Similarly,
\begin{equation}
	\begin{split}
		\MoveEqLeft
		\Jord _{5,2}\oplus \Jord _{-3,3}\ceqq \begin{bmatrix}5 & 1 & 0 & 0 & 0 \\ 0 & 5 & 0 & 0 & 0 \\ 0 & 0 & -2 & 1 & 0 \\ 0 & 0 & 0 & -2 & 1 \\ 0 & 0 & 0 & 0 & -2\end{bmatrix} \\
		& =\begin{bmatrix}5 & 0 & 0 & 0 & 0 \\ 0 & 5 & 0 & 0 & 0 \\ 0 & 0 & -2 & 0 & 0 \\ 0 & 0 & 0 & -2 & 0 \\ 0 & 0 & 0 & 0 & -2\end{bmatrix}+\begin{bmatrix}0 & 1 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 & 0\end{bmatrix}
	\end{split}
\end{equation}
is the Jordan-Chevalley decomposition of $\Jord _{5,2}\oplus \Jord _{-3,3}$.  The underlying intuition is that, because, in a suitable basis, any linear operator $T$ is a direct-sum of Jordan blocks, we can use this trick to write $T$ itself as a sum of a ``diagonalizable'' linear operator and a nilpotent one.

The Jordan-Chevalley Decomposition can perhaps be thought of as an alternative solution to the ``Not every linear operator is diagonalizable problem.''.  It says that every linear operator is the sum of a ``diagonalizable'' operator and a ``small'' (i.e.~nilpotent one).  We briefly mentioned this before, and because it's so closely related to Jordan canonical form, I think it's probably more accurate to say the Jordan-Chevalley Decomposition is another way of looking at the same solution to the diagonalization problem.

The Jordan-Chevalley Decomposition has at least two advantages over the Jordan Canonical Form Theorem.  The first is that it requires weaker hypotheses.  For example, the Jordan-Chevalley Decomposition will work over $\R$ where the Jordan Canonical Form Theorem did not.  Secondly, the statement of the result is a bit `cleaner'.  In particular, you can state the result with no mention of bases or matrices.  The downside is that it's not quite as strong as the Jordan Canonical Form Theorem---all it tells you is that $N$ is nilpotent, whereas the Jordan Canonical Form Theorem says that you can choose a basis in which $N$ takes a particularly nice form.  Despite this, quite often one doesn't need this extra strength, and the Jordan-Chevalley Decomposition proves to be more convenient.

Anyways, let's get to the result.
\begin{thm}{Jordan-Chevalley Decomposition}{JordanChevalleyDecomposition}
	Let $V$ be a finite-dimensional vector space over a perfect field and let $T\colon V\rightarrow V$ be linear.  Then, there are unique linear operators $D,N\colon V\rightarrow V$ such that
	\begin{enumerate}
		\item $T=D+N$;
		\item $D$ is semisimple;
		\item $N$ is nilpotent; and
		\item $DN=ND$
	\end{enumerate}
	Furthermore, $D$ and $N$ are expressible as polynomials in $T$.
	\begin{rmk}
		Unless you have a particularly remarkable background for someone taking a linear algebra course, I would not expect you to have much intuition for the definition of a perfect field (\cref{PerfectField}).  But that's okay.  For us, what's important is that ``perfect'' is a sufficiently weak hypothesis that this theorem will apply in all cases of interest (see, for example, \cref{prpC.4.3}).  Essentially, don't worry about it---if you know enough math to be working with fields that are not perfect then you know enough math to know when you might have to worry.
	\end{rmk}
	\begin{rmk}
		In terms of intuition, you should think of ``semisimple'' as meaning ``diagonalizable''.  Indeed, \cref{semisimple_linear_operator} says in particular that being semisimple is equivalent to being diagonalizable when you `enlarge' your field to an algebraic closure, and in particular, semisimple is actually equivalent to being diagonalizable when working over an algebraically closed field.  The only reason it isn't literally the case that semisimple is the same as diagonalizable is simply because you may not have all your eigenvalues in the ground field you're working with.  For example,
		\begin{equation}
			\begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}
		\end{equation}
		is semisimple over $\R$ even though it is not diagonalizable over $\R$.  Indeed, \cref{semisimple_linear_operator} says that this is semisimple \emph{because} it is diagonalizable over $\C$ (an algebraic closure of $\R$).
	\end{rmk}
	\begin{rmk}
		Note that $T$ is ``diagonalizable'' iff $N=0$.  In this sense, you can view the nilpotent part of $T$ as an ``obstruction'' to being diagonalizable.  It is in this sense that nilpotency is the only reason linear-transformations can fail to be diagonalizable.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
\begin{exm}{A counter-example of the Jordan-Chevalley Decomposition over an imperfect field}{}
	Let $p\in \Z ^+$ be prime, write $\F _p\ceqq \Z /p\Z$, and define $\F \ceqq \F _p(t)$.  Note that $\F$ is imperfect by \cref{AnImperfectField}.  As $\F$ is imperfect (\cref{thmC.4.2}), there is some $a\in \F$ that is not of the form $a=b^p$.  Define $V\ceqq \F [x]/(x^p-a)$ and $T\colon V\rightarrow V$ by
	\begin{equation}
		T(f+(x^p-a))\ceqq xf(x)+(x^p-a).
	\end{equation}
	\begin{exr}[breakable=false]{}{}
		Show that $T$ fails the conclusion of the Jordan-Chevalley decomposition.
		\begin{rmk}
			Hint:  See \href{https://www.wikiwand.com/en/Jordan\%E2\%80\%93Chevalley_decomposition#/Counterexample}{Wikipedia}.
		\end{rmk}
	\end{exr}
\end{exm}

\section{Summary}

Okay, so that was a long chapter.  Fortunately, the key ideas you really need to take note of are less in number than the length of the chapter alone might suggest.

Linear algebra is the study of vector spaces and linear-transformations.  In the previous chapter, we learned that, after chooses bases, we can associate column-vectors to vectors and matrices to linear-transformations.  The objective of this chapter was then ``How do I pick a basis that makes the matrix as simple as possible?''.  The final answer to that is you pick a Jordan basis, in which the associated matrix will be a direct-sum of Jordan blocks.  And that's really it---if you learn nothing else from this course, remember the following.
\begin{displayquote}
	If $T\colon V\rightarrow V$ is a linear operator on a finite-dimensional vector space over an algebraically-closed field, then there is a basis $\basis{B}$ of $V$ such that $\coordinates{T}{\basis{B}}$ is a direct-sum of Jordan blocks.
\end{displayquote}

Essentially everything else in this chapter was either a build-up to this or very closely related to it.  Eigenspaces were introduced in an attempt to diagonalize matrices, which were later generalized to generalized-eigenspaces when we realized that didn't always work.  Direct-sums, coordinates with respect to direct-sum decompositions, invariant subspaces, indecomposable subspaces, and nilpotent operators were all part of the background necessary to either prove or state the \namerefpcref{GeneralizedEigenspaceDecomposition}.  The Jordan Canonical Form Theorem gave a characterization when two linear-transformations were similar.  The minimal polynomial was used as a tool to give information about the Jordan canonical form.  And finally The Jordan-Chevalley Decomposition was presented as what is essentially a variant of the Jordan Canonical Form Idea.

\subsection{What now?}

I regard the Jordan Canonical Form Theorem as the most important theorem in elementary linear algebra, and in some sense, everything we have done has been built up for the purpose of proving and stating it.  Now that we've accomplished that primary goal, what to do?

The next big topic will be that of multilinear algebra.  I'll leave for the next chapter the motivation for this, though for the moment let me mention that this content will included a discussion of the determinant, something that has thus far been noticeably absent.

Finally, we'll conclude with a study of inner-product spaces, which themselves are vector spaces equipped with additional structure.  As such, I think it makes sense to cover pretty much everything there is about vector spaces themselves before introducing extra structure.

Anyways, let's get started\textellipsis