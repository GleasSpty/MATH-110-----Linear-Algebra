\chapter{Basic set theory}

\section{What is a set?}\label{sbsA.1.1}

While set theory is of course not the object of study of these notes, a certain amount of basic knowledge about sets is necessary for the study of essentially any area of mathematics.  As this is possibly the first upper-division mathematics course you've taken, I cannot assume you know any set theory.  On the other hand, as it isn't of direct interest in its own right, the requisite material has been placed in this appendix.

Though at this level we're really talking about philosophy and I'm sure other mathematicians have different viewpoints than me on this, my understanding of the foundations of mathematics is as follows.  First of all, one cannot get something from nothing; if you want to be able to make deductions, you're going to have to assume certain things.  For example, if you don't accept basic logical truths (e.g., from ``$A$ implies $B$.'' and ``$A$'' one can deduce $B$ ), you're not going to get anywhere.  Similarly, if we don't accept the fact that it makes sense to give names to certain ideas so that we can later refer to them (as I just did, e.g. ``$A$''), then we're likewise not going to get very far.  However, if we do make very simple assumptions about the type of mental tools one is able to use when doing mathematics, one is able to deduce all sorts of wondrous things.  I thus ask that you that grant me (i) that naive logical deduction is valid; and (ii) that the naive concept of a `set' is valid.

When I say ``naive concept of a `set'{}'', I am referring to the idea that, if you have a collection of things, whatever they may be, you are allowed to give that collection of things a name (e.g.~``$X$''), and now $X$ is a new thing that we may talk about, the \emph{set} of the aforementioned things.  From this perspective, you might say that the naive notion of a set is more a linguistic tool than anything else, and in this sense is a special case of the idea mentioned in the previous paragraph that one should be allowed to assign names to ideas so that we may later refer to them.  Indeed, because of this, for the most part, we will completely ignore any set-theoretic concerns in these notes, but before we do just blatantly ignore any potential issues, we should first probably (attempt to) justify this dismissal.

Intuitively, a set is just a thing that `contains' a bunch of other things, but this itself is of course not a precise mathematical definition.  Ultimately, I claim there there is no need to have such a precise definition, but let's suppose for the moment that we would like to define what a set is.  One way to do this would be to attempt to develop an axiomatic set theory, but there is a certain `circularity' problem in doing this.

The term ``axiomatic set theory'' here refers to any collection of axioms which attempt to make precise the intuitive idea of a set.  In a given theory, however, the symbols which we make use of to write down the axioms themselves form a \emph{set}.  The point is that, in attempting to write down a mathematically precise definition of a set, one must make use of the naive notion of a set.

Of course this example might not be very convincing.  Why not just not think of all the symbols together and just think of them individually?  It is true that if you fudge things around a bit you may be able to convince yourself that you're not really making use of the naive notion of a set here.  That being said, even if you can convince yourself that you can get around the problem of first requiring a `set' of symbols, sooner or later, in attempting to make sense out of an axiomatic set theory, you will need to make use of the naive notion of a set.

Because of this, we consider the idea of a set to be so fundamental as to be undefinable, and we simply assume that we can freely work with this intuitive idea of a collection of things all thought of as one thing, namely a set.

One has to be careful however.  Naive set theory has paradoxes, a famous example of which is Russel's Paradox\index{Russel's Paradox}.  Consider for example the set\footnote{Hopefully you have seen notation like this before.  If not, really quickly skip ahead to \crefnameref{sbsA.1.2} to look up the meaning of this notation.}
\begin{equation}\label{A.1.1}
X\coloneqq \left\{ Y:Y\notin Y\right\} .
\end{equation}
Is $X\in X$?  One resolution of this paradox is that it is nonsensical to construct the set of \emph{all} things satisfying a certain property.  Whenever you construct a set in this manner, your objects have to be already `living inside' some other set.  For example, we can write
\begin{equation}
X\coloneqq \left\{ Y\in U:Y\notin Y\right\}
\end{equation}
for some fixed set $U$.\footnote{``$U$'' is for ``universe''.}  Russel's Paradox now becomes the statement that $X\notin U$.

This is still somehow not enough.  For example, if you turn to \cref{exm1.2.2}, the category of sets, you'll see that we do need to make use of the notion of the collection of ``all sets'', and we've just said that we are not allowed to quantify over \emph{everything}, but only over things that are elements of a fixed set.  One way to do this is to fix a set $U$ which is closed under all the usual operations of set theory,\footnote{One way in which to make this precise is what is called a \emph{Grothendieck universe}\index{Grothendieck universe}.  The details of this will not matter for us, but if you're curious feel free to Google the term.} and then to interpret statements that refer to something like ``All sets such that\textellipsis '' as in fact meaning ``All elements of $U$ such that\textellipsis ''.  Upon doing this, the construction involved in Russel's Paradox is perfectly valid, and indeed, does give us a new set, and the `paradox' itself now simply becomes the argument that this new set is not an element of $U$.\footnote{One nice thing about this approach of avoiding paradoxes is that \emph{everything} is still a set, that is, there is no need to make this awkward distinction between `actual' sets and what would be referred to as \emph{proper classes}
\index{Proper class}.}

The content of this section was meant only to convince you that (i)~there is no way of getting around the fact that the idea of collecting things together is undefinably fundamental, and that (ii)~ultimately this naive idea is not paradoxical.

Disclaimer:  I am neither a logician nor a set-theorist, so take what I say with a grain of salt.

\section{The absolute basics}\label{sbsA.1.2}

\subsection{Some comments on logical implication}

For us, the term \term{statement}\index{Statement} will refer to something that is either true or false.  The word \term{iff}\index{Iff} is short-hand for the phrase \emph{if and only if}.  So, for example, if $A$ and $B$ are statements, then the sentence ``$A$ iff $B$.'' is logically equivalent to the two sentences ``$A$ if $B$.'' and ``$A$ only if $B$.''.  In symbols, we write $B\Rightarrow A$\index[notation]{$B\Rightarrow A$} and $A\Rightarrow B$\index[notation]{$A\Rightarrow B$} respectively.  The former logical implication is perhaps more obvious; the other might be slightly trickier to translate from the English to the mathematics.  The way you might think about it is this:  if $A$ is true, then, because $A$ is true \emph{only if} $B$ is true, it must have been the case that $B$ was true too.  Thus, ``$A$ only if $B$.'' is logically equivalent to ``$A$ implies $B$.''.  We then write ``$A\Leftrightarrow B$\index[notation]{$A\Leftrightarrow B$} as alternative notation for the English ``$A$ iff $B$''.

If $A$ and $B$ are statements, then $A\Rightarrow B$ is a statement:  $\text{True}\Rightarrow \text{True}$ is considered true, $\text{True}\Rightarrow \text{False}$ is considered false, $\text{False}\Rightarrow \text{True}$ is considered true, and $\text{False}\Rightarrow \text{False}$ is considered true.  Hopefully the first two of these make sense, but how does one understand why it should be the case that $\text{False}\Rightarrow \text{True}$ is true?  To see this, I think it helps to first note the following.\footnote{The symbol ``$\forall$\index[notation]{$\forall$}'' in English reads ``for all''.  Similarly, the symbol ``$\exists$\index[notation]{$\exists$}'' is read as ``there exists''.}
\begin{textequation}[A.1.3]
``$\forall x\in X,\ \mcal{P}(x)$.'' is logically equivalent to ``$x\in X\Rightarrow \mcal{P}(x)$.'',
\end{textequation}
where $\mcal{P}(x)$ is a statement that depends on $x$.

Now consider the following example in English.
\begin{textequation}
Every pig on Mars owns a shotgun.
\end{textequation}
Is this statement true or false?  Under the (hopefully legitimate assumption) that there is no pig on Mars at all, my best guess is that most native English speakers would say that this is a true statement.  In any case, this is mathematics, not linguistics, and for the sake of definiteness, we simply declare a statement such as this to be \emph{vacuously true} (unless of course there are pigs on Mars, in which case we would need to determine if they all owned shotguns).  This example is meant to convince you that, in the case that $X$ is empty, it is reasonable to declare the statement $\forall x\in X,P(x)$ to be true for tautological reasons.

Now, appealing back to \eqref{A.1.3}, hopefully it now also seems reasonable to declare statements of the form $\text{False}\Rightarrow B$ to be true (where $B$ is any statement), likewise for tautological reasons.

If we know a certain statement to be true, there are several other statements that we know automatically to be true.  For example, if $A$ is true, then $\neg \neg A$ is automatically true.\footnote{$\neg A$\index[notation]{$\neg A$}, read ``not $A$``, is a statement which is false if $A$ and is true if $A$ is false.}  Another important example of this is given by the \emph{contrapositive}.
\begin{dfn}{Converse, inverse, and contrapositive}{Converse}
Let $A$ and $B$ be statements.  Then,
\begin{enumerate}
\item the \term{converse}\index{Converse} of the statement $A\Rightarrow B$ is the statement $B\Rightarrow A$;
\item the \term{inverse}\index{Inverse} of the statement $A\Rightarrow B$ is $\neg A\rightarrow \neg B$; and
\item the \term{contrapositive}\index{Contrapositive} of the statement $A\Rightarrow B$ is $\neg B\rightarrow \neg A$.
\end{enumerate}
\begin{rmk}
Referring back to our earlier comments on the phrase ``iff'', if ever you want to prove ``$A$ iff $B$'', you must prove $A\Rightarrow B$ (i.e.~``$A$ only if $B$.'') as well as its \emph{converse}, $B\Rightarrow A$ (i.e.~$A$ if $B$).
\end{rmk}
\end{dfn}
\begin{prp}{}{prpA.2.4}
Let $A$ and $B$ be statements.  Then, $A\Rightarrow B$ is true iff its contrapositive $\neg B\Rightarrow \neg A$ is true.
\begin{proof}
$(\Rightarrow )$ Suppose that $A\Rightarrow B$ is true.  We would like to show that $\neg B\Rightarrow \neg A$.  So, suppose that $\neg B$ is true.  We would then like to prove $\neg A$.  We proceed by contradiction:  suppose that $A$ is true.  Then, as $A\Rightarrow B$, it must be that $B$ is true:  a contradiction of the fact that we have assumed that $\neg B$ is true.  Therefore, our assumption that $A$ is true must have been false.  Thus, it must be that $\neg A$ is true.

\blankline
\noindent
$(\Leftarrow )$ As the contrapositive of the contrapositive is the original statement, this follows from $(\Rightarrow )$.
\end{proof}
\end{prp}
\begin{exm}{}{}
Let $P$ be the statement ``If it is raining, then it is wet.''.

The converse of $P$ is ``If it is wet, then it is raining.''.

The inverse of $P$ is ``If it is not raining, then it is not wet.''.

The contrapositive of $P$ is ``If it is not wet, then it is not raining.

Hopefully this makes it clear how the converse can be false even if the original statement is true.  Also be sure to understand in this example how the contrapositive is indeed equivalent to the original statement.
\end{exm}

Given that a statement is true iff its contrapositive is true, it is important to know how to correctly negate statements (and of course this is important to know for other reasons as well).
\begin{exm}{}{}
Let $\mcal{P}(x)$ be a statement that depends on $x$.
\begin{enumerate}
\item ``$\neg (\forall x,\ \mcal{P}(x))$'' is equivalent to ``$\exists x,\ \neg \mcal{P}(x)$''.
\item ``$\neg (\exists x,\ \mcal{P}(x))$'' is equivalent to ``$\forall x,\ \neg \mcal{P}(x)$''.
\item ``$\neg (A\text{ and }B)$'' is equivalent to ``$\neg A\text{ or }\neg B$''.
\item ``$\neg (A\text{ or }B)$'' is equivalent to ``$\neg A\text{ and }\neg B$''.
\end{enumerate}
\begin{rmk}
For example, suppose you want to prove the statement ``Every positive integer is even.'' is \emph{false}.  To do this, you want to exhibit a positive integer which is not even.  Explicitly, the original statement is ``$\forall x\in \Z ^+,\ x\text{ is even.}$'', and so its negation is ``$\exists x\in \Z ^+,\ x\text{ is not even.}$''.  For some reason, this tends to trip students up when I ask them to show that a statement is false:  to prove that statements of this form\footnote{That is, of the form ``$\forall x,\ \mcal{P}(x)$''.  Of course, not every statement is of this form, and so proving a statement is false doesn't necessarily mean you have to give a counter-example (for example, if I ask you to prove that $\abs{\N}=\abs{\R}$ is false, it would not make sense to give a counter-example).} are false, you \emph{must} exhibit a counter-example---explaining why a counter-example should exist, without \emph{proving}\footnote{It is almost always the case that the easiest way to prove a counter-example exists is simply to write one down.} one exists, is not enough.  For example, don't say ``The statement ``Every partially-ordered set is totally-ordered.'' is false because there is an extra condition in the definition of totally-ordered.''---in this case, you \emph{must} give an example of a partially-ordered set which is not totally-ordered.\footnote{See \cref{dfnA.1.24,TotalOrder}.}
\end{rmk}
\end{exm}

\subsection{A bit about proofs}\label{sbsABitAboutProofs}

Proofs are absolutely fundamental to mathematics.  Indeed, you might say that mathematics \emph{is} the study of those truths which are provable.\footnote{In contrast to those truths are which true by observation.  For example, while the statements ``$x\in \R$ implies $x^2\geq 0$.'' and ``The mass of the electron is $\SI{9.10938356(11)e-31}{\kilogram}$.'' are both true, they are true in fundamentally different ways---the former is true because we can prove it and the latter is true because we measure it.}  But what actually \emph{is} a proof?

A proof is essentially just a particularly detailed argument that a statement is true.  The question then is ``How much detail?''.  Well, an extremist might say that a proof should be detailed enough so as to be verifiable by a computer---if a computer can verify it using axioms alone, then there can be no doubt at all as to the truth of the statement.  Doing this in practice, however, well, would be a little bit insane---no one (or almost no one) writes proofs in this amount of detail.

The objective then I would say it to provide enough detail so as to convince \emph{your target audience} that enough detail could be filled in, at least \emph{in principle}, so as to be verified by a computer, if a member of your target audience really wanted to take (waste?) their time doing so.  This is why two different proofs of the same statement, one several pages long and another a paragraph long, can both be considered equally valid proofs:  one proof could have been written to be accessible to undergraduates and the other to be accessible to professional mathematicians.  As a student, however, I would recommend you consider your target audience to be \emph{yourself}.  You should put down enough detail so that, if you came back to your proof after a year of not thinking about it, you should be able to follow your work no problem.  In particular, if you're ever writing a proof and you wonder ``Is this valid?'', the answer is ``No, it's not valid.''---you need to add more detail until there is \emph{no doubt} whatsoever that your argument is correct.  Tricking me (or yourself) into thinking you know the details when in fact you do not is not the way to go about learning mathematics.

Okay, so enough with this wishy-washy philosophical BS.  I should probably at least give you some \emph{concrete} advice about proof-writing.  I think probably most of proof-writing should be learned by doing, but I suppose I can say at least a couple of things.\footnote{Keep in mind that in the following subsubsections we will often make use of examples to illustrate concepts that we technically have not yet developed the mathematics for yet.  First of all, you needn't worry, as because we are just using the examples for the purposes of illustration, this doesn't make our development circular.  Secondly, if you can't follow an example because you haven't seen it before, don't worry---just get what you can out of it and move on.}\footnote{If you are fine with proofs, you can probably safely skip to the next subsection, \crefnameref{sbsSets}.}

\subsubsection{Iff}

We mentioned the meaning of the word ``iff'' in the previous section, and we wound up giving an example of a proof which involved the phrase (\cref{prpA.2.4}).  Allow us to elaborate.
\begin{important}
If ever asked to prove a statement of the form ``$A$ iff $B$'', you need to prove \emph{two things}:  first, assuming $A$, you prove $B$; then, assuming $B$, you prove $A$.
\end{important}
See \cref{prpA.2.4} for a concrete example of this.

\subsubsection{The following are equivalent}

The phrase ``The following are equivalent.'' is similar to the phrase ``iff'', but is used when dealing with more than two statements.  For example, consider the following claim.
\begin{important}
Let $m,n\in \Z$.  Then, the following are equivalent.
\begin{enumerate}
\item \label{tfae.i}$m<n$.
\item \label{tfae.ii}$m\leq n-1$.
\item \label{tfae.iii}$m+1\leq n$.
\end{enumerate}
\end{important}
To prove this, you need to prove that \cref{tfae.i} iff \cref{tfae.ii}, \cref{tfae.i} iff \cref{tfae.iii}, and \cref{tfae.ii} iff \cref{tfae.iii}---this is exactly what it means for all the three statements to be logically-equivalent to one another.  On the face of it, it seems like this would mean we would have to do $2\times 3=6$ proofs.  Not so.  In fact, it is enough to prove \cref{tfae.i} implies \cref{tfae.ii}, \cref{tfae.ii} implies \cref{tfae.iii}, and \cref{tfae.iii} implies \cref{tfae.i}.  Using these three implications alone, you can go from any one statement to any other.  For example, \cref{tfae.ii} implies \cref{tfae.i} because, if \cref{tfae.ii}, then \cref{tfae.iii}, and hence \cref{tfae.i}.

\subsubsection{For all\textellipsis}

If the statement you are trying to prove is of the form ``$\forall X\in X,\ \mcal{P}(x)$'', you should almost certainly start your proof with something like ``Let $x\in X$ be arbitrary.''  You then prove $\mcal{P}(x)$ itself.  Pretty self-explanatory.

\subsubsection{The contrapositive and proof by contradiction}

\emph{Proof by contradiction} and \emph{proof by contraposition} are two closely related proof techniques.  In fact, in a sense to be explained below, they're the \emph{same} proof technique.  Before we get there, however, let us first explain what these two techniques refer to.

First, we explain ``contradiction''.  Assume you want to prove the statement ``$A$ implies $B$.''.  Of course, you first assume that $A$ is true.  You now try to prove that $B$ is true.  Sometimes doing this directly can prove difficult, and in such cases, you can try what is referred to as \emph{proof by contradiction}\index{Proof by contradiction}:  Suppose that $\neg B$ is true.  Now, using $A$ \emph{and} $\neg B$, try to prove something you already know to be false.  As the \emph{only} assumption you made was $\neg B$, that assumption must have been incorrect, and therefore $\neg \neg B$ is true, and hence $B$ is true.\footnote{This logic implicitly uses what is called the \emph{Principle of the Excluded Middle}\index{Principle of the Excluded Middle}, which says that, if $A$ is a statement, then $A$ is true or $A$ is false.  Some mathematicians reject this as valid (or so Wikipedia claims).  They are crazy.  Such crazy mathematicians thus cannot use proofs by contradiction.  Pro-tip:  don't be crazy.}

On the other hand, \emph{proof by contraposition}\index{Proof by contraposition} refers to nothing more than an application of \cref{prpA.2.4}.  That is, if you would like to prove that ``$A$ implies $B$'', you instead prove that ``$\neg B$ implies $\neg A$''.

All that remains in this subsubsection is an explanation of the relationship between proof by contraposition and proof by contradiction.  As this relationship is not particularly important, feel free to skip to the next subsubsection.

Superficially, proof by contradiction and proof by contraposition appear to be distinct, but related techniques.  On the other hand, they are equivalent in a sense to be described as follows.\footnote{The explanation of exactly in what sense these two proof techniques are equivalent is not particularly useful.  Certainly, I find it highly unlikely that what follows in this subsubsection will be of significant use in actual proof writing.  Thus, feel free to skip to the end of the following proof unless you are particularly curious.}  First of all, we have to be precise about what we mean by ``proof by contradiction'' and ``proof by contraposition''.  The precise statement of ``proof by contraposition'' is given in \cref{prpA.2.4}:  ``$A$ implies $B$'' is equivalent to ``$\neg B$ implies $\neg A$''.  On the other hand, the precise statement of ``proof by contradiction'' is given in the following statement.
\begin{prp}{}{prpA.2.7}
Let $A$ and $B$ be statements.  Then, $A\Rightarrow B$ is true iff $(A\text{ and }\neg B)\Rightarrow \text{False}$ is true.
\begin{proof}
$(\Rightarrow )$ Suppose that $A\Rightarrow B$ is true.   We would like to show that $(A\text{ and }\neg B)\Rightarrow \text{False}$.  So, suppose that $A$ and $\neg B$ are true.  As $A\Rightarrow B$ is true, it follows that $B$ is true.  But then, $B$ and $\neg B$ are true, and hence $\text{FALSE}$.

\blankline
\noindent
$(\Leftarrow )$ Suppose that $(A\text{ and }\neg B)\Rightarrow \text{False}$ is true.  Taking the contrapositive, it follows that $\neg A$ or $B$ is true.  We would like to show that $A\Rightarrow B$ is true.  Taking the contrapositive again, it suffices to show that $\neg B\Rightarrow \neg A$.  So, suppose $\neg B$.  We wish to prove $\neg A$.  However, as we know that $\neg A$ or $B$ is true, it in fact must be the case that $\neg A$ is true. 
\end{proof}
\end{prp}
If you examine the proofs of \cref{prpA.2.4} and \cref{prpA.2.7},\footnote{The precise statements of ``proof by contraposition'' and ``proof by contradiction'' respectively.} you will find respectively that the former makes use of proof by contradiction and that the latter makes use of proof by contraposition.  It is in this sense that they are equivalent proof techniques.

Okay, so up until now, this has all be pretty precise, but I would like to give an intuitive explanation as to the difference between the two.  Every proof by contraposition and be reduced to a proof by contradiction in the following way:  assume your hypotheses $A$, proceed by contradiction and assume $\neg B$, and proceed to prove $\neg A$:  contradiction.  The key is that in a proof by contraposition your contradiction is of the form $A$ and $\neg A$, whereas with proof by contradiction you obtain more general contradictions.  Thus, while superficially proof by contradiction might seem much stronger, it is in fact not actually so.  This is similar to how "the Principle of Strong Induction seems stronger than (just) the Principle of Induction, but in fact they are equivalent statements---see below.

Finally, we end with a quick comment on usage.  First of all, it is very easy to rephrase a proof by contraposition as a proof by contradiction (as explained above), and so, if you like, you needn't worry about proof by contraposition at all.  Furthermore, proof by contradiction tends to be most useful when ``$B$'' in ``$A$ implies $B$.'' is somehow `already negated'---for example, think of how one might try to prove the statement ``If $x\in \R$ and $x^2=2$, then $\sqrt{x}\notin \Q$.''. 

\subsubsection{Without loss of generality\textellipsis}

You will often see the phrase ``Without loss of generality\textellipsis'' used in proofs.  It is easiest to demonstrate what this means, and how to use it yourself, with an example.

The definition of integral (\cref{dfnA.1.69}) reads
\begin{important}
A rg $\coord{X,+,0,\cdot}$ is \emph{integral} iff it has the property that, whenever $x\cdot y=0$, it follows that either $x=0$ or $y=0$.
\end{important}
So, imagine you were doing a proof, and you know that $\coord{X,+,0,\cdot}$ was an integral rg,\footnote{You shouldn't need to know what a rg is to understand the explanation of ``Without loss of generality\textellipsis ''.} and that $x\cdot y=0$ for $x,y\in X$.  The definition of integral implies that $x=0$ or $y=0$.  \emph{At this point, you could say}, ``Without loss of generality, suppose that $x=0$.''.  You then continue the proof using the fact that $x=0$.

Logically, you would first have to assume that $x=0$, finish the proof in that case, and then go back to the case where $y=0$, and finish the proof again.  However, if the proofs are essentially identical\footnote{Obviously, if the proof needed to do the case $x=0$ is significantly different from the proof needed to do the case $y=0$ (i.e.~is more than just a letter swap $x\leftrightarrow y$), you should not use this phrase and instead write up the proofs of both cases individually.} in these two cases, you are `allowed' to cut your work in half with the phrase ``Without loss of generality\textellipsis ''---there is no point in repeating the same logic with a different letter twice.

\subsubsection{If XYZ we are done, so suppose that \texorpdfstring{$\neg$}{not}XYZ}

As with ``Without loss of generality\textellipsis '', the use of this phrase is easiest to demonstrate with an example.

The definition of \emph{prime} reads
\begin{important}
Let $p\in \Z$.  Then, $p$ is \emph{prime}\index{Prime} iff \textellipsis whenever $p\mid (mn)$, it follows that $p\mid m$ or $p\mid n$.\footnote{We have omitted part of the definition in the ``\textellipsis'' that is irrelevant for us at the moment (essentially the requirement that $p\neq -1,0,1$).}
\end{important}
So, let $p\in \Z$ and suppose that you want to prove that $p$ is prime.  To prove this condition, you would say ``Let $m,n\in \Z$ and suppose that $p\mid (mn)$.''  From here, you now want to prove that $p\mid m$ or $p\mid n$.  At this point, you can say ``If $p\mid m$ we are done, so suppose that $p\nmid m$.''.

Hopefully, the logic of this is pretty self-explanatory.  If it helps, however, you can view this as essentially the same as proof by contradiction.  If we were to proceed by contradiction, instead we would say ``Suppose that $p\nmid m$ and $p\nmid n$.'', and from that deduce a contradiction.  Instead, in this case, we shall assume $p\nmid m$, and use that to prove that $p\mid m$.

\subsubsection{Proving two sets are equal}

A lot of proofs require you to show that two different sets are in fact one in the same.  The way to prove this is made precise with \cref{exrA.2.10}.  In the meantime, however, we can say the following.
\begin{important}
Let $S$ and $T$ be sets and suppose that you want to prove that $S=T$.  You then need to prove \emph{two} things:  if $s\in S$, then $s\in T$; and if $t\in T$, then $t\in S$.
\end{important}
This logic is very much identical to that used for proving ``iff'' statements.

\subsubsection{Induction}

\paragraph{Induction} Let $\mcal{P}_m$ be a statement for $m\in \N$.  The \term{Principle of Induction}\index{Principle of Induction} says that
\begin{important}
If (i)~$\mcal{P}_0$ is true and (ii)~$\mcal{P}_m\Rightarrow \mcal{P}_{m+1}$ is true for all $m\in \N$, then $\mcal{P}_m$ is true for all $m\in \N$.\footnote{(i)~is referred to as the \term{initial case} or the \term{initial step}, and (ii)~is referred to as the \term{inductive step}.}
\end{important}
The logic is as follows.  Suppose we want to prove $\mcal{P}_3$.  Then, because $\mcal{P}_0$ and $\mcal{P}_0\Rightarrow \mcal{P}_1$, it follows that $\mcal{P}_1$.\footnote{Incidentally, the passage from ``$A$ and $A\Rightarrow B$'' to $B$ is called \emph{modus ponens}\index{Modus ponens}, which itself is short for ``modus pōnendō pōnēns'', which is Latin for (literally) ``the method to be affirmed, affirms''.}  Then, because $\mcal{P}_1$ and $\mcal{P}_1\Rightarrow \mcal{P}_2$, it follows that $\mcal{P}_2$.  Then, because $\mcal{P}_2$ and $\mcal{P}_2\Rightarrow \mcal{P}_3$, it follows that $\mcal{P}_3$.  Of course, nothing is special about $m=3$, and so this logic can be used to prove $\mcal{P}_m$ for all $m\in \N$.\footnote{Though it won't be able to prove $\mcal{P}_{\infty}$!  For example, a common fake proof that $\uppi$ is rational essentially proves by induction that, for all $m\in \Z ^+$, the decimal approximation of $\uppi$ with $m$ digits is rational, and `therefore' $\uppi$ is rational.  Sorry, but that's not how induction works.}

You can also use similar logic to define things.  For example, you might define a bijection $f\colon \N \rightarrow \N \times \N$ by
\begin{equation}
\begin{split}
f(0) & \coloneqq \coord{0,0} \\
f(m+1) & \coloneqq \begin{cases}\coord{f(m)_x-1,f(m)_y+1} & \text{if }f(m)_x\geq 1 \\ \coord{f(m)_x+f(m)_y+1,0} & \text{otherwise.\footnotemark}\end{cases}
\end{split}
\end{equation}\footnotetext{The picture is that you go `down the diagonal', unless you `hit the edge', in which case you `hop to the top of the next diagonal'.}

\paragraph{Strong Induction} Equally as valid, for the exact same reason, is what is sometimes referred to the \term{Principle of Strong Induction}\index{Principle of Strong Induction}, which says that
\begin{important}
If (i)~$\mcal{P}_0$ is true and (ii)~$(\forall 0\leq k\leq m,\ \mcal{P}_k)\Rightarrow \mcal{P}_{m+1}$ is true for all $m\in \N$, then $\mcal{P}_m$ is true for all $m\in \N$.
\end{important}
The key difference between this and `regular' induction is that, in the induction step, you don't just assume $\mcal{P}_m$, but instead, you assume $\mcal{P}_0$, and $\mcal{P}_1$, and $\mcal{P}_2$, and \textellipsis $\mcal{P}_m$.  Superficially, this does indeed seem stronger,\footnote{Because during the inductive step, you don't get to assume just the single statement $\mcal{P}_m$, but rather the $m+1$ statements $\mcal{P}_0$, and $\mcal{P}_1$, and $\mcal{P}_2$, and \textellipsis $\mcal{P}_m$.} but in fact `regular' induction and strong induction are equivalent, though sometimes it can be quite convenient to be make use of all of $\mcal{P}_0,\ldots ,\mcal{P}_m$ instead of just $\mcal{P}_m$.

For example, suppose that you want to prove that every positive integer greater-than-or-equal to $2$ is divisible by some prime.  In this case, the initial step is simply to prove that ``$2$ is divisible by some prime.''.  This is of course trivial:  $2$ is divisible by $2$, which is prime.  Here's where things get a bit different, however:  for the inductive step, assume that $2,3,4,\ldots ,m$ are all divisible by some prime.  Using this, we want to show that $m+1$ is divisible by some prime.  Well, either $m+1$ is prime itself, in which case $m+1$ is divisible by $m+1$, or it is not prime, in which case $m+1$ is divisible by some integer $k$ with $2\leq k\leq m$.  By the induction hypothesis, $k$ is divisible by some prime, and hence $m+1$ is in turn divisible by some prime.

Note that we will not usually make a distinction between `regular' induction and strong induction in these notes.  When using either method, we shall simply say something like ``We proceed by induction\textellipsis ''.

\paragraph{Well-founded induction} The most powerful form of induction which subsumes all other `types' of induction is known as \emph{well-founded induction}.  As it is less elementary than what we have discussed thus far, we leave a detailed discussion of this until \cref{sbsWellFoundedInduction}.  We can, however, at least say a little for the time being..

The basic idea is to replace the set with which you index your statements (previously $\N$) with a more general type of set $X$.  It turns out that the only structure on $\N$ relevant to induction is the ordering, and so you don't just replace $\N$ with a set $X$, you replace $\coord{\N ,\leq}$ with a pair $\coord{X,\preceq}$, where $X$ is a set and $\preceq$ is a \emph{relation} (\cref{Relation}) on $X$.  It turns out that (\cref{WellFoundedInduction}) the only property of $\leq$ on $\N$ that was necessary for induction to work is what is called \emph{well-foundedness} (\cref{MaximalAndMinimal}), which states that every nonempty subset has a minimal element (\cref{MaximalAndMinimal}).

The \term{Principle of Well-Founded Induction}\index{Principle of Well-Founded Induction} then says that
\begin{important}
	If for every $x_0\in X$, $(\forall x<x_0,\mcal{P}_x)\Rightarrow \mcal{P}_{x_0}$, then $\mcal{P}_x$ is true for all $x\in X$.
\end{important}

If you ever want to perform an induction-like argument, but it's not working because you have more than countably-infinite many statements, try well-founded induction.

Finally, we mention that there is a method of proof called \emph{transfinite induction}\index{Transfinite induction}.  It is technically a special case of well-founded induction, but much more common.\footnote{For some reason, not too many people seem to know of well-founded induction.}  Roughly speaking, transfinite induction is to well-ordered as well-founded induction is to well-founded.  We refrain from discussing it because (i) we don't need to---well-founded induction is stronger and (ii) the usual way it's stated requires the development of what are called \emph{ordinals}\index{Ordinal}), which would take us quite astray.  Still, you should be aware of it so you can go and learn it if you ever feel this will be useful to you (if you become a mathematician, it almost certainly will be at some point).

\subsection{Sets}\label{sbsSets}

The idea of a set is something that contains other things.
\begin{textequation}
If $X$ is a \term{set} which contains an \term{element} $x$, then we write $x\in X$\index[notation]{$x\in X$}.\footnotemark Two sets are equal iff they contain the same elements.
\end{textequation}\footnotetext{Sometimes we will also write $X\ni x$\index[notation]{$X\ni x$} if it happens to be more convenient to write it in that order (for example, in $\R \ni x\mapsto x^2$).}
\begin{dfn}{Empty-set}{}
The \term{empty-set}\index{Empty-set}, $\emptyset$\index[notation]{$\emptyset$}, is the set $\emptyset \coloneqq \{ \}$.
\begin{rmk}
That is, $\emptyset$ is the set which contains no elements.
\end{rmk}
\end{dfn}
\begin{rmk}
If ever you see an equals sign with a colon in front of it (e.g.~in ``$\emptyset \coloneqq \{ \}$''), it means that the equality is true \emph{by definition}.  This is used in definitions themselves, but also outside of definitions to serve as a reminder as to why the equality holds.\index[notation]{$\coloneqq $}
\end{rmk}
\begin{dfn}{Subset}{}
Let $X$ and $Y$ be sets.  Then, $X$ is a \term{subset}\index{Subset} of $Y$, written $X\subseteq Y$\index[notation]{$X\subseteq Y$}, iff whenever $x\in X$ it is also the case that $x\in Y$.
\end{dfn}
\begin{rmk}
Generally speaking we put slashes through symbols to indicate that the statement that would have been conveyed without the slash is false.  For example, $x\notin X$ means that $x$ is not an element of $X$, the statement that $X\not \subseteq Y$ means that $X$ is not a subset of $Y$, etc..
\end{rmk}
\begin{exr}{}{exrA.2.10}
Let $X$ and $Y$ be sets.  Show that $X=Y$ iff $X\subseteq Y$ and $Y\subseteq X$.
\end{exr}
\begin{dfn}{Proper subset}{ProperSubset}
Let $X$ be a subset of $Y$.  Then, $X$ is a \term{proper subset}\index{Proper subset} of $Y$, written $X\subset Y$\index[notation]{$X\subset Y$}, iff there is some $y\in Y$ that is not also in $X$.
\begin{rmk}
You should note that many authors use the notation ``$X\subset Y$'' simply to indicate that $X$ is a (\emph{not-necessarily-proper}) subset of $Y$.
\end{rmk}
\end{dfn}
\begin{rmk}
Let $X$ be a set, let $\mcal{P}$ be a property that an element in $X$ may or may not satisfy, and let us write $\mcal{P}(x)$ iff $x$ satisfies the property $\mcal{P}$.  Then, the notation
\begin{equation*}
\left\{ x\in X:\mcal{P}(x)\right\}
\end{equation*}
is read ``The set of all elements in $X$ such that $\mcal{P}(x)$.'' and represents a set whose elements are precisely those elements of $X$ for which $\mcal{P}$ is true.  Sometimes this is also written as
\begin{equation*}
\left\{ x\in X|\mcal{P}(x)\right\} ,
\end{equation*}
but my personal opinion is that this can look ugly (or even slightly confusing) if, for example, $\mcal{P}(x)$ contains an absolute value in it, e.g.
\begin{equation*}
\left\{ x\in \R |\abs{x}<1\right\} .
\end{equation*}
\end{rmk}
\begin{dfn}{Complement}{}
Let $X$ and $Y$ be sets.  Then, the \term{complement}\index{Complement} of $Y$ in $X$, $X\setminus Y$\index[notation]{$X\setminus Y$}, is defined by
\begin{equation}
X\setminus Y\coloneqq \{ x\in X:x\notin Y\} .
\end{equation}
If $X$ is clear from context, sometimes we write $Y^{\comp}\coloneqq X\setminus Y$\index[notation]{$Y^{\comp}$}.
\end{dfn}
\begin{dfn}{Union and intersection}{}
Let $A,B$ be subsets of a set $X$.  Then, the \term{union}\index{Union} of $A$ and $B$, $A\cup B$\index[notation]{$A\cup B$}, is defined by
\begin{equation}
A\cup B\coloneqq \left\{ x\in X:x\in A\text{ or }x\in B\right\} .
\end{equation}
The \term{intersection}\index{Intersection} of $A$ and $B$, $A\cap B$\index[notation]{$A\cap B$}, is defined by
\begin{equation}
A\cap B\coloneqq \left\{ x\in X:x\in A\text{ and }x\in B\right\} .
\end{equation}
\begin{rmk}
More generally, if $\collection{S}$ is a collection\footnote{Technically, the term \term{collection}\index{Collection} is just synonymous with the term ``set'', though it tends to be used in cases when the elements of the set itself are to be thought of as other sets (e.g.~here where the elements of $\collection{S}$ are subsets of $X$).} of subsets of $X$, then the \emph{union} and \emph{intersection} of all sets in $\collection{S}$ are defined by
\begin{align*}
\bigcup _{S\in \collection{S}}S & \coloneqq \left\{ x\in X:\exists S\in \collection{S}\text{ such that }x\in S\text{.}\right\} \\
\intertext{and}
\bigcap _{S\in \collection{S}}S & \coloneqq \left\{ x\in X:\forall S\in \collection{S},\ x\in S\text{.}\right\} .
\end{align*}
\end{rmk}
\end{dfn}
\begin{dfn}{Disjoint and intersecting}{}
Let $A,B$ be subsets of a set $X$.  Then, $A$ and $B$ are \term{disjoint}\index{Disjoint} iff $A\cap B=\emptyset$.  $A$ and $B$ \term{intersect}\index{Intersect} (or \term{meet}\index{Meet}) iff $A\cap B\neq \emptyset$.
\end{dfn}
\begin{exr}{De Morgan's Laws}{DeMorgansLaws}\index{De Morgan's Laws}
Let $\collection{S}$ be a collection of subsets of a set $X$.  Show that
\begin{equation}
\bigg( \bigcup _{S\in \collection{S}}S\bigg) ^{\comp}=\bigcap _{S\in \collection{S}}S^{\comp}\text{ and }\bigg( \bigcap _{S\in \collection{S}}S\bigg) ^{\comp}=\bigcup _{S\in \collection{S}}S^{\comp}.
\end{equation}
\end{exr}
\begin{exr}{}{}
Let $X$ be a set and let $S,T\subseteq X$.  Show that $S\setminus T=S\cap T^{\comp}$.
\end{exr}
\begin{dfn}{Symmetric-difference}{SymmetricDifference}
	Let $A,B$ be subsets of a set $X$.  Then, the \term{symmetric-difference}\index{Symmetric-difference} of $A$ and $B$, $A\bigtriangleup B$\index[notation]{$A\bigtriangleup B$}, is defined by
	\begin{equation}
		A\bigtriangleup B\ceqq (A\cap B^{\comp})\cup (A^{\comp}\cap B).
	\end{equation}
	\begin{rmk}
		If you draw a ``Venn diagram'', you break up $X$ into four disjoint pieces:  everything outside $A$ and $B$, things inside both $A$ and $B$, things inside $A$ but not $B$, and things inside $B$ but not $A$.  The symmetric difference is the union of the last two regions.
		
		Put another way, the symmetric difference is the elements in $A\cup B$ that $A$ and $B$ do \emph{not} have in common.
	\end{rmk}
\end{dfn}

The union and intersection of two sets are ways of constructing new sets, but one important thing to keep in mind is that, a priori, the two sets $A$ and $B$ are assumed to be contained within another set $X$.  But how do we get entirely new sets without already `living' inside another?  There are several ways to do this.
\begin{dfn}{Cartesian-product}{CartesianProduct}
Let $X$ and $Y$ be sets.  Then, the \emph{Cartesian-product}\index{Cartesian-product} of $X$ and $Y$, $X\times Y$\index[notation]{$X\times Y$}, is
\begin{equation}
X\times Y\coloneqq \left\{ \coord{x,y}:x\in X,y\in Y\right\} .
\end{equation}
\begin{rmk}
If you really insist upon everything being defined in terms of sets we can take
\begin{equation}
\coord{x,y}\index[notation]{$\coord{x,y}$}\coloneqq \left\{ x,\{ x,y\} \right\} .
\end{equation}
The reason we use the notation $\coord{x,y}$ as opposed to the probably more common notation $(x,y)$ is to avoid confusion with the notation for open intervals.
\end{rmk}
\begin{rmk}
If $Y=X$, then it is common to write $X^2\coloneqq X\times X$, and similarly for products of more than two sets (e.g.~$X^3\coloneqq X\times X\times X$).  Elements in finite products are called \term{tuples}\index{Tuple} or sometimes \term{lists}\index{List}.  For example, the elements of $X^2$ are $2$-tuples (or just \term{ordered pairs}\index{Ordered pair}), the elements in $X^3$ are $3$-tuples, etc..\footnote{If you really want to be pedantic about things, you might complain ``OMG what is this crazy new symbol `$3$'!?  We haven't defined the naturals yet!''.  In this case, you should merely interpret $X^3$ as short-hand for $X\times X\times X$.  Similar comments apply throughout this appendix.}
\end{rmk}
\end{dfn}
\begin{dfn}{Disjoint-union}{DisjointUnion}
Let $X$ and $Y$ be sets.  Then, the \term{disjoint-union}\index{Disjoint-union} of $X$ and $Y$, $X\sqcup Y$\index[notation]{$X\sqcup Y$}, is
\begin{equation}
\begin{split}
X\sqcup  Y & \coloneqq \left\{ \coord{a,m}:m\in \{ 0,1\} ,\right. \\ & \qquad \left. a\in X\text{ if }m=0,\ a\in Y\text{ if }m=1\right\} .
\end{split}
\end{equation}
\begin{rmk}
Intuitively, this is supposed to be a copy of $X$ together with a copy of $Y$.  $a$ can come from either set, and the $0$ or $1$ tells us which set $a$ is supposed to come from.  Thus, we think of $X\subseteq X\sqcup Y$ as $X=\left\{ \coord{a,0}:a\in X\right\}$ and $Y\subseteq X\sqcup Y$ as $Y\left\{ \coord{a,1}:a\in Y\right\}$.
\end{rmk}
\end{dfn}
The key difference between the union and disjoint-union is that, in the case of the union of $A$ and $B$, an element that $x$ is both in $A$ and in $B$ is a \emph{single} element in $A\cup B$, whereas in the disjoint-union there will be two copies of it:  one in $A$ and one in $B$.  Hopefully the next example will help clarify this.
\begin{exm}{Union vs.~disjoint-union}{}
Define $A\coloneqq \{ a,b,c\}$ and $B\coloneqq \{ c,d,e,f\}$.  Then, $A\cup B=\{ a,b,c,d,e,f\}$.  On the other hand, $A\sqcup B=\{ a,b,c_A,c_B,d,e,f\}$, where $A\sqcup B\supseteq A=\{ a,b,c_A\}$ and $A\sqcup B\supseteq B=\{ c_B,d,e,f\}$.
\end{exm}
\begin{dfn}{Power set}{}
Let $X$ be a set.  Then, the \term{power set}\index{Power set} of $X$, $2^X$\index[notation]{$2^X$}, is the set of all subsets of $X$,
\begin{equation}
2^X\coloneqq \left\{ A:A\subseteq X\right\} .
\end{equation}
\begin{rmk}
We will discuss the motivation for this notation in the next subsection (see \cref{exrA.1.26x}).
\end{rmk}
\end{dfn}

\section{Relations, functions, and orders}

Having defined Cartesian products, we can now make the following definition.
\begin{dfn}{Relation}{Relation}
A \emph{relation}\index{Relation} between two sets $X$ and $Y$ is a subset $R$ of $X\times Y$.
\begin{rmk}
For a given relation $R$, we write $x\sim _Ry$\index[notation]{$x\sim _Ry$}, or just $x\sim y$\index[notation]{$x\sim y$} if $R$ is clear from context, iff $\coord{x,y}\in R$.  Often we will simply refer to the relation by the symbol $\sim$ instead of $R$.
\end{rmk}
\begin{rmk}
It is important to be able to understand how to translate between the two different notations for writing a relation.  In one direction, if you know $R\subseteq X\times Y$, then $x\sim y$ iff $\coord{x,y}\in R$, as already mentioned.  In the other direction, if you know $\sim$, then $R=\left\{ \coord{x,y}\in X\times Y:x\sim y\right\}$.
\end{rmk}
\begin{rmk}
If $X=Y$, we will say that $\sim$ is a relation \emph{on} $X$.
\end{rmk}
\end{dfn}
\begin{dfn}{Composition}{Composition}
Let $X$, $Y$, and $Z$ be sets, and let $R$ be a relation on $X$ and $Y$, and let $S$ be a relation on $Y$ and $Z$.  Then, the \term{composition}\index{Composition}, $S\circ R$\index[notation]{$S\circ R$}, of $R$ and $S$ is the relation on $X$ and $Z$ defined by
\begin{equation}
\begin{split}
\MoveEqLeft
S\circ R\coloneqq \left\{ \coord{x,z}\in X\times Z:\exists y\in Y\right. \\ & \qquad \left. \text{such that }\coord{x,y}\in R\text{ and }\coord{y,z}\in S\text{.}\right\} .
\end{split}
\end{equation}
\begin{rmk}
If $R$ is a relation on $X$ (so that $R\circ R$ makes sense), for $k\in \N$, we shall abbreviate $R^k\coloneqq \underbrace{R\circ \cdots \circ R}_k$\index[notation]{$R^k$}, with $R^0\coloneqq \{ \coord{x,x}\in X\times X:x\in X\}$.\footnote{$R^0$ is of course the identity function on $X$---see \cref{IdentityFunction}.}
\end{rmk}
\begin{rmk}
You will see in the next definition that a function is in fact just a very special type of relation, in which case, this composition is exactly the composition that you (hopefully) know and love.
\end{rmk}
\end{dfn}
\begin{exr}{}{exrA.3.4}
Let $R$, $S$, and $T$ be relations between $X$ and $Y$, $Y$ and $Z$, and $Z$ and $W$ respectively.  Show that
\begin{equation}
(T\circ S)\circ R=T\circ (S\circ R).
\end{equation}
\end{exr}
While the appropriate generality in which to make the next definitions of restriction and corestriction is for arbitrary relations, the intuition you should use to understand the concepts will almost certainly come from your understanding of functions (and we will have little to no need to make use of these concepts in this amount of generality), so feel free to first read the definition of a function (\cref{Function}) and related concepts and then come back to this if at first this seems confusing.
\begin{dfn}{Restriction and corestriction}{RestrictionAndCorestriction}
Let $f$ be a relation between two sets $X$ and $Y$, let $S\subseteq X$, and let $T\subseteq Y$.  Then, the \term{restriction}\index{Restriction} of $f$ to $S$, $\restr{f}{S}$\index[notation]{$\restr{f}{S}$}, is a relation between $S$ and $Y$ defined by
\begin{equation}
\restr{f}{S}\coloneqq f\circ \left\{ \coord{s,x}\in S\times X:s=x\right\} .\footnote{Here, $\left\{ \coord{s,x}\in S\times X:s=x\right\}$ is of course to be interpreted as a relation between $S$ and $X$.}
\end{equation}
The \term{corestriction}\index{Corestriction} of $f$ to $T$, $\corestr{f}{T}$\index[notation]{$\corestr{f}{T}$}, is a relation between $X$ and $T$ defined by
\begin{equation}
\corestr{f}{T}\coloneqq \left\{ \coord{y,t}\in Y\times T:y=t\right\} \circ f.\footnote{Similarly as before, $\left\{ \coord{y,t}\in Y\times T:y=t\right\}$ is to be interpreted as a relation between $Y$ and $T$.}
\end{equation}
If $g=\restr{f}{S}$, then we will also say that $f$ \term{extends}\index{Extend} $g$.  If $g=\corestr{f}{T}$, then we will also say that $f$ \term{coextends}\index{Coextend} $g$.
\begin{rmk}
As mentioned before, to understand this, it probably helps to think of what these concepts mean in the case that the relation is in fact a function (note that a function is just a special type of relation---see \cref{Function}).  For example, if we have the function $f\colon \R \rightarrow \R$ defined by $f(x)\coloneqq x^2$, then we can obtain a new function $\restr{f}{(-1,1)}\colon (-1,1)\rightarrow \R$ by restricting to $(-1,1)\subseteq \R$, and that new function is still given by the same `rule', $f(x)\coloneqq x^2$---the only thing that has changed is the domain.\footnote{I realize we are making use of notation we have not yet technically.  This is not a problem from a mathematical perspective as I am only trying to explain.  From a pedagogical perspective, hopefully I'm not making use of anything unfamiliar to you---if so, flip ahead.}

Corestriction, on the other hand, is when we change the codomain of the relation.  For example, we can corestrict this same function to $\R _0^+$ to obtain the function $\corestr{f}{\R _0^+}\colon \R \rightarrow \R _0^+$, once again, still with the `rule' $f(x)\coloneqq x^2$.\footnote{The term ``corestriction'' is incredibly uncommon, as one often simply changes the codomain of the function without explicitly mentioning so.  This is technically sloppy, but almost never actually causes problems.  Still, it is important to realize that functions with different codomains are always different functions.}
\end{rmk}
\begin{rmk}
These concepts will almost always arise in the case where the relation $f$ is in fact a function.  One reason we state the definitions in this more general case, besides just to be more general, is that \emph{the corestriction of a function is not always going to be a new function}.  For example, if we again define $f\colon \R \rightarrow \R$ by $f(x)\coloneqq x^2$, $\corestr{f}{(-1,1)}$ is a relation that is no longer a function---the reason is that, for example, $\coord{2,y}\notin \corestr{f}{(-1,1)}$ for any $y\in (-1,1)$.  This is easy, but quite subtle, and not super important, so don't worry if this doesn't make sense to you at the moment.  In fact, the corestriction of a function to $T\subseteq Y$ is another function iff $T$ is contained in the image of $f$.
\end{rmk}
\end{dfn}

There are several different important types of relations.  Perhaps the most important is the notion of a function.
\begin{dfn}{Function}{Function}
A \term{function}\index{Function} from a set $X$ to a set $Y$ is a relation $\sim _f$ that has the property that for each $x\in X$ there is exactly one $y\in Y$ such that $x\sim _fy$.  For a given function $\sim _f$, we denote by $f(x)$\index[notation]{$f(x)$} that unique element of $Y$ such that $x\sim _ff(x)$.  $X$ is the \term{domain}\index{Domain (of a function)} of $f$ and $Y$ is the \term{codomain}\index{Codomain (of a function)} of $f$.  The notation $f\colon X\rightarrow Y$\index[notation]{$f\colon X\rightarrow Y$} means ``$f$ is a function from $X$ to $Y$.''.  The set of all functions from $X$ to $Y$ is denoted $Y^X$\index[notation]{$Y^X$}.
\begin{rmk}
The motivation for this notation is that, if $X$ and $Y$ are finite sets, then the cardinality of the set of all functions from $X$ to $Y$ is $\abs{Y}^{\abs{X}}$.
\end{rmk}
\begin{rmk}
The arrow ``$\mapsto$''\index[notation]{$\mapsto$} can be used to define a function with necessarily giving it a name.  For example, one can write
\begin{equation}
\R \ni x\mapsto 3x^2-5\in \R
\end{equation}
as notation for the function $f\colon \R \rightarrow \R$ defined by $f(x)\ceqq 3x^2-5$.  Of course, this is convenient if it is unnecessary to give a specific name.
\end{rmk}
\begin{rmk}
The ``$x$'' in ``$f(x)$'' is sometimes referred to as the \term{argument}\index{Argument} of the function.
\end{rmk}
\end{dfn}
\begin{ntn}{Placeholders for arguments}{}
Let $f$ be a function.  A lot of the time, if we want to refer to $f$, we just say, well, ``$f$''.  Besides this, however, we also frequently write ``$f(x)$''.  Strictly speaking, this is incorrect---the function itself is $x\mapsto f(x)$, whereas $f(x)$ is the value of the function $f$ at $x$ in the domain.  In practice, however, it is very common to write ``$f(x)$'' to denote the function itself, and not just a particular value.

We may also use the symbols ``$\blankdot$'' or ``$\blank$'' to indicate the argument of a function.  For example, we might denote a function using the notation $\braket{\blankdot ,\blankdot}\colon V\times V\rightarrow \K$\index[notation]{$f(\blankdot )$} or $\braket{\blank ,\blank}\colon V\times V\rightarrow \K$\index[notation]{$f(\blank )$}.  This notation means that ``$\braket{\blankdot ,\blankdot}$'' is the name of a function, and furthermore, we denote its value at the element $\coord{v_1,v_2}\in V\times V$ as ``$\braket{v_1,v_2}$''.  Thus, the dots tell you where to `plug in' the variables.  Similarly for ``$\blank$''.
\end{ntn}
\begin{exm}{Identity function}{IdentityFunction}
For every set $X$, there is a function, $\id _X:X\rightarrow X$\index[notation]{$\id _X$}, the \term{identity function}\index{Identity function}, defined by
\begin{equation}
\id _X(x)\coloneqq x.
\end{equation}
\end{exm}
\begin{dfn}{Inverse function}{}
Let $f\colon X\rightarrow Y$ and $g\colon Y\rightarrow X$ be functions.  Then, $g$ is a \term{left-inverse}\index{Left-inverse (of a function)} of $f$ iff $g\circ f=\id _X$; $g$ is a \term{right-inverse}\index{Right-inverse (of a function)} of $f$ iff $f\circ g=\id _Y$; $g$ is a \term{two-sided-inverse}\index{Two-sided-inverse (of a function)}, or just \term{inverse}\index{Inverse (of a function)}, iff $g$ is both a left- and right-inverse of $f$.
\begin{exr}[breakable=false]{}{}
Let $g$ and $h$ be two (two-sided)-inverses of $f$.  Show that $g=h$.
\end{exr}
Because of the uniqueness of two-sided-inverses, we may write $f^{-1}$\index[notation]{$f^{-1}$} for the unique two-sided-inverse of $f$.
\end{dfn}
\begin{exr}{}{}
Provide examples to show that left-inverses and right-inverses need not be unique.
\end{exr}
\begin{exr}{}{exrA.1.23}
Let $X$ be a nonempty set.
\begin{enumerate}
\item \label{enmA.1.23.i}Explain why there is \emph{no} function $f\colon X\rightarrow \emptyset$.
\item \label{enmA.1.23.ii}Explain why there is \emph{exactly one} function $f\colon \emptyset \rightarrow X$.
\item \label{enmA.1.23.iii}How many functions are there $f\colon \emptyset \rightarrow \emptyset$?
\end{enumerate}
\end{exr}
\begin{dfn}{Image}{}
Let $f\colon X\rightarrow Y$ be a function and let $S\subseteq X$.  Then, the \term{image}\index{Image} of $S$ under $f$, $f(S)$, is
\begin{equation}
f(S)\coloneqq \left\{ f(x):x\in S\right\} .
\end{equation}
The \term{range}\index{Range} of $f$, $f(X)$, is the image of $X$ under $f$.
\begin{rmk}
We may also write $\Ima (f)\coloneqq f(X)$\index[notation]{$\Ima (f)$} for the range of $f$.  If we simply say ``image of $f$'', you should interpret this to mean ``image of $X$ under $f$'', i.e., the range $f(X)$.
\end{rmk}
\begin{rmk}
Note the difference between range and codomain.  For example, consider the function $f\colon \R \rightarrow \R$ defined by $f(x)\coloneqq x^2$.  Then, the codomain is $\R$ but the range is just $[0,\infty )$.  In fact the range and codomain are the same precisely when $f$ is surjective (see \cref{exrA.1.32}.\cref{exrA.1.32.ii}).
\end{rmk}
\end{dfn}
\begin{dfn}{Preimage}{}
Let $f\colon X\rightarrow Y$ be a function and let $T\subseteq Y$.  Then, the \term{preimage} of $T$ under $f$, $f^{-1}(T)$, is
\begin{equation}
f^{-1}(T)\coloneqq \left\{ x\in X:f(x)\in T\right\} .
\end{equation}
\end{dfn}
\begin{exr}{}{}
Let $f\colon X\rightarrow Y$ be a function and let $T\subseteq Y$.  Show that $f^{-1}(T^{\comp})=f^{-1}(T)^{\comp}$.  For $S\subseteq X$, find examples to show that we need not have either $f(S^{\comp})\subseteq f(S)^{\comp}$ nor $f(S)^{\comp}\subseteq f(S^{\comp})$.
\end{exr}
\begin{dfn}{Injectivity, surjectivity, and bijectivity}{}
Let $f\colon X\rightarrow Y$ be a function.  Then,
\begin{enumerate}
\item (Injective) $f$ is \term{injective}\index{Injective} iff for every $y\in Y$ there is at most one $x\in X$ such that $f(x)=y$.
\item (Surjective) $f$ is \term{surjective}\index{Surjective} iff for every $y\in Y$ there is at least one $x\in X$ such that $f(x)=y$.
\item (Bijective) $f$ is \term{bijective}\index{Bijective} iff for every $y\in Y$ there is exactly one $x\in X$ such that $f(x)=y$.
\end{enumerate}
\begin{rmk}
It follows immediately from the definitions that a function $f\colon X\rightarrow Y$ is bijective iff it is both injective and surjective.
\end{rmk}
\end{dfn}
\begin{exr}{}{exrA.1.32}
Let $f\colon X\rightarrow Y$ be a function.
\begin{enumerate}
\item \label{exrA.1.32.i}Show that $f$ is injective iff whenever $f(x_1)=f(x_2)$ it follows that $x_1=x_2$.
\item \label{exrA.1.32.ii}Show that $f$ is surjective iff $f(X)=Y$.
\end{enumerate}
\end{exr}
\begin{exm}{The domain and codomain matter}{exmA.1.45}
Consider the `function' $f(x)\coloneqq x^2$.  Is this `function' injective or surjective?  Defining functions like this may have been kosher back when you were doing mathematics that wasn't actually mathematics, but no longer.  The question does not make sense because you have not specified the domain or codomain.  For example, $f\colon \R \rightarrow \R$ is neither injective nor surjective, $f\colon \R _0^+\rightarrow \R$ is injective but not surjective, $f\colon \R \rightarrow \R _0^+$ is surjective but not injective, and $f\colon \R _0^+\rightarrow \R _0^+$ is both injective and surjective.  Hopefully this example serves to illustrate:  functions are not (just) `rules'---if you have not specified the domain and codomain, then \emph{you have not specified the function}.
\end{exm}
\begin{exr}{}{}
Let $f\colon X\rightarrow Y$ be a function between nonempty sets.  Show that
\begin{enumerate}\label{exrA.1.9}
\item \label{enmA.1.9.i}$f$ is injective iff it has a left inverse;
\item \label{enmA.1.9.ii}$f$ is surjective iff it has a right inverse; and
\item \label{enmA.1.9.iii}$f$ is bijective iff it has a (two-sided) inverse.
\end{enumerate}
\begin{rmk}
By \cref{exrA.1.23}.\cref{enmA.1.23.ii}, there \emph{is} exactly one function from $\emptyset$ to $\{ \emptyset \}$.  This function is definitely injective as every element in the codomain has \emph{at most one} preimage.  On the other hand, there is \emph{no} function from $\{ \emptyset \}$ to $\emptyset$ (by \cref{exrA.1.23}.\cref{enmA.1.23.i}), and so certainly no left-inverse to the function from $\emptyset$ to $\{ \emptyset \}$.  This is why we require the sets to be nonempty.
\end{rmk}
\end{exr}
\begin{exr}{}{exrA.1.30}
Let $f\colon X\rightarrow Y$ be a function, and let $\collection{S}$ and $\collection{T}$ be a collection of subsets of $X$ and $Y$ respectively.  Show that the following statements are true.
\begin{enumerate}
\item \label{enmA.1.30.i}$f^{-1}\left( \bigcup _{T\in \collection{T}}T\right) =\bigcup _{T\in \collection{T}}f^{-1}(T)$.
\item \label{enmA.1.30.ii}$f^{-1}\left( \bigcap _{T\in \collection{T}}T\right) =\bigcap _{T\in \collection{T}}f^{-1}(T)$.
\item \label{enmA.1.30.iii}$f\left( \bigcup _{S\in \collection{S}}S\right) =\bigcup _{S\in \collection{S}}f(S)$
\item \label{enmA.1.30.iv}$f\left( \bigcap _{S\in \collection{S}}S\right) \subseteq \bigcap _{S\in \collection{S}}f(S)$.
\end{enumerate}
Find an example to show that we need not have equality in \cref{enmA.1.30.iv}.  On the other hand, show that \cref{enmA.1.30.iv} is true if $f$ is injective.
\end{exr}
\begin{exr}{}{exrA.1.10}
Show that
\begin{enumerate}
\item the composition of two injections is an injection;
\item the composition of two surjections is a surjection; and
\item the composition of two bijections is a bijection.
\end{enumerate}
\end{exr}
\begin{exr}{}{exrA.1.47}
Let $f\colon X\rightarrow Y$ be a function, let $S\subseteq X$, and let $T\subseteq Y$.  Show that the following statements are true.
\begin{enumerate}
\item \label{enmA.1.47.i}$f\left( f^{-1}(T)\right) \subseteq T$, with equality for all $T$ iff $f$ is surjective.
\item \label{enmA.1.47.ii}$f^{-1}\left( f(S)\right) \supseteq S$, with equality for all $S$ iff $f$ is injective.
\end{enumerate}
Find examples to show that we need not have equality in general.
\begin{rmk}
Maybe this is a bit silly, but I remember which one is which as follows.  First of all, write these both using $\subseteq$, not $\supseteq$, that is, $S\subseteq f^{-1}(f(S))$ and $f(f^{-1}(S))\subseteq S$.  Then, the ``$-1$'' is always closest to the symbol that represents being `smaller' (that is ``$\subseteq$'').  It is easy to remember which conditions imply equality if you remember that surjective functions have right-inverses and injective functions have left-inverse.\footnote{Modulo the stupid case when the domain is the empty-set---see the remark in \cref{exrA.1.9}.}
\end{rmk}
\end{exr}
\begin{exr}{}{exrA.1.27}
Let $X$ and $Y$ be sets, and let $x_0\in X$ and $y_0\in Y$.  If there is some bijection from $X$ to $Y$, show that in fact there is a bijection from $X$ to $Y$ which sends $x_0$ to $y_0$.
\end{exr}
\begin{exr}{}{exrA.1.26x}
Let $X$ be a set.  Construct a bijection from $2^X$, the power set of $X$, to $\{ 0,1\}^X$, the set of functions from $X$ into $\{ 0,1\}$.
\begin{rmk}
This is the motivation for the notation $2^X$ to denote the power set.
\end{rmk}
\end{exr}

\subsection{Arbitrary disjoint-unions and products}

\begin{dfn}{Disjoint-union (of a collection)}{DisjointUnionCollection}
Let $\collection{X}$ be an indexed collection\footnote{By \term{indexed collection}\index{Indexed collection} we mean a set in which elements are allowed to be repeated.  So, for example, $\collection{X}$ is allowed to contain two copies of $\N$.  The reason for the term ``\emph{indexed} collection'' is that indices are often used to distinguish between the two identical copies, e.g.,~$\collection{Y}=\{ \N _1,\N _2\}$---as sets are not allowed to `repeat' elements, we add the indices so that, strictly speaking, $\N _1\neq \N _2$ as elements of $\collection{X}$, even though they represent the same set.  (If this is confusing, don't think about it too hard---it's just a set where elements are allowed to be repeated.)} of sets.  Then, the \term{disjoint-union}\index{Disjoint-union} over all $X\in \collection{X}$, $\coprod _{X\in \collection{X}}X$\index[notation]{$\coprod _{X\in \collection{X}}X$}, is
\begin{equation}
\coprod _{X\in \collection{X}}X\coloneqq \{ \coord{x,X}:X\in \collection{X}\, x\in X\} .
\end{equation}
\begin{rmk}
The intuition and way to think of notation is just the same as it was in the simpler case of the disjoint-union of two sets (\cref{DisjointUnion}).
\end{rmk}
\end{dfn}
\begin{dfn}{Restrictions (of functions on a dis\-joint-union)}{}
Let $\collection{X}$ be an indexed collection of sets, let $Y$ be a set, and let $f\colon \coprod _{X\in \collection{X}}X\rightarrow Y$ be a function.  Then, the \term{restriction of $f$ to $X$}\index{Restriction (disjoint-union)}, $\restr{f}{X}:X\rightarrow Y$\index[notation]{$\restr{f}{X}$}, is defined by
\begin{equation}
\restr{f}{X}(x)\coloneqq f(\coord{x,X}).
\end{equation}
In particular, the \term{inclusion}\index{Inclusion (disjoint-union)} is defined to be
\begin{equation}
\iota _X\coloneqq \restr{[\id _{\coprod _{X\in \collection{X}}}]}{X},
\end{equation}
that is, the restriction of the identity $\id _{\coprod _{X\in \collection{X}}X}:\coprod _{X\in \collection{X}}X\rightarrow \coprod _{X\in \collection{X}}X$.
\begin{rmk}
While from a set-theoretic perspective, this is just a special case of restriction (see \cref{RestrictionAndCorestriction}), we state it separately because we wish to draw an analogy with projections (see \cref{Components}), a concept which is not a special case of something we have seen before. 
\end{rmk}
\end{dfn}

\begin{dfn}{Cartesian-product (of a collection)}{CartesianProductCollection}
Let $\collection{X}$ be an indexed collection of sets.  Then, the \term{Cartesian-product}\index{Cartesian-product} over all $X\in \collection{X}$, $\prod _{X\in \collection{X}}X$\index[notation]{$\prod _{X\in \collection{X}}X$}, is
\begin{equation}
\prod _{X\in \collection{X}}X\coloneqq \left\{ f\colon \collection{X}\rightarrow \coprod _{X\in \collection{X}}X:f(X)\in X\right\} .
\end{equation}
\begin{rmk}
Admittedly this notation is a bit obtuse.  The cartesian-product is still supposed to be thought of a collection of ordered-`pairs', except now the pairs aren't just pairs, but can be $3$, $4$, or even infinitely many `coordinates'.  The coordinates are indexed by elements of $\collection{X}$, and the $X$-coordinate for $X\in \collection{X}$ must lie in $X$ itself.  Thus, for example, $X_1\times X_2=\prod _{X\in \collection{X}}X$ for $\collection{X}=\{ X_1,X_2\}$.  The key that is probably potentially the most confusing is that the elements of $\collection{X}$ are playing more than one role:  on one hand, they index the coordinates, and on the other hand, they are the set in which the coordinates take their values.  Hopefully keeping in mind the case $\collection{X}=\{ X_1,X_2\}$ helps this make sense.  So, for example, in the statement ``$f(X)\in X$'', on the left-hand side, $X$ is being thought of as an `index', and on the right-hand side it is being thought of as the `space' in which a coordinate `lives'.  This is thus literally just the statement that the $X$-coordinate of $f\in \prod _{X\in \collection{X}}X$ must be an element of the set $X$.
\end{rmk}
\begin{rmk}
For $x\in \prod _{X\in \collection{X}}X$, we write $x_X\coloneqq x(X)$ for the \term{$X$-component}\index{Component (cartesian-product)} or \term{$X$-coordinate}\index{Coordinate (cartesian-product)}.
\end{rmk}
\begin{rmk}
For $x\in \collection{I}$, we may also suggestively write
\begin{equation}
\langle x_i:i\in \collection{I}\langle \ceqq x,
\end{equation}
analogous to how one writes $\coord{x,y}\in X\times Y$ for elements in a Cartesian product of two sets.  (We have only changed the letter of our indexing set\footnote{Like the term ``collection'', the term \term{indexing set} is technically just synonymous with the word ``set''.  There is nothing mathematically different about it.  This term is only used to clarify to the human readers out there how one should intuitively think of the set, specifically that it is a set whose elements are being used to ``index'' other things.} for legibility.)	
\end{rmk}
\begin{rmk}
For a function defined on a Cartesian product, say $f\colon X\times Y\rightarrow Z$, we shall write $f(x,y)\ceqq f(\coord{x,y})$\index[notation]{$f(x,y)$}.
\end{rmk}
\end{dfn}
\begin{dfn}{Components (of functions into a product)}{Components}
Let $\collection{X}$ be an indexed collection of sets, let $Y$ be a set, and let $f\colon Y\rightarrow \prod _{X\in \collection{X}}X$ be a function.  Then, the \term{$X$-component}\index{Component (of a function into a product)}, $f_X:Y\rightarrow X$\index[notation]{$f_X$}, is defined by
\begin{equation}
f_X(y)\coloneqq f(y)_X.
\end{equation}
In particular, the \term{projection}\index{Projection (cartesian-product)}, $\pi _X$\index[notation]{$\pi _X$}, is defined to be
\begin{equation}
\pi _X\coloneqq [\id _{\prod _{X\in \collection{X}}X}]_X,
\end{equation}
that is, it is the $X$-component of the identity $\id _{\prod _{X\in \collection{X}}X}:\prod _{X\in \collection{X}}X\rightarrow \prod _{X\in \collection{X}}X$.
\begin{rmk}
For example, in the case $f\colon Y\rightarrow X_1\times X_2$, then $f(y)=\coord{f_1(y),f_2(y)}$.
\end{rmk}
\end{dfn}
\begin{exr}{}{exrA.3.41}
Let $\collection{I}$ and $X$ be sets.
\begin{enumerate}
\item \label{exrA.3.41(i)}Find a bijection
\begin{equation}
\collection{I}\times X\rightarrow \coprod _{i\in \collection{I}}X.
\end{equation}
\item \label{exrA.3.41(ii)}Find a bijection
\begin{equation}
X^{\collection{I}}\rightarrow \prod _{i\in \collection{I}}X.
\end{equation}
\end{enumerate}
\begin{rmk}
\cref{exrA.3.41(i)} says that if all the sets appearing in a disjoint-union are the same, then that disjoint-union is `the same as' the Cartesian product of the indexing set and the single set appearing in the disjoint union.
	
Similarly, \cref{exrA.3.41(ii)} says that if all the sets appearing in are Cartesian-product are the same, then it is `the same as' the set of all functions from the indexing set to the single set appearing in the product.
\end{rmk}
\end{exr}

\horizontalrule

Before introducing other important special cases of relations, we must first introduce several properties of relations.
\begin{dfn}{}{}
Let $\sim$ be a relation on a set $X$.
\begin{enumerate}
\item (Reflexive) $\sim$ is \term{reflexive}\index{Reflexive} iff $x\sim x$ for all $x\in X$.
\item (Symmetric) $\sim$ is \term{symmetric}\index{Symmetric} iff $x_1\sim x_2$ is equivalent to $x_2\sim x_1$ for all $x_1,x_2\in X$.
\item (Transitive) $\sim$ is \term{transitive}\index{Transitive} iff $x_1\sim x_2$ and $x_2\sim x_3$ implies $x_1\sim x_3$.
\item (Antisymmetric) $\sim$ is \term{antisymmetric}\index{Antisymmetric} iff $x_1\sim x_2$ and $x_2\sim x_1$ implies $x_1=x_2$.\footnote{Admittedly the terminology here with ``symmetric'' and ``antisymmetric'' is a bit unfortunate.}
\item (Total) $\sim$ is \term{total}\index{Total} iff for every $x_1,x_2\in X$, $x_1\sim x_2$ or $x_2\sim x_1$.
\end{enumerate}
\end{dfn}

\subsection{Equivalence relations}

\begin{dfn}{Equivalence relation}{}
An \term{equivalence relation}\index{Equivalence relation} on a set $X$ is a relation on $X$ that is reflexive, symmetric, and transitive.
\end{dfn}
\begin{exm}{Integers modulo $m$}{exmA.1.53}
Let $m\in \Z ^+$ and let $x,y\in \Z$.  Then, $x$ and $y$ are \term{congruent modulo $m$}, written $x\cong y\bpmod{m}$, iff $x-y$ is divisible by $m$.
\begin{exr}{}{}
Check that $\cong \bpmod{m}$ is an equivalence relation.
\end{exr}
For example, $3$ and $10$ are congruent modulo $7$, $1$ and $-3$ are congruent modulo $4$, $-2$ and $6$ are congruent modulo $8$, etc..
\begin{rmk}
We will see a `better' way of viewing the integers modulo $m$ in \cref{exmA.1.117}.  It is better in the sense that it is much more elegant and concise, but requires a bit of machinery and will probably not be as transparent if you have never seen it before.  Thus, it is probably more enlightening, at least the first time, to see things spelled out in explicit detail.
\end{rmk}
\end{exm}
\begin{dfn}{Equivalence class}{}
Let $\sim$ be an equivalence relation on a set $X$ and let $x_0\in X$.  Then, the \term{equivalence class}\index{Equivalence class} of $x_0$, $[x_0]_\sim$\index[notation]{$[x_0]_\sim$}, is
\begin{equation}\label{A.1.10}
[x_0]_\sim \coloneqq \left\{ x\in X:x\sim x_0\right\} =\left\{ x\in X:x_0\sim x\right\} .
\end{equation}
\begin{rmk}
If $\sim$ is clear from context, we may simply write $[x_0]\ceqq [x_0]_{\sim}$\index[notation]{$[x_0]$}.
\end{rmk}
\begin{rmk}
We may also on occasion write $x_0/\sim \ceqq [x_0]_{\sim}$\index[notation]{$x_0/\sim$} for the equivalence class.
\end{rmk}
\begin{rmk}
In words, the equivalence class of $x_0$ is the set of elements equivalent to $x$.
\end{rmk}
\begin{rmk}
Note that the second equation of \eqref{A.1.10} uses the symmetry of the relation.
\end{rmk}
\end{dfn}
\begin{exm}{Integers modulo $m$}{exmA.1.57}
This is a continuation of \cref{exmA.1.53}.  For example, the equivalence class of $5$ modulo $6$ is
\begin{equation}
[5]_{\cong \bpmod{6}}=\left\{ \ldots ,-1,5,11,17,\ldots \right\} ,
\end{equation}
the equivalence class of $-1$ modulo $8$ is
\begin{equation}
[1]_{\cong \bpmod{8}}=\left\{ \ldots ,-17,-9,-1,7,15,\ldots \right\} ,
\end{equation}
etc..
\end{exm}
An incredibly important property of equivalence classes is that they form a partition of the set.
\begin{dfn}{Partition}{dfnA.1.11}
Let $X$ be a set.  Then, a \term{partition}\index{Partition} of $X$ is a collection $\collection{X}$ of subsets of $X$ such that
\begin{enumerate}
\item \label{A.1.11.1}$X=\bigcup _{U\in \collection{X}}U$; and
\item \label{A.1.11.2}for $U_1,U_2\in \collection{X}$ either $U_1=U_2$ or $U_1$ is disjoint from $U_2$.
\end{enumerate}
\end{dfn}
\begin{prp}{}{prpA.1.12}
Let $\sim$ be an equivalence relation on a set $X$ and let $x_1,x_2\in X$.  Then, either (i)~$x_1\sim x_2$ or (ii)~$[x_1]_\sim$ is disjoint from $[x_2]_\sim$.
\begin{proof}
If $x_1\sim x_2$ we are done, so suppose that this is not the case.  We wish to show that $[x_1]_\sim$ is disjoint from $[x_2]_\sim$, so suppose that this is not the case.  Then, there is some $x_3\in X$ with $x_1\sim x_3$ and $x_3\sim x_2$.  Then, by transitivity $x_1\sim x_2$:  a contradiction.  Thus, it must be the case that $[x_1]_\sim$ is disjoint from $[x_2]_\sim$.
\end{proof}
\end{prp}
\begin{crl}{}{crlA.1.13}
Let $X$ be a set and let $\sim$ be an equivalence relation on $X$.  Then, the collection $\collection{X}\coloneqq \left\{ [x]_\sim :x\in X\right\}$ is a partition of $X$.
\begin{proof}
The previous proposition, \cref{prpA.1.12}, tells us that $\collection{X}$ has property \cref{A.1.11.2} of the definition of a partition, \cref{dfnA.1.11}.  Property \cref{A.1.11.1} follows from the fact that $x\in [x]_\sim$, so that indeed
\begin{equation}
X=\bigcup _{x\in X}[x]_\sim =\bigcup _{U\in \collection{X}}U.
\end{equation}
\end{proof}
\end{crl}
Conversely, a partition of a set defines an equivalence relation.
\begin{exr}{}{exrA.1.41}
Let $X$ be a set, let $\collection{X}$ be a partition of $X$, and define $x_1\sim x_2$ iff there is some $U\in \collection{X}$ such that $x_1,x_2\in U$.  Show that $\sim$ is an equivalence relation.
\end{exr}
\begin{exm}{Integers modulo $m$}{exmA.1.63}
This in turn is a continuation of \cref{exmA.1.57}.  The equivalence classes modulo $4$ are
\begin{equation}
\begin{split}
[0]_{\cong \bpmod{4}} & =\left\{ \ldots ,-8,-4,0,4,8,\ldots \right\} \\
[1]_{\cong \bpmod{4}} & =\left\{ \ldots ,-7,-3,1,5,9,\ldots \right\} \\
[2]_{\cong \bpmod{4}} & =\left\{ \ldots ,-6,-2,2,6,10,\ldots \right\} \\
[3]_{\cong \bpmod{4}} & =\left\{ \ldots ,-5,-1,3,7,11,\ldots \right\} .
\end{split}
\end{equation}
You can verify directly that (i)~each integer appears in at least one of these equivalence classes and (ii)~that no integer appears in more than one.  Thus, indeed, the set $\left\{ [0]_{\cong \bpmod{4}},[1]_{\cong \bpmod{4}},[2]_{\cong \bpmod{4}},[3]_{\cong \bpmod{4}}\right\}$ is a partition of $\Z$.
\end{exm}

Given a set $X$ with an equivalence relation $\sim$, we obtain a new set $X/\sim$, the collection of all equivalence classes of elements in $X$ with respect to $\sim$.
\begin{dfn}{Quotient set}{dfnA.1.42}
Let $\sim$ be an equivalence relation on a set $X$.  Then, the \term{quotient of $X$ with respect to $\sim$}\index{Quotient set}, $X/\sim$, is defined by
\begin{equation}
X/\sim \coloneqq \left\{ [x]_\sim :x\in X\right\} .
\end{equation}
The function $\q :X\rightarrow X/\sim$ defined by $\q (x)\coloneqq [x]_\sim$ is the \term{quotient function}\index{Quotient function}.
\begin{rmk}
	If one wants to define a function $f$ on $X/\sim$, often times one will define $f([x]_{\sim})$ in terms of $x$ itself.  This is dubious, however, as how do we know that our definition gives the same result if $x_1\sim x_2$?  (We can't have $f([x_1]_{\sim})\neq f([x_2]_{\sim})$ if $[x_1]_{\sim}=[x_2]_{\sim}$, now can we?)  Thus, if ever we do want to make a definition like this we must first prove that our definition does not depend on the ``representative'' of the equivalence class $[x]_{\sim}$ we have chosen.  More precisely, we must show that if $x_1\sim x_2$, then $f(x_1)=f(x_2)$.  If this is the case, we say that our definition $f$ is \term{well-defined}\index{Well-defined}.  (We elaborate on this below.)
\end{rmk}
\end{dfn}
Of course the quotient function is surjective.  What's perhaps a bit more surprising is that \emph{every} surjective function can be viewed as the quotient function with respect to some equivalence relation.
\begin{exr}{}{exrA.1.81}
Let $\q :X\rightarrow Y$ be surjective and for $x_1,x_2\in X$, define $x_1\sim _{\q}x_2$\index[notation]{$\sim _{\q}$} iff $x_1,x_2\in \q ^{-1}(y)$ for some $y\in Y$.  Show that (i)~$\sim _{\q}$ is an equivalence relation on $X$ and (ii)~that $\q (x)=[x]_{\sim _{\q}}$.
\end{exr}
\begin{exm}{Integers modulo $m$}{exmA.1.69}
This in turn is a continuation of \cref{exmA.1.63}.  For example, the quotient set mod $5$ is
\begin{equation}
\begin{multlined}
\Z /\cong \, (\operatorname{mod}\; 5) \\ =\left\{ [0]_{\cong \bpmod{5}},[1]_{\cong \bpmod{5}},[2]_{\cong \bpmod{5}},\right. \\ \left. [3]_{\cong \bpmod{5}},[4]_{\cong \bpmod{5}}\right\} .
\end{multlined}
\end{equation}
\end{exm}

It is quite common for us, after having defined the quotient set, to want to define operations on the quotient set itself.  For example, we would like to be able to add integers modulo $24$ (we do this when telling time).  In this example, we could make the following definition.
\begin{equation}
[x]_{\cong \bpmod{24}}+[y]_{\cong \bpmod{24}}\coloneqq [x+y]_{\cong \bpmod{24}}.
\end{equation}
This is okay, but before we proceed, we have to check that this definition is \emph{well-defined}.  That is, there is a potential problem here, and we have to check that this potential problem doesn't actually happen.  I will try to explain what the potential problem is.

Suppose we want to add $3$ and $5$ modulo $7$.  On one hand, we could just do the obvious thing $3+5=8$.  But because we are working with \emph{equivalence classes}, I should just as well be able to add $10$ and $5$ and get the same answer.  In this case, I get $10+5=15$.  At first glance, it might seem we got different answers, but, alas, while $8$ and $15$ are not the same integer, they `are' the same \emph{equivalence class} modulo $7$.

In symbols, if I take two integers $x_1$ and $x_2$ and add them, and you take two integers $y_1$ and $y_2$ \emph{with $y_1$ equivalent to $x_1$ and $y_2$ equivalent to $x_2$}, it had better be the case that $x_1+x_2$ is equivalent to $y_1+y_2$.  That is, the answer should not depend on the ``representative'' of the equivalence class we chose to do the addition with.
\begin{exm}{Integers modulo $m$}{}
This in turn is a continuation of \cref{exmA.1.69}.  Let $m\in \Z ^+$, let $x_1,x_2\in \Z$, and define
\begin{equation}
[x_1]_{\cong \bpmod{m}}+[x_2]_{\cong \bpmod{m}}\coloneqq [x_1+x_2]_{\cong \bpmod{m}}.
\end{equation}
We check that this is well-defined.  Suppose that $y_1\cong x_1\bpmod{m}$ and $y_2\cong x_2\bpmod{m}$.  We must show that $x_1+x_2\cong y_1+y_2\bpmod{m}$.  Because $y_k\cong x_k\bpmod{m}$, we know that $y_k-x_k$ is divisible by $m$, and hence $(y_1-x_1)+(y_2-x_2)=(y_1+y_2)-(x_1+x_2)$ is divisible by $m$.  But this is just the statement that $x_1+x_2\cong y_1+y_2\bpmod{m}$, exactly what we wanted to prove.
\begin{exr}[breakable=false]{}{}
Define multiplication modulo $m$ and show that is is well-defined.
\end{exr}
\end{exm}

\subsection{Preorders}

\begin{dfn}{Preorder}{dfnA.1.19}
A \term{preorder}\index{Preorder} on a set $X$ is a relation $\leq$ on $X$ that is reflexive and transitive.  A set equipped with a preorder is a \term{preordered set}\index{Preordered set}.
\begin{rmk}
The notation $x_1<x_2$\index[notation]{$x_1<x_2$} is shorthand for ``$x_1\leq x_2$ and $x_1\neq x_2$''.
\end{rmk}
\begin{rmk}
Note that an equivalence relation is just a very special type of preorder.
\end{rmk}
\end{dfn}
\begin{exr}{}{}
Find an example of
\begin{enumerate}
\item a relation that is both reflexive and transitive (i.e.~a preorder);
\item a relation that is reflexive but not transitive;
\item a relation that is not reflexive but transitive; and
\item a relation that is neither reflexive nor transitive.
\end{enumerate}
\end{exr}
The notion of an \emph{interval} is obviously important in mathematics and you almost have certainly encountered them before in calculus.  We give here the abstract definition (it of course turns out that this agrees with the definition you are familiar in case $X=\R$ with the usual order).
\begin{dfn}{Interval}{Interval}
Let $\coord{X,\leq}$ be a preordered set and let $I\subseteq X$.  Then, $I$ is an \term{interval}\index{Interval} iff for all $x_1,x_2\in I$ with $x_1\leq x_2$, whenever $x_1\leq x\leq x_2$, it follows that $x\in I$.
\begin{rmk}
In other words, $I$ is an interval iff everything in-between two elements of $I$ is also in $I$.
\end{rmk}
\begin{rmk}
As you are probably aware, the following notation is common.
\begin{subequations}
\begin{align*}
[x_1,x_2] & \coloneqq \{ x\in X:x_1\leq x\leq x_2\} \\
(x_1,x_2) & \coloneqq \{ x\in X:x_1<x<x_2\} \\
[x_1,x_2) & \coloneqq \{ x\in X:x_1\leq x<x_2\} \\
(x_1,x_2] & \coloneqq \{ x\in X:x_1<x\leq x_2\} .
\end{align*}
\end{subequations}
The first and second are called respectively \term{closed intervals}\index{Closed interval} and \term{open intervals}\index{Open interval}.  Terminology for the third and fourth is less common, but you might call them respectively the \term{half-closed-open intervals}\index{Half-closed-open interval} and \term{half-open-closed intervals}\index{Half-open-closed interval}.

Feel free to check that these sets are all in fact intervals.\footnote{Warning:  Though there can be intervals not of this form---for example, $\{ x\in \Q :0\leq x\leq \sqrt{2}\}$ is not of this form.}
\end{rmk}
\end{dfn}
\begin{dfn}{Monotone}{dfnA.1.21}
Let $X$ and $Y$ be preordered sets and let $f\colon X\rightarrow Y$ be a function.  Then, $f$ is \term{nondecreasing}\index{Nondecreasing} iff $x_1\leq x_2$ implies that $f(x_1)\leq f(x_2)$.  If the second inequality is strict for distinct $x_1$ and $x_2$, i.e.~if $x_1<x_2$ implies $f(x_1)<f(x_2)$, then $f$ is \term{increasing}\index{Increasing}.  If the inequality is in the other direction, i.e.~if $x_1\leq x_2$ implies $f(x_1)\geq f(x_2)$, then $f$ is \term{nonincreasing}\index{Nonincreasing}.  If it is both strict and reversed, i.e.~if $x_1<x_2$ implies $f(x_1)>f(x_2)$, then $f$ is \term{decreasing}\index{Decreasing}.  $f$ is \term{monotone} iff it is either nondecreasing or nonincreasing and $f$ is \term{strictly monotone} iff it is either increasing or decreasing.
\begin{rmk}
Note that the $\leq$ that appears in $x_1\leq x_2$ is \emph{different} than the $\leq$ that appears in $f(x_1)\leq f(x_2)$:  the former is the preorder on $X$ and the latter is the preorder on $Y$.  We will often abuse notation in this manner.
\end{rmk}
\end{dfn}

In this course, we will almost always be dealing with preordered sets whose preorder is in addition antisymmetric (or are equivalence relations).
\begin{dfn}{Partial-order}{dfnA.1.24}
A \term{partial-order}\index{Partial-order} is an antisymmetric preorder.  A set equipped with a partial-order is a \term{partially-ordered set}\index{Partially-ordered set} or a \term{poset}\index{Poset}.
\end{dfn}
There are two preorders that you can define on any set.  They are not terribly useful, except perhaps for producing counter-examples.
\begin{exm}{Discrete and indiscrete (orders}{exmA.1.95}
	Let $X$ be a set.  Declare $x_1\leq _{\mrm{D}}x_2$ iff $x_1=x_2$.  That is, $x\leq _{\mrm{D}}x$ is true, and nothing else.
	\begin{exr}[breakable=false]{}{}
		Show that $\coord{X,\leq _{\mrm{D}}}$ is a partial-order.
		\begin{rmk}
			$\leq _{\mrm{D}}$ is the \term{discrete-order}\index{Discrete-order} on $X$.
		\end{rmk}
	\end{exr}
	
	Now declare $x_1\leq _{\mrm{I}}x_2$ for all $x_1,x_2\in X$.  That is, $x_1\leq _{\mrm{I}}x_2$ is \emph{always} true.
	\begin{exr}[breakable=false]{}{}
		Show that $\coord{X,\leq _{\mrm{I}}}$ is a total preorder that is not antisymmetric in general.
		\begin{rmk}
			In particular, this shows that there are total preorders which are not partial-orders (\cref{dfnA.1.24}).
		\end{rmk}
		\begin{rmk}
			$\leq _{\mrm{I}}$ is the \term{indiscrete-order}\index{Indiscrete-order} on $X$.
		\end{rmk}
	\end{exr}
\end{exm}
A much more useful collection of examples of partially-ordered sets is that are those exhibited as power-sets.
\begin{exm}{Power set}{}
The archetypal example of a partially-ordered set is given by the power set.  Let $X$ be a set and for $U,V\in 2^X$, define $U\leq V$ iff $U\subseteq V$.
\begin{exr}[breakable=false]{}{exrA.1.26}
Check that $\coord{2^X,\leq}$ is in fact a partially-ordered set.
\end{exr}
\end{exm}
\begin{exr}{}{}
What is an example of a preorder that is not a partial-order?
\end{exr}

While we will certainly be dealing with nontotal partially-ordered sets, totality of an ordering is another property we will commonly come across.
\begin{dfn}{Total-order}{TotalOrder}
A \term{total-order}\index{Total-order} is a total partial-order.  A set equipped with a total-order is a \term{totally-ordered set}\index{Totally-ordered set}.
\end{dfn}
\begin{exr}{}{}
What is an example of a partially-ordered set that is not a totally-ordered set.
\end{exr}

And finally we come to the notion of well-ordering, which is an incredibly important property of the natural numbers.
\begin{dfn}{Well-order}{WellOrder}
A \term{well-order}\index{Well-order} on a set $X$ is a total-order that has the property that every nonempty subset of $X$ has a smallest element.  A set equipped with a well-order is a \term{well-ordered set}\index{Well-ordered set}. 
\end{dfn}
In fact, we do not need to assume a priori that the order is a total-order.  This follows simply from the fact that every nonempty subset has a smallest element.
\begin{prp}{}{prpA.1.51}
Let $X$ be a partially-ordered set that has the property that every nonempty subset of $X$ has a smallest element.  Then, $X$ is totally-ordered (and hence well-ordered).
\begin{proof}
Let $x_1,x_2\in X$.  Then, the set $\{ x_1,x_2\}$ has a smallest element.  If this element is $x_1$, then $x_1\leq x_2$.  If this element is $x_2$, then $x_2\leq x_1$.  Thus, the order is total, and so $X$ is totally-ordered.
\end{proof}
\end{prp}
\begin{exr}{}{}
What is an example of a totally-ordered set that is not a well-ordered set?
\end{exr}

\subsection{Well-founded induction}\label{sbsWellFoundedInduction}

In the very beginning of this chapter when discussing proof techniques (\cref{sbsABitAboutProofs}), we mentioned \emph{well-founded induction}.  It is time we return to this and make the statement precise.
\begin{dfn}{Well-founded}{WellFounded}
	Let $X$ be a set and let $\preceq$ be a relation on $X$.  Then, $\coord{X,\preceq}$ is \term{well-founded}\index{Well-founded} iff every nonempty subset of $X$ has a minimal element.
\end{dfn}
The relationship between being well-ordered and well-founded is as follows.
\begin{prp}{}{}
	Let $X$ be a set and let $\preceq$ be a relation on $X$.  Then, $\coord{X,\preceq}$ is a well-ordered set iff $\preceq$ is a well-founded total-order.
	\begin{proof}
		$(\Rightarrow )$ Suppose that $\coord{X,\preceq}$ is a well-ordered set.  Then, $\preceq$ is well-founded because minima are minimal.  Let $x_1,x_2\in X$.  Then, $\{ x_1,x_2\}$ is nonempty, and so has a minimum, say $x_1$.  Then, $x_1\preceq x_2$, and so $\preceq$ is total.
		
		\blankline
		\noindent
		$(\Leftarrow )$ Suppose that $\preceq$ is a well-founded total-order.  Let $S\subseteq X$ be nonempty.  Then, $S$ has a minimal element $s_0\in S$.  We wish to show that $s_0$ is a minimum of $S$.  So, let $s\in S$.  We wish to show that $s_0\preceq s$.  By totality, either $s_0\preceq s$ or $s\preceq s_0$.  In the former case, we are done.  In the latter case, by minimality, we have $s=s_0$.  By reflexivity (total-orders are reflexive), this would imply $s_0\preceq s$, and we are done.
	\end{proof}
\end{prp}
We are ultimately interested in well-foundedness itself because of its relevance to the most powerful form of induction (I know of).
\begin{thm}{Well-founded Induction}{WellFoundedInduction}\index{Well-founded Induction}
	Let $X$ be a set, let $\preceq$ be a well-founded relation on $X$, and let $\mcal{P}\colon X\rightarrow \{ 0,1\}$.  Then, if $\mcal{P}(y)=1$ for all $y\preceq x$, $y\neq x$, implies that $\mcal{P}(x)=1$, it follows that $\mcal{P}(x)=1$ for all $x\in X$.
	\begin{rmk}
		Of course, we are thinking of $\mcal{P}(x)$ as a statement that may or may not be true for a given $x$, and $\mcal{P}(x)=1$ corresponds to it being true and $\mcal{P}(x)=0$ corresponds to it being false.
	\end{rmk}
	\begin{rmk}
		Note how this gives us `normal' induction in case $X\ceqq \N$ and $\preceq \ceqq \leq$.  Indeed, it similarly generalizes transfinite induction (whatever that is).
	\end{rmk}
	\begin{proof}
		Suppose that $\mcal{P}(y)=1$ for all $y\preceq x$, $y\neq x$, implies that $\mcal{P}(x)=1$.  Define $S\ceqq \{ x\in X:\mcal{P}(x)=0\}$.  We wish to show that $S$ is empty.  We proceed by contradiction:  suppose that $S$ is nonempty.  Then, $S$ has a minimal element $s_0\in S$.  Let $y\in X$ be such that $y\preceq s_0$ and $y\neq s_0$.  If $y\in S$, then by minimality, we would have $y=s_0$, which is not the case.  Thus, $y\notin S$, and hence $\mcal{P}(y)=1$.  Thus, by hypotheses, $\mcal{P}(s)=1$:  a contradiction.
	\end{proof}
\end{thm}

\subsection{Zorn's Lemma}

We end this subsection with an incredibly important result known as \emph{Zorn's Lemma}.  At the moment, it's importance might not seem obvious, and perhaps one must see it in action in order to appreciate its significance.  For the time being at least, let me say this:  if ever you are trying to produce something maximal by adding things to a set one-by-one (e.g.~if you are trying to construct a basis by picking linearly-independent vectors one-by-one), but you are running into trouble because, somehow, this process will never stop, not even if you `go on forever':  give Zorn's Lemma a try.
\begin{dfn}{Upper-bound and lower-bound}{}
Let $\coord{X,\leq}$ be a preordered set, let $S\subseteq X$, and let $x\in X$.  Then, $x$ is an \term{upper-bound}\index{Upper bound} iff $s\leq x$ for all $s\in S$.  $x$ is a \term{lower-bound}\index{Lower-bound} iff $x\leq s$ for all $s\in S$.
\end{dfn}
\begin{dfn}{Maximum and minimum}{}
Let $\coord{X,\leq}$ be a preordered set and let $x\in X$.  Then, $x$ is a \term{maximum}\index{Maximum} of $X$ iff $x$ is an upper-bound of all of $X$.  $x$ is a \term{minimum}\index{Minimum} of $X$ iff $x$ is a lower-bound of all of $X$.
\end{dfn}
\begin{dfn}{Maximal and minimal}{MaximalAndMinimal}
Let $\coord{X,\leq}$ be a preordered set, let $S\subseteq X$, and let $x\in S$.  Then, $x$ is \term{maximal}\index{Maximal} in $S$ iff whenever $y\in S$ and $y\geq x$, it follows that $x=y$.  $x$ is \term{minimal}\index{Minimal} in $S$ iff whenever $y\in S$ and $y\leq x$ it follows that $x=y$.
\begin{rmk}
In other words, maximal means that there is no element in $S$ strictly greater than $x$ (and similarly for minimal).  Contrast this with maxi\emph{mum} and mini\emph{mum}:  if $x$ is a maximum of $S$ it means that $y\leq x$ for all $y\in S$ (and analogously for minimum).
\end{rmk}
\begin{rmk}
Note that, in a \emph{partially}-ordered set anyways, maximum elements are always maximal (see \cref{exrA.1.103}, but not conversely (and similarly for minimum and minimal) (see \cref{exmA.1.103}).
\end{rmk}
\end{dfn}
\begin{exm}{Maximum vs.~maximal}{exmA.1.103}
To understand the difference between maximal and maximum, consider the following diagram.\footnote{This diagram is meant to define a poset in which $E\leq C$, $E\leq D$, $C\leq A$, $C\leq B$, $D\leq A$, and $D\leq B$ (of course, these aren't the only relations---for example, we also mean to imply that $C\leq C$, that $E\leq A$, etc.).}
\begin{equation}
\begin{tikzcd}
A & & B \\
C \ar[u] \ar[rru] & & D \ar[llu] \ar[u] \\
  & E \ar[lu] \ar[ru] &
\end{tikzcd}
\end{equation}
Then, $A$ and $B$ are both \emph{maximal}, because nothing is strictly larger than them.  On the other hand, neither of them are \emph{maximum} (and in fact, there is no maximum), because neither of them is larger than everything ($A$ is not larger than $B$ and $B$ is not larger than $A$).  Of course, the difference between minimal and minimum is exactly analogous.
\end{exm}
\begin{exr}{}{exrA.1.103}
Let $X$ be a \emph{partially}-ordered set and let $S\subseteq X$.
\begin{enumerate}
\item Show that every maximum of $S$ is maximal in $S$.
\item Show that $S$ has at most one maximum element.
\item Come up with an example of $X$ and $S$ where $S$ has two distinct maximal elements.
\end{enumerate}
\end{exr}
\begin{dfn}{Downward-closed and upward-\\closed}{}
Let $X$ be a preordered set and let $S\subseteq X$.  Then, $S$ is \term{downward-closed}\index{Downward closed} in $X$ iff whenever $x\leq s\in S$ it follows that $x\in S$.  $S$ is \term{upward-closed}\index{Upward closed} in $X$ iff whenever $x\geq s\in S$ it follows that $x\in S$.
\end{dfn}
\begin{prp}{}{prpA.1.56}
Let $X$ be a well-ordered set and let $S\subset X$ be downward-closed in $X$.  Then, there is some $s_0\in X$ such that $S=\left\{ x\in X:x<s_0\right\}$.
\begin{proof}
As $S$ is a proper subset of $X$, $S^{\comp}$ is nonempty.  As $X$ is well-ordered, it follows that $S^{\comp}$ has a smallest element $s_0$.  We claim that $S=\left\{ x\in X:x<s_0\right\}$.  First of all, let $x\in X$ and suppose that $x<s_0$.  If it were \emph{not} the case that $x\in S$, then $s_0$ would no longer be the smallest element in $S^{\comp}$.  Hence, we must have that $x\in S$.  Conversely, let $x\in S$.  By totality, either $x\leq s_0$ or $s_0\leq x$.  As $x\in S$ and $s_0\in S^{\comp}$, we cannot have that $x=s_0$, so in fact, in the former case, we would have $x<s_0$, and we are done, so it suffices to show that $s_0\leq x$ cannot happen.  If $s_0\leq x$, then because $S$ is downward-closed in $X$ and $x\in S$, it would follows that $s_0\in S$:  a contradiction.  Therefore, it cannot be the case that $s_0\leq x$.
\end{proof}
\end{prp}
\begin{thm}{Zorn's Lemma}{ZornsLemma}\index{Zorn's Lemma}
Let $X$ be a partially-ordered set.  Then, if every well-ordered subset has an upper-bound, then $X$ has a maximal element.
\begin{proof}\footnote{Proof adapted from \cite{Grayson}.}
\Step{Make hypotheses}
Suppose that every well-ordered subset has an upper bound.  We proceed by contradiction:  suppose that $X$ has no maximal element.

\Step{Show that every well-ordered subset has an upper-bound \emph{not} contained in it}
Let $S\subseteq X$ be a well-ordered subset, and let $u$ be some upper-bound of $S$.  If there were no element in $X$ strictly greater than $u$, then $u$ would be a maximal element of $X$.  Thus, there is some $u'>u$.  It cannot be the case that $u'\in S$ because then we would have $u'\leq u$ because $u$ is an upper-bound of $S$.  But then the fact that $u'\leq u$ and $u\leq u'$ would imply that $u=u'$:  a contradiction.  Thus, $u'\notin S$, and so constitutes an upper-bound not contained in $S$.

\Step{Define $u(S)$}
For each well-ordered subset $S\subseteq X$, denote by $u(S)$ some upper-bound of $S$ not contained in $S$.

\Step{Define the notion of a $u$-set}
We will say that a well-ordered subset $S\subseteq X$ is a $u$-set iff $x_0=u\left( \left\{ x\in S:x<x_0\right\} \right)$ for all $x_0\in S$.

\Step{Show that for $u$-sets $S$ and $T$, either $S$ is downward-closed in $S$ or $T$ is downward closed in $S$}[ZornsLemma.5]
Define
\begin{equation}
D\coloneqq \bigcup _{\substack{A\subseteq X \\ A\text{ is downward-closed in }S \\ A\text{ is downward-closed in }T}}A.
\end{equation}
That is, $D$ is the union of all sets that are downward-closed in both $S$ and $T$.

We first check that $D$ itself is downward-closed in both $S$ and $T$.  Let $d\in D$, let $s\in S$, and suppose that $s\leq d$.  As $d\in D$, it follows that $d\in A$ for some $A\subseteq X$ downward-closed in both $S$ and $T$.  As $A$ is in particular downward-closed in $S$, it follows that $s\in D$, and so $D$ is downward-closed in $S$.  Similarly it is downward-closed in $T$.

If $D=S$, then $S=D$ is downward-closed in $T$, and we are done.  Likewise if $D=T$.  Thus, we may as well assume that $D$ is a proper subset of both $S$ and $T$.  Then, by \cref{prpA.1.56}, there are $s_0\in S$ and $t_0\in T$ such that $\{ s\in S:s<s_0\} =D=\{ t\in T:t<t_0\}$.  Because $S$ and $T$ are $u$-sets, it follows that
\begin{equation*}
s_0=u\left( \{ s\in S:s<s_0\} \right) =u\left( \{ t\in T:t<t_0\} \right) =t_0.
\end{equation*}
Define $D\cup \{ s_0\} \eqqcolon D'\coloneqq D\cup \{ t_0\}$.
Let $d\in D'$, let $s\in S$, and suppose that $s\leq d$.  Either $d=s_0$ or $d\in D$.  In the latter case, $d<s_0$.  Either way, $d\leq s_0$, and so we have that $s\leq d\leq s_0$, and so either $s=s_0$ or $s<s_0$; either way, $s\in D'$.  The conclusion is that $D'$ is downward-closed in $S$.  It is similarly downward-closed in $T$.  By the definition of $D$, we must have that $D'\subseteq D$:  a contradiction.  Thus, it could not have been the case that $D$ was a proper subset of both $S$ and $T$.

\Step{Define $U$}
Define
\begin{equation}
U\coloneqq \bigcup _{\substack{S\subseteq X \\ S\text{ is a }u\text{-set}}}S.
\end{equation}
We show that $U$ is a $u$-set in the next step.  Here, we argue that this is sufficient to complete the proof.

Define $U'\coloneqq U\cup \{ u(U)\}$.  We wish to check that $U'$ is likewise a $u$-set.  First note that $U'$ is still a well-ordered subset of $X$.  Now let $x_0\in U'$.  We wish to show that $x_0=u\left( \left\{ x\in U':x<x_0\right\} \right)$.  Note that $\left\{ x\in U':x<x_0\right\} =\left\{ x\in U:x<x_0\right\}$.  Hence, because $U$ is a $u$-set, $u\left( \left\{ x\in U':x<x_0\right\} \right) =x_0$, as desired.

Thus, as $U'$ is a $u$-set, from the definition of $U$, we will have $U'\subseteq U$:  a contradiction, which will complete the proof.  Thus, it does indeed suffice to show that $U$ is a $u$-set.

\Step{Finish the proof by showing that $U$ is a $u$-set}
We first need to check that $U$ is well-ordered.  Let $A\subseteq U$ be nonempty.  For $S\subseteq X$ a $u$-set, define $A_S\coloneqq A\cap S$.  For each $A_S$ that is nonempty, denote by $a_S$ the smallest element in $A_S$ (which exists as $S$ is in particular well-ordered).  Let $T\subseteq X$ be some other $u$-set.  Then, by \cref{ZornsLemma.5}, without loss of generality, $S$ is downward-closed in $T$.  In particular, $S\subseteq T$ so that $a_S\in T$.  Hence, $a_T\leq a_S$.  Then, because $S$ is downward-closed in $T$, $a_T\in S$, and hence $a_T\leq a_S$, and hence $a_T=a_S$.  We claim that this unique element is a smallest element of $A$.

To see this, let $a\in A$.  $a$ is then in particular an element of $U$, and there is some $u$-set $S$ such that $a\in S$.  Then, $a\in A_S\coloneqq A\cap A$, and hence $a_S\leq a$.

Let $u_0\in U$.  All that remains to be shown is that $u_0=u\left( \{ x\in U:x<u_0\} \right)$.  To do this, we first show  that every $u$-set is downward-closed in $U$.

Let $S\subseteq X$ be a $u$-set, let $s\in S$, let $x\in U$, and suppose that $x\leq s$.  As $x\in U$, there is some $u$-set $T$ such that $x\in T$.  Then, by \cref{ZornsLemma.5} again, either $S$ is downward-closed in $T$ or $T$ is downward-closed in $S$.  If the former case, then we have that $x\in S$ because $x\leq s$.  On the other hand, in the latter case, we have that $x\in S$ because $x\in T\subseteq S$.

Now we finally return to showing that $u_0=u\left( \{ x\in U:x<u_0\} \right)$.  By definition of $U$, $u_0\in S$ for some $u$-set $S$, and therefore, $u_0=u\left( \{ x\in S:x<u_0\} \right)$.  Therefore, it suffices to show that if $x\in U$ is less than $u_0$, then it is in $S$ (because then $\{ x\in S:x<u_0\} =\{ x\in U:x<u_0\}$)
u.  This, however, follows from the fact that $S$ is downward-closed in $U$.
\end{proof}
\end{thm}

\section{Sets with algebraic structure}

\begin{dfn}{Binary operation}{}
A \term{binary operation}\index{Binary operation} $\cdot$ on a set $X$ is a function $\cdot :X\times X\rightarrow X$.  It is customary to write $x_1\cdot x_2\coloneqq \cdot (x_1,x_2)$ for binary operations.
\begin{rmk}
Sometimes people say that \emph{closure} is an axiom.  This is not necessary.  That a binary operation on $X$ takes values \emph{in} $X$ implicitly says that the operation is closed.  That doesn't mean that you never have to check closure, however.  For example, in order to verify that the even integers $2\Z$ are a subrng (see \cref{dfnA.1.86}) of $\Z$, you do have to check closure---you need to check this in order that $+\colon 2\Z \times 2\Z \rightarrow 2\Z$ be a binary operation on $2\Z$ (and similarly for $\cdot$).
\end{rmk}
\end{dfn}
\begin{dfn}{}{}
Let $\cdot$ be a binary relation on a set $X$.
\begin{enumerate}
\item (Associative) $\cdot$ is \term{associative}\index{Associative} iff $(x_1\cdot x_2)\cdot x_3=x_1\cdot (x_2\cdot x_3)$ for all $x_1,x_2,x_3\in X$.
\item (Commutative) $\cdot$ is \term{commutative}\index{Commutative} if $x_1\cdot x_2=x_2\cdot x_1$ for all $x_1,x_2\in X$.
\item (Identity) An \term{identity} of $\cdot$ is an element $1\in X$ such that $1\cdot x=x=x\cdot 1$ for all $x\in X$.
\item (Inverse) If $\cdot$ has an identity and $x\in X$, then an \term{inverse} of $x$ is an element $x^{-1}\in X$ such that $x\cdot x^{-1}=1=x^{-1}\cdot x$.
\end{enumerate}
\end{dfn}

We first consider sets equipped just a single binary operation.
\begin{dfn}{Magma}{}
A \term{magma}\index{Magma} is a set equipped with a binary operation.
\end{dfn}
\begin{exr}{}{exrA.1.34}
Let $\coord{X,\cdot }$ be a magma and let $x_1,x_2,x_3\in X$.  Show that $x_1=x_2$ implies $x_1\cdot x_3=x_2\cdot x_3$.
\begin{rmk}
My hint is that the solution is so trivial that it is easy to overlook.
\end{rmk}
\begin{rmk}
This is what justifies the `trick' (if you can call it that) of doing the same thing to both sides of an equation that is so common in algebra.
\end{rmk}
\begin{rmk}
Note that the converse is not true in general.  That is, we can have $x_1\cdot x_2=x_1\cdot x_3$ with $x_2\neq x_3$.
\end{rmk}
\end{exr}
\begin{dfn}{Semigroup}{Semigroup}
A \term{semigroup}\index{Semigroup} is a magma $\coord{X,\cdot}$ such that $\cdot$ is associative.
\end{dfn}
\begin{dfn}{Monoid}{Monoid}
A \term{monoid}\index{Monoid} $\coord{X,\cdot ,1}$ is a semigroup $\coord{X,\cdot}$ equipped with an identity $1\in X$.
\end{dfn}
\begin{exr}{Identities are unique}{exrA.1.77}
Let $X$ be a monoid and let $1,1'\in X$ be such that $1\cdot x=x=x\cdot 1$ and $1'\cdot x=x=x\cdot 1'$ for all $x\in X$.  Show that $1=1'$.
\end{exr}
\begin{dfn}{Group}{Group}
A \term{group}\index{Group} is a monoid $\coord{X,\cdot ,1}$ equipped with a function $\blank ^{-1}:X\rightarrow X$ so that $x^{-1}$ is an inverse of $x$ for all $x\in X$.
\begin{rmk}
Usually this is just stated as ``$X$ has inverses.''.  This isn't wrong, but this way of thinking about things doesn't generalize to universal algebra or category theory quite as well.  The way to think about this is that, inverses, like the binary operation (as well as the identity) is \emph{additional structure}.  This is in contrast to the axiom of associativity which should be thought of as a \emph{property} satisfied by an \emph{already existing} structure (the binary operation).
\end{rmk}
\begin{rmk}
Usually we write $\coord{X,\cdot ,1,\blank ^{-1}}$ to denote a group $X$ with binary operation $\cdot$ with identity $1$ and with the inverse of $x\in X$ being given by $x^{-1}$.  However, especially if the group is commutative, it is also common to write $\coord{X,+,0,-}$ to denote the same thing.  In this case, what we previously would have written as $x^3$, would now be written as $3x$.  It is important to realize that, even though the symbols being used are different, the axioms they are required to satisfy are exactly the same---the change in notation serves no other purpose other than to be suggestive.
\end{rmk}
\end{dfn}
\begin{exr}{Inverses are unique}{exrA.1.79}
Let $X$ be a group, let $x\in X$, and let $y,z\in X$ both be inverses of $x$.  Show that $y=z$.
\end{exr}
\begin{exr}{}{}
Let $\coord{X,\cdot,1,\blank ^{-1}}$ be a group and let $x_1,x_2,x_3\in X$.  Show that if $x_1\cdot x_2=x_1\cdot x_3$, then $x_2=x_3$.
\begin{rmk}
Thus, the converse to \cref{exrA.1.34} holds in the case of a group.
\end{rmk}
\end{exr}
\begin{dfn}{Homomorphism (of magmas)}{HomomorphismOfMagmas}
Let $X$ and $Y$ be magmas and let $f\colon X\rightarrow Y$ be a function.  Then, $f$ is a \term{homomorphism}\index{Homomorphism (of magmas)} iff $f(x_1\cdot x_2)=f(x_1)\cdot f(x_2)$ for all $x_1,x_2\in X$.
\begin{rmk}
Informally, we say that ``$f$ \emph{preserves}'' the binary operation.
\end{rmk}
\begin{rmk}
Note that, once again, the $\cdot$ in $f(x_1\cdot x_2)$ is \emph{not} the same as the $\cdot$ in $f(x_1)\cdot f(x_2)$.  Confer the remark following the definition of a nondecreasing function, \cref{dfnA.1.21}.
\end{rmk}
\begin{rmk}
There are similar definitions for monoids and groups, with extra conditions because of the extra structure.  For monoids,\footnote{And more generally any magma with identity.} we additionally require that $\phi (1)=1$.  For groups, in turn additionally require that $\phi (x^{-1})=\phi (x)^{-1}$.  This is why we might say ``homomorphism of monoids'' instead of just ``homomorphism''---we are clarifying that we are additionally requiring this extra condition.
\end{rmk}
\end{dfn}

We now move on to the study of sets equipped with \emph{two} binary operations.
\begin{dfn}{Rg}{Rg}
A \term{rg}\index{Rg} is a set equipped with two binary operations $\coord{X,+,\cdot}$ such that
\begin{enumerate}
\item $\coord{X,+}$ is a commutative monoid,
\item $\coord{X,\cdot}$ is a semigroup, and
\item $\cdot$ \term{distributes}\index{Distributive} over $+$, that is, $x_1\cdot (x_2+x_3)=x_1\cdot x_2+x_1\cdot x_3$ and $(x_1+x_2)\cdot x_3=x_1\cdot x_3+x_2\cdot x_3$ for all $x_1,x_2,x_3\in X$.
\end{enumerate}
\begin{rmk}
In other words, writing out what it means for $\coord{X,+}$ to be a commutative monoid and for $\coord{X,\cdot}$ to be a a semigroup, these three properties are equivalent to
\begin{enumerate}
\item $+$ is associative,
\item $+$ is commutative,
\item $+$ has an identity,
\item $\cdot$ is associative,
\item $\cdot$ distributes over $+$.
\end{enumerate}
\end{rmk}
\begin{rmk}
For $x\in X$ and $m\in \Z ^+$, we write $m\cdot x\coloneqq \underbrace{x+\cdots +x}_{m}$.  Note that we do \emph{not} make this definition for $m=0\in \N$.  An empty-sum is \emph{always} $0$ (by definition), but $0\cdot x$ need not be $0$ in a general rg.
\end{rmk}
\begin{rmk}
Whenever we say that a rg is commutative, we mean that the \emph{multiplication} is commutative (this should be obvious---addition is always commutative).  Instead of saying referring to things as ``commutative rgs'' etc.~we will often shorten this to ``crg''\index{Crg}\index{Crig}\index{Cring}\index{Crng} etc..

As commutativity is such a nice property to have, elements which commute with everything have a special name:  $x\in X$ is \term{central}\index{Central} iff $x\cdot y=y\cdot x$ for all $y\in X$.
\end{rmk}
\begin{rmk}
I have actually never seen the term rg used before.  That being said, I haven't seen \emph{any} term to describe such an algebraic object before.  Nevertheless, I have seen both the terms rig and rng before (see below), and, well, given those terms, ``rg'' is pretty much the only reasonable term to give to such an algebraic object.  We don't have a need to work with rgs directly, but we will work with both rigs and rngs, and so it is nice to have an object of which both rigs and rngs are special cases.
\end{rmk}
\end{dfn}
\begin{dfn}{Rng}{dfnA.1.86}
A \term{rng}\index{Rng} is a rg such that $\coord{X,+,0,-}$ is a commutative group, that is, a rg that has additive inverses.
\end{dfn}
\begin{exr}{}{exrA.1.43}
Let $\coord{X,+,0-,\cdot}$ be a rng and let $x_1,x_2\in X$.  Prove the following properties.
\begin{enumerate}
\item $0\cdot x_1=0$ for all $x_1\in X$.
\item $(-x_1)\cdot x_2=-(x_1\cdot x_2)$ for all $x_1,x_2\in X$.
\end{enumerate}
\end{exr}
\begin{exm}{A rg that is not a rng}{}
The even natural numbers $2\N$ with their usual addition and multiplication is also an example of a rg that is not a rng.
\end{exm}
\begin{dfn}{Rig}{dfnA.1.33}
A \term{rig}\index{Rig} is a rg such that $\coord{X,\cdot ,1}$ is a monoid, that is, a rg that has a multiplicative identity.
\begin{rmk}
In a rig $R$, we write
{\footnotesize
\begin{equation}\label{eqnA.1.34}
    R^{\times}\coloneqq \left\{ r\in R:r\text{ has a multiplicative inverse.}\right\} .
\end{equation}\index[notation]{$R^{\times}$}
}
$R^{\times}$ is a group with respect to the ring multiplication and is known as the \term{group of units}\index{Group of units} in $R$.
\end{rmk}
\begin{rmk}
Just as the empty sum is defined to be $0$ in a rg (see the remark in \cref{Rg}), the empty product is defined to be $1$ in a rig.\footnote{Of course, similar conventions apply for all types of algebraic objects (in particular, for monoids), but we shall not keep repeating this.}
\end{rmk}
\begin{rmk}
I believe it is more common to refer to rigs as \emph{semirings}\index{Semiring}.  I dislike this terminology because it suggests an analogy with semigroups, of which there is none.  The term rig is also arguably more descriptive---even if you didn't know what the term meant, you might have a good chance of guessing, especially if you had seen the term rng before.
\end{rmk}
\end{dfn}
\begin{dfn}{Characteristic}{Characteristic}
Let $\coord{X,+,0,\cdot ,1}$ be a rig.  Then, either (i)~there is some $m\in \Z ^+$ such that $m\cdot 1=0\in X$ or (ii)~there is no such $m$.  In the former case, the smallest positive integer such that $\underbrace{1+\dots +1}_m=0\in X$ is the \term{characteristic}\index{Characteristic (of a rig)}, and in the latter case the \term{characteristic} is $0$.  We denote the characteristic by $\Char (X)$.
\begin{rmk}
For example, the characteristic of $\Z /m\Z$ is $m$, whereas the characteristic of $\Z$ is $0$.
\end{rmk}
\end{dfn}
\begin{exm}{A rg that is not a rig}{}
The even natural numbers $2\N$ with their usual addition and multiplication is a rg that is not a rig.
\end{exm}
\begin{dfn}{Ring}{Ring}
A \term{ring}\index{Ring} is a rg that is both a rig and a rng.
\begin{rmk}
The motivation for the terminology is as follows.  Historically, the term ``ring'' was the first to be used.  It is not uncommon for authors to use the term ring to mean both our definition and our definition minus the requirement of having a multiplicative identity.  To remove this ambiguity in terminology, we take the term ``ring'' to imply the existence of the identity and the removal of the ``i'' from the word is the term used for objects which do not necessarily have an identity.  Similarly, thinking of the ``n'' in ``ring'' as standing for ``negatives'', a rig is just a ring that does not necessarily posses additive inverses.
\end{rmk}
\begin{rmk}
Note that it follows from \cref{exrA.1.43} that $-1\cdot x=-x$ for all $x\in X$, $X$ a ring.
\end{rmk}
\end{dfn}
\begin{exr}{}{exrA.1.130}
Let $X$ be a ring and suppose that $0=1$.  Show that $X=\{ 0\}$.
\begin{rmk}
This is called the \term{zero cring}\index{Zero cring}.
\end{rmk}
\end{exr}
\begin{dfn}{Integral}{dfnA.1.69}
A rg $\coord{X,+,0,\cdot}$ is \term{integral}\index{Integral} iff it has the property that, whenever $x\cdot y=0$, it follows that either $x=0$ or $y=0$.
\begin{rmk}
Usually the adjective ``integral'' is applied only to crings, in which case people refer to this as an \term{integral domain}\index{Integral domain} instead of an integral cring.  As the natural numbers have this property (i.e.~$xy=0\Rightarrow x=0\text{ or }y=0$) I wanted an adjective that would describe rgs with this property and ``integral'' was an obvious choice because of common use of the term ``integral domain''.\footnote{The adjective ``integral'' itself also appears in the context of schemes, and the usage there is consistent with the usage here (in a sense that will be obvious on the off-chance you know what a scheme is).}  It is then just more systematic to refer to them as integral crings instead of integral domains.  This is usually not an issue because it is not very common to work with rigs or rgs.
\end{rmk}
\end{dfn}
\begin{dfn}{Division ring}{DivisionRing}
A \term{division ring}\index{Division ring} is a ring $\coord{X,+,0,-\cdot ,1}$ such that $\coord{X\setminus \{ 0\} ,\cdot ,1,\blank ^{-1}}$ is a group.
\begin{rmk}
In other words, a division ring is a ring in which every nonzero element has a multiplicative inverse.
\end{rmk}
\begin{rmk}
This condition makes just as much sense for rigs as it does for rings, however, to the best of my knowledge there is no accepted term for rigs in which every nonzero element has a multiplicative inverse (and as we shall have no need for such objects, we refrain from introducing a term ourselves).
\end{rmk}
\begin{rmk}
Sometimes people use the term \term{skew-field}\index{Skew-field} instead of division ring.
\end{rmk}
\end{dfn}
\begin{exr}{}{}
Show that all division rings are integral.
\end{exr}
\begin{dfn}{Field}{Field}
A \term{field}\index{Field} is a commutative division ring.
\end{dfn}
\begin{exr}{}{}
Let $F$ be a field with positive characteristic $p$.  Show that $p$ is prime.
\end{exr}
Given any rg $R$, we can define another rg, the \emph{opposite ring} of $R$, whose elements are the same but whose multiplication is in the `opposite' order.  Of course, this construction returns the same thing if $R$ is commutative, but not in general.  We don't elaborate too much on this because we don't make use of the construction very much, so you needn't worry if you don't immediate see it's use---remember, this appendix is primarily supposed to serve as a reference, to look definitions and facts up as you need them, not as a tool for learning per se.
\begin{dfn}{Opposite rg}{OppositeRg}
	Let $X$ be a rg.  Then, the \term{opposite rg}\index{Opposite rg} of $X$, $\coord{X^{\op},+^{\op},0^{\op},-^{\op},\cdot ^{\op}}$\index[notation]{$X^{\op}$}, is defined by
	\begin{data}
		\item $X^{\op}\ceqq X$;
		\item $x+^{\op}y\ceqq x+y$;
		\item $0^{\op}\ceqq 0$;
		\item $-^{\op}x\ceqq -x$; and
		\item $x\cdot ^{\op}y\ceqq y\cdot x$.
	\end{data}
	\begin{rmk}
		In other words, everything is the same except for the multiplication, with the new multiplication being defined to be the old multiplication in the ``opposite'' order.
	\end{rmk}
	\begin{rmk}
		Of course, if $X$ is a rig, $X^{\op}$ is canonically a rig, and similarly for rngs and rings.
	\end{rmk}
	\begin{rmk}
		Obviously, if $X$ is commutative, then $X=X^{\op}$ (as rgs, not just sets).
	\end{rmk}
\end{dfn}
\begin{dfn}{Homomorphism (of rgs)}{HomomorphismOfRgs}
Let $\coord{X,+,\cdot}$ and $\coord{Y,+,\cdot}$ be rgs and let $f\colon X\rightarrow Y$ be a function.  Then,$f$ is a \term{homomorphism}\index{Homomorphism (of rgs)} iff $f$ is both a homomorphism (of magmas) from $\coord{X,+}$ to $\coord{Y,+}$ and from $\coord{X,\cdot}$ to $\coord{Y,\cdot}$.
\begin{rmk}
Explicitly, this means that
\begin{equation}
f(x+y)=f(x)+f(y),f(0)=0
\end{equation}
and
\begin{equation}
f(xy)=f(x)f(y).
\end{equation}
\end{rmk}
\begin{rmk}
Similarly as in the definition of monoid homomorphisms \cref{HomomorphismOfMagmas}, we add corresponding extra conditions about preserving identities and inverses for rigs, rngs, and rings.\footnote{But \emph{not} fields.  This is why the definitions are stated in such a way that the additive inverses for rings are regarded as \emph{structure}, whereas the multiplicative inverses for fields are regarded as \emph{properties}---homomorphisms should preserve all ``structure''.  This is a subtle and, for now, unimportant point, and so if this doesn't make sense, you can ignore it for the time being.}
\end{rmk}
\end{dfn}

\subsection{Quotient groups and quotient rngs}

It is probably worth noting that this subsubsection is of relatively low priority.  We present this information here essentially because it gives a more unified, systematic, sophisticated, and elegant way to view things presented in other places in the notes, but it is also not really strictly required to understand these examples.

If you have never seen quotient rngs before, it may help to keep in the back of your mind a concrete example as you work through the definitions.  We recommend you keep in mind the example $R\coloneqq \Z$ and $I\coloneqq m\Z$ (all multiplies of $m$) for some $m\in \Z ^+$.  In this case, the quotient $R/I$ is (supposed to be, and in fact will turn-out to be) the integers modulo $m$.  While this is a quotient rng, it is also of course a quotient group (just forget about the multiplication), so this example may also help you think about quotient groups as well.

Before we get started with the precise mathematics, let's talk about the intuition.\footnote{I think it's fair to say that quotient algebraic structures are among the most difficult things students encounter when first beginning algebra, and so it is worthwhile to take some extra time to step back and think about what one is actually trying to accomplish.}  At a naive level, if you ask yourself ``How does one obtain $\Z /m\Z$ from $\Z$?'', while I suppose you might come up with other answers, the `correct' one is that ``You obtain $\Z /m\Z$ from $\Z$ by `setting $m=0$'.''.  The intuition and motivation for quotient rings is \emph{how to make precise the intuition of `setting things equal to zero'}.

For reasons of `consistency', you'll see that you can't \emph{just} set $m=0$.  If you set $m=0$, you must also set $m+m=2m=0$, and so on.  Thus, if you want to set $m=0$, in fact you must set all multiples of $m$ equal to zero.  In general, the sets of objects which are you `allowed' to set equal to zero at once are called \emph{ideals}.  Thus, $\{ m\}$ itself is not an ideal because it would be `inconsistent' to only set $m=0$.  Instead, you take the `ideal generated by $m$', which turns out to be $m\Z$, and set all elements of $m\Z$ equal to zero.  If $R$ is a rng and $I\subseteq R$ is an ideal, then $R/I$ is the notation we use to represent the rng obtained from $R$ by `setting' every element of $I$ equal to $0$.

As we shall use quotient groups to define quotient rngs, we do them first.  The first thing to notice is that every subgroup of a group induces an equivalence relation.
\begin{prp}{Cosets (in groups)}{Cosets}
Let $G$ be a group, let $H\subseteq G$, and define
\begin{equation}\label{eqnA.4.31}
g_1\cong g_2\bpmod{H}\text{ iff }g_2^{-1}g_1\in H\text{ for }g_1,g_2\in G.
\end{equation}
Then, $\cong \bpmod{H}$ is an equivalence relation iff $H$ is a subgroup of $G$.

Furthermore, in the case this is an equivalence relation,
\begin{equation}
[g]_{\cong \bpmod{H}}=gH.
\end{equation}
\begin{rmk}
To clarify, $[g]_{\cong \bpmod{H}}$ is the equivalence class of $g$ with respect to $\cong \bpmod{H}$ and $gH\ceqq \{ gh:h\in H\}$.
\end{rmk}
\begin{rmk}
The equivalence class of $g$ with respect $\cong \bpmod{H}$ is the \term{left $H$-coset}\index{Left coset}.  The set of all left $H$-cosets is denoted by $G/H\ceqq G/\sim _{\cong \bpmod{H}}=\left\{ gH:g\in G\right\}$\index[notation]{$G/H$}.
\end{rmk}
\begin{rmk}
By changing the definition of the equivalent relation to ``\textellipsis iff $g_1g_2^{-1}\in H$'', then we obtain the corresponding definition of \term{right $H$-cosets}\index{Right coset}, given explicitly by $Hg$.  The set of all right $H$-cosets is denoted by $H\backslash G$\index[notation]{$H\backslash G$}.\footnote{This notation is technically ambiguous with the notation used for relative set complementation, however, in practice there will never be any confusion.  Furthermore, if you pay extra special attention to the spacing, this uses the symbol \texttt{\textbackslash \ backslash} where set complementation uses the symbol \texttt{\textbackslash \setminus}.}  Of course, in general if the binary operation is not commutative, then $gH\neq Hg$.
\end{rmk}
\begin{proof}
$(\Rightarrow )$ Suppose that $\cong \bpmod{H}$ is an equivalence relation.  Let $g_1,g_2\in S$.  As $g_i\cong g_i\bpmod{H}$, we have that $g_i^{-1}g_i=1\in H$.  Then, $1^{-1}g_i=g_i\in H$, and so $g_i\cong 1\bpmod{H}$.  By symmetry, $1\cong g_i\bpmod{S}$, and so $g_i^{-1}1=g_i^{-1}\in H$.  We then have that $g_1\cong 1\bpmod{H}$ and $1\cong g_2\bpmod{H}$, and hence, $g_1\cong g_2\bpmod{H}$, and hence $g_2^{-1}g_1\in H$.  Thus, $H$ is indeed a subgroup of $G$.

\blankline
\noindent
$(\Leftarrow )$ Suppose that $H$ is a subgroup of $G$.  Then, $1\in H$, and so $g^{-1}g=1\in H$, and so $g\cong g\bpmod{H}$.  That is, $\cong \bpmod{H}$ is reflexive.  If $g_1\cong g_2\bpmod{S}$, then $g_2^{-1}g_1\in H$, then $g_1^{-1}g_2=(g_2^{-1}g_1)^{-1}\in H^{-1}=H$, and so $g_2\cong g_1\bpmod{H}$.  Thus, $\cong \bpmod{S}$ is symmetric.  If $g_1\cong g_2\bpmod{S}$ and $g_2\cong g_3\bpmod{H}$, then $g_2^{-1}g_1,g_3^{-1}g_2\in H$, and so $g_3^{-1}g_1=(g_3^{-1}g_2)(g_2^{-1}g_1)\in HH\subseteq H$, and so $g_1\cong g_3\bpmod{H}$.  Thus, $\cong \bpmod{H}$ is transitive, hence an equivalence relation.

\blankline
\noindent
We now prove the ``Furthermore\textellipsis '' part.  Certainly, as $(gh)^{-1}g=h^{-1}g^{-1}g=h^{-1}\in H$, $g\cong gh\bpmod{H}$ for all $h\in H$.  On the other hand, if $g_1\cong g_2\bpmod{H}$, then $g_2^{-1}g_1\in H$, and so $g_2^{-1}g_1=h$ for some $h\in H$, so that $g_1=g_2h$.  Thus, $[g]_{\cong \bpmod{H}}=gH$.
\end{proof}
\end{prp}
For a subgroup $H$ of $G$, $G/H$ will always be a set.  However, in good cases, it will be more than just a set---it will be a group in its own right.
\begin{dfn}{Ideals and quotient groups}{IdealsAndQuotientGroups}
Let $G$ be a group, let $H\subseteq G$ be a subgroup, and let $g_1,g_2\in G$.  Define
\begin{equation}
(g_1H)\cdot (g_2H)\coloneqq (g_1g_2)H.
\end{equation}
$H$ is an \term{ideal}\index{Ideal (in a group)} iff this is well-defined on the quotient set $G/H$.  In this case, $G/H$ is itself a group, the \term{quotient group}\index{Quotient group} of $G$ modulo $H$.
\begin{rmk}
Recall that (\cref{Cosets}) $gH$ is the equivalent class of $g$ modulo $H$, and so, in particular, these definitions involve picking representatives of equivalence classes.  Thus, in order for these operations to make sense, they must be well-defined.  In general, they will not be well-defined, and we call $H$ an \emph{ideal} precisely in the `good' case where these operations make sense.
\end{rmk}
\begin{rmk}
In the spirit of \cref{Cosets}, you should really be thinking of $H$ as a \emph{subset} that has the property that $\cong \bpmod{H}$ (defined by \eqref{eqnA.4.31}) is an equivalence relation.  Of course, this is perfectly equivalent to being a subgroup, but that's not the reason we care---we care because it gives us an equivalence relation.  This distinction will be more important for rings.
\end{rmk}
\begin{rmk}
In the context of groups, it is \emph{much} more common to refer to ideals as \term{normal subgroups}\index{Normal subgroup}.  As always, we choose the terminology we do because it is more universally consistent, even if less common.
\end{rmk}
\end{dfn}
There is an easy condition to check that in order to determine whether a given subgroup is in fact an ideal that does not require checking the well-definedness directly.
\begin{exr}{}{}
Let $G$ be a group and let $H\subseteq G$ be a subset.  Show that $H$ is an ideal iff (i)~it is a subgroup and (ii)~$gHg^{-1}\subseteq H$ for all $g\in G$.
\end{exr}

And now we turn to quotient rngs, whose development is completely analogous.
\begin{prp}{Cosets (in rngs)}{CosetsInRngs}
Let $R$ be a group, let $S\subseteq R$, and define
\begin{equation}
r_1\cong r_2\bpmod{S}\text{ iff }-r_2+r_1\in S\text{ for }r_1,r_2\in R.
\end{equation}
Then, $\cong \bpmod{S}$ is an equivalence relation iff $S$ is a subgroup of $\coord{R,+,0,-}$.

Furthermore, in the case this is an equivalence relation,
\begin{equation}
[r]_{\cong \bpmod{S}}=r+S.
\end{equation}
\begin{rmk}
To clarify, $[r]_{\cong \bpmod{S}}$ is the equivalence class of $r$ with respect to $\cong \bpmod{S}$ and $r+S\ceqq \{ r+s:s\in S\}$.
\end{rmk}
\begin{rmk}
The equivalence class of $r$ with respect $\cong \bpmod{S}$ is the \term{left $S$-coset}.  The set of all left $S$-cosets is denoted by $R/S\ceqq R/\sim _{\cong \bpmod{S}}=\left\{ r+S:r\in R\right\}$\index[notation]{$R/S$}.
\end{rmk}
\begin{rmk}
By changing the definition of the equivalent relation to ``\textellipsis iff $r_1-r_2\in S$'', then we obtain the corresponding definition of \term{right $S$-cosets}, given explicitly by $S+r$.  In this case, however, the binary operation in question ($+$) is commutative, and so $r+S=S+r$, that is, the left and right cosets coincide, and so we can simply say \term{coset}\index{Coset}.  In particular, there is no need to talk about the set of right $S$-cosets, which would have been denoted $S\backslash R$.
\end{rmk}
\begin{proof}
We leave this as an exercise.
\begin{exr}[breakable=false]{}{}
Prove this yourself.
\begin{rmk}
Hint:  Use the proof of \cref{Cosets} as a guide.
\end{rmk}
\end{exr}
\end{proof}
\end{prp}
You can check that $m\Z$ is indeed a subrng of $\Z$ and that $\Z /m\Z$ consists of just $m$ cosets:
\begin{equation}
0+m\Z ,1+m\Z ,\ldots ,(m-1)+m\Z,
\end{equation}
though you are probably more familiar just writing this as
\begin{equation}
0\bpmod{m},1\bpmod{m},\ldots ,m-1\bpmod{m}.
\end{equation}
Of course, however, $\Z /m\Z$ is more than just a set, it has a ring structure of its own, and in good cases, $R/S$ will obtain a canonical ring structure of its own as well.
\begin{dfn}{Ideals and quotient rngs}{IdealsAndQuotientRngs}
Let $R$ be a rng, let $S\subseteq \coord{R,+,0,-}$ be a subgroup, and let $r_1,r_2\in R$.  Define
\begin{equation}
(r_1+S)+(r_2+S)\coloneqq (r_1+r_2)+S
\end{equation}
and
\begin{equation}
(r_1+S)\cdot (r_2+S)\coloneqq (r_1\cdot r_2)+S.
\end{equation}
$S$ is an \term{ideal}\index{Ideal (in a ring)} iff both of these operations are well-defined.  In this case, $R/S$ is the \term{quotient rng}\index{Quotient rng} of $R$ modulo $S$.
\begin{rmk}
I mentioned in a remark of the definition of quotient groups \cref{IdealsAndQuotientGroups} that you should really be thinking of the condition there that ``$H\subseteq G$ a subgroup'' as the condition that $\cong \bpmod{H}$ be well-defined.  This shows its relevance here as, in the spirit of \cref{CosetsInRngs}, the appropriate condition is \emph{not} ``$S\subseteq R$ a subrng'' but instead that ``$S\subseteq \coord{R,0,+,-}$ be a subgroup''.  This is particularly important if you're working with rings, as in this case, the `correct' definition of subring requires that subrings include $1$, however, if an ideal $I$ contains $1$, then $I=R$.\footnote{Because, by the absorption property, $r=r\cdot 1\in I$ for all $r\in R$.}  Thus, if you write ``$S\subseteq R$ a subring'' instead of ``$S\subseteq \coord{R,+,0,-}$ a subgroup'', your definition would imply that the only ideal in $R$ is $R$ itself!\footnote{In case it's not obvious, that would constitute a particularly shitty definition.}
\end{rmk}
\end{dfn}
Just as before, we have an easy way of checking whether a given subring is in fact an ideal.
\begin{exr}{}{exrA.4.45}
Let $R$ be a rng and let $S\subseteq R$ be a subset.  Show that $S$ is an ideal iff (i)~it is a subrng and (ii)~$r\in R$ and $s\in S$ implies that $r\cdot s,s\cdot r\in S$.
\begin{rmk}
The second property is sometimes called ``absorbing'', because elements in the ideal `absorb' things into the ideal when you multiply them.
\end{rmk}
\end{exr}
\begin{exm}{Integers modulo $m$}{exmA.1.117}
Let $m\in \Z ^+$.
\begin{exr}[breakable=false]{}{}
Show that $m\Z$ is an ideal in $\Z$.
\end{exr}
Then, the \term{integers modulo $m$}\index{Integers modulo $m$} are defined to be the quotient cring $\Z /m\Z$.
\end{exm}

\section[Cardinality, countability, and the naturals]{Cardinality, countability, and the natural numbers}

The goal of this section is to define what is called the \emph{cardinality} of a set.  Intuitively, the cardinality of a set is the number of elements that set contains.  The concept of cardinality will then allow us to define the \emph{natural numbers} (at least as a well-ordered set---we won't worry about the arithmetic of the natural numbers here), which in turn will allow us to define the important concept of \emph{countability}.

\subsection{Cardinality}\label{sbs1.1.1}

The first step in defining the cardinality of sets is being able to decide when two sets have the same number of elements.  So, suppose we are given two sets $X$ and $Y$ and that we would like to determine whether $X$ and $Y$ have the same number of elements.  How would you do this?

Intuitively, you could start by trying to label all the elements in $Y$ by elements of $X$, without repeating labels.  If either (i)~you ran out of labels before you finished labeling all elements in $Y$ or (ii)~you were forced to assign more than one label to an element of $Y$, then you could deduce that $X$ and $Y$ did \emph{not} have the same number of elements.  To make this precise, we think of this labeling as a function from $X$ to $Y$.  Then, the first case corresponds to this labeling function not being surjective and the second case corresponds to this labeling function not being injective.

The more precise intuition is then that the two sets $X$ and $Y$ have the same number of elements, that is, the same cardinality, iff there is a bijection $f\colon X\rightarrow Y$ between them:  that $f$ is an injection says that we don't use a label more than once (or equivalently that $Y$ has at least as many elements as $X$) and that $f$ is a surjection says that we label everything at least once (or equivalently that $X$ has at least as many elements as $Y$).  This yields the following definition.
\begin{dfn}{Equinumerous}{}
	Let $X$ and $Y$ be sets.  Then, $X$ and $Y$ are \term{equinumerous}\index{Equinumerous} iff there is a bijection from $X$ to $Y$.
\end{dfn}
\begin{exr}{}{}
	Show that the relation of equinumerosity is an equivalence relation on the collection of sets.
\end{exr}

So we've determined what it means for two sets to have the same cardinality, but what actually \emph{is} a cardinality?  The trick is to identify a cardinal with the collection of all sets which have that cardinality.
\begin{dfn}{Cardinal number}{dfn1.1.2}
	A \term{cardinal number}\index{Cardinal number} is an element of
	\begin{equation}
	\aleph \coloneqq \Obj (\Set )/\cong \coloneqq \left\{ [X]_{\cong}:X\in \Obj (\Set )\right\} ,
	\end{equation}
	where $\cong$ is the equivalence relation of equinumerosity.
	\begin{rmk}
		In other words, a cardinal is an equivalence class of sets, the equivalence relation being equinumerosity.  Furthermore, for $X$ a set, we write $\abs{X}\coloneqq [X]_{\cong _{\Set}}$\index[notation]{$\abs{X}$}.
	\end{rmk}
\end{dfn}

The next objective we would like to achieve is to be able to compare cardinals.  As cardinal numbers are supposed to be a `size' of some sort, we should have a notion of what it means for one cardinal to be larger than another.  Of course, there is such a notion, and the relation so defined turns out to be a \emph{well-order}.  Once we define the natural numbers, it will then follow automatically that it restricts to a well-order on $\N$---see \cref{crlA.5.27}.

As for what the definition of that well-order should be, recall our explanation of thinking of a function $f\colon X\rightarrow Y$ as `labeling elements of $Y$ with elements of $X$'---see the beginning of \crefnameref{sbs1.1.1}.  We argued that our definition of `same number of elements' should have the properties that (i)~every element of $Y$ is labeled and (ii)~no element of $Y$ is labeled more than once.  Similarly, our definition of ``$Y$ has at least as many element of $X$'' should have the property that are not forced to label an element of $Y$ more than once (i.e.~that $f$ is injective), but not necessarily that every element of $Y$ is labeled.
\begin{dfn}{}{dfn1.1.23}
	Let $m,n\in \aleph$ and let $M$ and $N$ be sets such that $m=\abs{M}$ and $n=\abs{N}$.  Then, we define $m\leq n$\index[notation]{$m\leq n$} iff there is an injective map from $M$ to $N$.
	\begin{exr}[breakable=false]{}{}
		Check that $\leq$ is well-defined.
	\end{exr}
\end{dfn}
You might be thinking ``Ah, that makes sense.  But why use injective?  Couldn't we also say that $\abs{X}\geq \abs{Y}$ iff there is a \emph{surjective} function $X\rightarrow Y$?''.  Unfortunately, this is only \emph{almost} correct.
\begin{exr}{}{}
	\begin{enumerate}
		\item Show that if $X$ and $Y$ are nonempty sets, then $\abs{X}\leq \abs{Y}$ iff there is a surjective function from $Y$ to $X$.
		\item On the other hand, show how this might fail without the assumption of nonemptiness.
	\end{enumerate}
\end{exr}
\begin{prp}{}{}
	$\coord{\aleph ,\leq}$ is a preordered set.
	\begin{proof}
		Recall that being a preorder just means that $\leq$ is reflexive and transitive (see \cref{dfnA.1.19}).
		
		Let $m,n,o\in \N$ and let $M,N,O$ be sets such that $m=\abs{M}$, $n=\abs{N}$, $o=\abs{O}$.  The identity map from $M$ to $M$ is an injection (and, in fact, a bijection), which shows that $m=\abs{M}\leq \abs{M}=m$, so that $\leq$ is reflexive.
		
		To show transitivity, suppose that $m\leq n$ and $n\leq o$.  Then, there is an injection $f\colon M\rightarrow N$ and an injection from $g\colon N\rightarrow O$.  Then, $g\circ f\colon M\rightarrow O$ is an injection (this is part of \cref{exrA.1.10}), and so we have $m=\abs{M}\leq \abs{O}=o$, so that $\leq$ is transitive, and hence a preorder.
	\end{proof}
\end{prp}
That $\leq$ is a preorder is relatively easy.  The next step, showing that it is a partial-order, is considerably more difficult, and even has a name.
\begin{thm}{Bernstein-Cantor-Schröder Theo-\\rem}{thm1.1.26}\index{Bernstein-Cantor-Schröder Theorem}
	$\coord{\aleph ,\leq}$ is a partially-ordered set.
	\begin{rmk}
		This theorem is usually stated as ``If there is an injection from $X$ to $Y$ and there is an injection from $Y$ to $X$, then there is a bijection from $X$ to $Y$.''.
	\end{rmk}
	\begin{rmk}
		This theorem is \emph{incredibly} useful for showing that two sets have the same cardinality---it's often much easier to construct an injection in each direction than it is to construct a single bijection---and it would do you well to not forget it.
	\end{rmk}
	\begin{proof}\footnote{Proof adapted from \cite[pg.~29]{Abbott}.}
		\Step{Recall what it means to be a partial-order}
		Recall that being a partial-order just means that $\leq$ is an antisymmetric preorder.  We have just shown that $\leq$ is a preorder (see \cref{dfnA.1.24}), so all that remains to be seen is that $\leq$ is antisymmetric.
		
		\Step{Determine what explicitly we need to show}
		Let $m,n\in \aleph$ and let $M,N$ be sets such that $m=\abs{M}$ and $n=\abs{N}$.  Suppose that $m\leq n$ and $n\leq m$.  By definition, this means that there is an injection $f\colon M\rightarrow N$ and an injection $g\colon N\rightarrow M$.  We would like to show that $m=n$.  By definition, this means we must show that there is a bijection from $M$ to $N$.
		
		\Step{Note the existence of left-inverse to both $f$ and $g$}
		If $M$ is empty, then as $N$ injects into $M$, $N$ must also be empty, and we are done.  Likewise, if $N$ is empty, we are also done.  Thus, we may as well assume that $M$ and $N$ are both nonempty.  We can now use the result of \cref{exrA.1.9} which says that both $f$ and $g$ have left inverses.\footnote{To use this, we first needed to have that $M$ and $N$ are nonempty.}  Denote these inverses by $f^{-1}:N\rightarrow M$ and $g^{-1}:M\rightarrow N$ respectively, so that
		\begin{equation}
		f^{-1}\circ f=\id _M\text{ and }g^{-1}\circ g=\id _N.\footnote{Note that it is \emph{not} necessarily the case that $f\circ f^{-1}=\id _N$ (and similarly for $g$).  This certainly constitutes an abuse of notation, as we should really be reserving the notation $f^{-1}$ for a \emph{two}-sided inverse, but as this makes the proof quite a bit more readable, we ignore such pedantry for the time being.}
		\end{equation}
		
		\Step{Define $C_x$}
		Fix an element $x\in M$ and define
		\begin{equation*}
		\begin{multlined}
		C_x\coloneqq \left\{ \ldots ,g^{-1}\left( f^{-1}\left( g^{-1}(x)\right) \right) ,f^{-1}\left( g^{-1}(x)\right) ,g^{-1}(x),x, \right. \\ \left. f(x),g\left( f(x)\right) ,f\left( g\left( f(x)\right) \right) ,\ldots \right\} \subseteq M\sqcup N.\footnote{The ``$C$'' is for ``chain''.}
		\end{multlined}
		\end{equation*}
		Note that $C_x$ is `closed' under application of $f$, $g$, $f^{-1}$, and $g^{-1}$, in the sense that, if $x'\in C_x$ and $f(x')$ makes sense (i.e.~if $x'\in M$), then $f(x')\in C_x$, and similarly for $g$, $f^{-1}$, and $g^{-1}$.
		
		\Step{Show that $\{ C_x:x\in M\}$ forms a partition of $M\sqcup N$}
		We now claim that the collection $\left\{ C_x:x\in M\right\}$ forms a partition of $M\sqcup N$ (recall that this means that any two given $C_x$s are either identical or disjoint---see \cref{dfnA.1.11}).  If $C_{x_1}$ is disjoint from $C_{x_2}$ we are done, so instead suppose that there is some element $x_0$ that is in both $C_{x_1}$ and $C_{x_2}$.  First, let us do the case in which $x_0\in M$.  From the definition of $C_x$, we then must have that
		\begin{equation}
		[g\circ f]^k(x_1)=x_0=[g\circ f]^l(x_2)
		\end{equation}
		for some $k,l\in \Z$.  Without loss of generality, suppose that $k\leq l$.  Then, applying $f^{-1}\circ g^{-1}$ to both sides of this equation $k$ times,\footnote{If $k$ happens to be negative, it is understood that we instead apply $g\circ f$ $-k$ times.} we find that
		\begin{equation}
		x_1=[g\circ f]^{l-k}(x_2).
		\end{equation}
		In other words, $x_1\in C_{x_2}$.  Not only this, but $f(x_1)\in C_{x_2}$ as well because $f(x_1)=f\left( [g\circ f]^{l-k}(x_2)\right)$.  Similarly, $g^{-1}(x_1)\in C_{x_2}$, and so on.  It follows that $C_{x_1}\subseteq C_{x_2}$.  Switching $1\leftrightarrow 2$ and applying the same arguments gives us $C_{x_2}\subseteq C_{x_1}$, and hence $C_{x_1}=C_{x_2}$.  Thus, indeed, $\left\{ C_x:x\in M\right\}$ forms a partition of $M\sqcup N$.  In particular, it follows that
		\begin{equation}\label{1.1.29}
		C_x=C_{x'}\text{ for all }x'\in C_x.
		\end{equation}
		
		\Step{Define $X_1,X_2,Y_1,Y_2$}
		Now define
		\begin{equation}\label{1.1.30}
		A\coloneqq \bigcup _{\substack{x\in M\st \\ C_x\cap N\subseteq f(M)}}C_x
		\end{equation}
		as well as
		\begin{subequations}
			\begin{align}
				X_1\coloneqq&M\cap A, & Y_1\coloneqq&N\cap A, \\
				X_2\coloneqq&M\cap A^{\comp}, & Y_2\coloneqq&N\cap A^{\comp}.
			\end{align}
		\end{subequations}
		Note that, as $\{ C_x:x\in M\}$ is a partition of $M\sqcup N$, we have that
		\begin{equation}\label{1.1.34}
		A^{\comp}=\bigcup _{\substack{x\in M\st \\ C_x\cap N\not \subseteq f(M)}}C_x.
		\end{equation}
		
		\Step{Show that $\restr{f}{X_1}:X_1\rightarrow Y_1$ is a bijection}
		We claim that $\restr{f}{X_1}:X_1\rightarrow Y_1$ is a bijection.  First of all, note the it $x\in X_1$, then in fact $f(x)\in Y_1$, so that this statement indeed does make sense.  Of course, it is injective because $f$ is.  To show surjectivity, let $y\in Y_1\coloneqq N\cap A$.  From the definition of $A$ \eqref{1.1.30}, we see that $y\in C_x\cap N$ for some $C_x$ with $C_x\cap N\subseteq f(M)$, so that $y=f(x')$ for some $x'\in M$.  We still need to show that $x'\in X_1$.  However, we have that $x'=f^{-1}(y)$, and so as $y\in C_x$, we have that $x'=f^{-1}(y)\in C_x$ as well.  We already had that $C_x\cap N\subseteq f(M)$, so that indeed $x'\in A$, and hence $x'\in X_1$.  Thus, $\restr{f}{X_1}:X_1\rightarrow Y_1$ is a bijection.
		
		\Step{Show that $\restr{g}{Y_2}:Y_2\rightarrow X_2$ is a bijection}
		We now show that $\restr{g}{Y_2}:Y_2\rightarrow X_2$ is a bijection.  Once again, all we must show is surjectivity, so let $x\in X_2=M\cap A^{\comp}$. From the definition of $A$ \eqref{1.1.30}, it thus cannot be the case that $C_x\cap N$ is contained in $f(M)$, so that there is some $y\in C_x\cap N$ such that $y\notin f(M)$.  By virtue of \eqref{1.1.29}, we have that $C_x=C_y$, and in particular $x\in C_y$.  From the definition of $C_y$, it follows that either (i)~$x=y$, (ii)~$x$ is in the image of $f^{-1}$, or (iii)~$x$ is in the image of $g$ (the other possibilities are excluded because $x\in M$).  Of course it cannot be the case that $x=y$ because $x\in M$ and $y\in N$.  Likewise, it cannot be the case that $x$ is in the image of $f^{-1}$ because $x\in A^{\comp}$.  Thus, we must have that $x=g(y')$ for some $y'\in N$.  Once again, we still must show that $y'\in Y_2$.  However, we have that $y'=g^{-1}(x)$, so that $y'\in C_x$.  Furthermore, as $C_x\cap N$ is not contained in $f(M)$, from \eqref{1.1.34} it follows that $C_x\subseteq A^{\comp}$.  Thus, $y'\in C_x\subseteq A^{\comp}$, and so $y'\in Y_2$.  Thus, $\restr{g}{Y_2}:Y_2\rightarrow X_2$ is a bijection.
		
		\Step{Construct the bijection from $M$ to $N$}
		Finally, we can define the bijection from $M$ to $N$.  We define $h \colon M\rightarrow N$ by
		\begin{equation}
		h(x)\coloneqq \begin{cases}f(x) & \text{if }x\in X_1 \\ g^{-1}(x) & \text{if }x\in X_2.\end{cases}
		\end{equation}
		Note that $\{ X_1,X_2\}$ is a partition of $M$ and $\{ Y_1,Y_2\}$ is a partition of $N$.  To show injectivity, suppose that $h(x_1)=h(x_2)$.  If this element is in $Y_1$, then because $\restr{f}{X_1}:X_1\rightarrow Y_1$ is a bijection, it follows that both $x_1,x_2\in X_1$, so that $f(x_1)=h(x_1)=h(x_2)=f(x_2)$, and hence that $x_1=x_2$.  Similarly if this element is contained in $Y_2$.  To show surjectivity, let $y\in N$.  First assume that $y\in Y_1$.  Then, $f^{-1}(y)\in X_1$, so that $h\left( f^{-1}(y)\right) =y$.  Similarly, if $y\in Y_2$, then $h\left( g(y)\right) =y$.  Thus, $h$ is surjective, and hence bijective.
	\end{proof}
	\begin{rmk}
		I think perhaps the mathematical precision here has obfuscated the core idea of the proof.  Briefly, the basic idea is as follows.  Once we have defined the chains $C_x$s, they `break-up' $M$ and $N$ into `chunks' in such a way that it suffices to construct a bijection separately on each chunk (that is, they form a \emph{partition}).  If the elements of $C_x$ in the codomain are actually contained in the image of $f$, then $f$ itself can serve as the bijection on that ``chunk''---otherwise, we can use $g$.
	\end{rmk}
\end{thm}
Finally, we check that $\leq$ is a well-order on $\aleph$.
\begin{thm}{}{thm1.1.34}
	$\coord{\aleph ,\leq}$ is well-ordered.
	\begin{proof}\footnote{Proof adapted from \cite{Honig}.}
		\Step{Conclude that it suffices to show that every nonempty subset has a smallest element}
		By \cref{prpA.1.51}, we do not need to check totality explicitly, and so it suffices to show that every nonempty subset of $\aleph$ has a smallest element.
		
		\Step{Define $\mcal{T}$ as a preordered set}
		So, let $S\subseteq \aleph$ be a nonempty collection of cardinals and for each $m\in S$ write $m=\abs{M_m}$ for some set $M_m$.  Define
		\begin{equation}
		M\coloneqq \prod _{m\in S}M_m
		\end{equation}
		and
		\begin{equation*}
		\begin{split}
		\mcal{T} & \coloneqq \left\{ T\subseteq M:T\in \Obj (\Set );\text{ for all }x,y\in T,\right. \\ & \qquad \left. \text{if }x\neq y\text{ it follows that }x_m\neq y_m\text{ for all }m\in S\text{.}\right\} .
		\end{split}
		\end{equation*}
		Order $\mcal{T}$ by inclusion.
		
		\Step{Verify that $\mcal{T}$ satisfies  the hypotheses of Zorn's Lemma}
		We wish to apply Zorn's Lemma (\cref{ZornsLemma}) to $\mcal{T}$.  To do that of course, we must first verify the hypotheses of Zorn's Lemma.  $\mcal{T}$ is a partially-ordered set by \cref{exrA.1.26}.  Let $\mcal{W}\subseteq \mcal{T}$ be a well-ordered subset and define
		\begin{equation}
		W\coloneqq \bigcup _{T\in \mcal{W}}T.
		\end{equation}
		It is certainly the case that $T\subseteq W$ for all $T\in \mcal{W}$.  In order to verify that $W$ is indeed an upper-bound of $\mcal{W}$ in $\mcal{T}$, however, we need to check that $W$ is actually an element of $\mcal{T}$.  So, let $x,y\in W$ be distinct.  Then, there are $T_1,T_2\in \mcal{W}$ such that $x\in T_1$ and $y\in T_2$.  Because $\mcal{W}$ is in particular totally-ordered, we may without loss of generality assume that $T_1\subseteq T_2$.  In this case, both $x,y\in T_2$.  As $T_2\in \mcal{T}$, it then follows that $x_m\neq x_m$ for all $m\in S$.  It then follows in turn that $W\in \mcal{T}$.
		
		\Step{Conclude the existence of a maximal element}
		The hypotheses of Zorn's Lemma being verified, we deduce that there is a maximal element $T_0\in \mcal{T}$.
		
		\Step{Show that there is some projection whose restriction to the maximal element is surjective}
		Let $\pi _m:M\rightarrow M_m$ be the canonical projection.  We claim that there is some $m_0\in S$ such that $\pi _{m_0}(T_0)=M_{m_0}$.  To show this, we proceed by contradiction:  suppose that for all $m\in M$ there is some element $x_m\in M_m\setminus \pi _m(T_0)$.  Then, $T_0\cup \{ x\}\in \mcal{T}$ is strictly larger than $T_0$:  a contradiction of maximality.  Therefore, there is some $m_0\in S$ such that $\pi _{m_0}(T_0)=M_{m_0}$.
		
		\Step{Construct an injection from $M_{m_0}$ to $M_m$ for all $m\in S$}
		The defining condition of $\mcal{T}$ is simply the statement that $\restr{\pi _m}{T}:T\rightarrow M_m$ is injective for all $T\in \mcal{T}$.  In particular, by the previous step, $\restr{\pi _{m_0}}{T_0}:T_0\rightarrow M_{m_0}$ is a bijection.  And therefore, the composition $\pi _m\circ \restr{\pi _{m_0}}{T_0}^{-1}:M_{m_0}\rightarrow M_m$ is an injection from $M_{m_0}$ to $M$.  Therefore,
		\begin{equation}
		m_0=\abs{M_{m_0}}\leq \abs{M_m}=m
		\end{equation}
		for all $m\in S$.  That is, $m_0$ is the smallest element of $S$, and so $\aleph$ is well-ordered.
	\end{proof}
\end{thm}

\subsection{The natural numbers}

The key idea used to define the natural numbers is that the natural numbers should be precisely those cardinals which are finite.  We thus must now answer the question ``What does it mean to be `finite'?''.  This is actually a tad bit tricky.

Of course, we don't have a precise definition yet, but everyone has an intuitive idea of what it means to be infinite.  So, consider an `infinite set' $X$.  Now remove one element $x_0\in X$ to form the set $U\coloneqq X\setminus \{ x_0\}$.  For any reasonable definition of ``infinite'', removing a single element from an infinite set should not change the fact that it is infinite, and so $U$ should still be infinite.  In fact, more should be true.  Not only should $U$ still be infinite, but it should still have the same cardinality as $X$.\footnote{We will see in the next chapter that there are infinite sets which are not of the same cardinality.  That is, in this sense, there is more than one type of infinity.}  It is this idea that we take as the defining property of being infinite.
\begin{dfn}{Finite and infinite}{}
	Let $X$ be a set.  Then, $X$ is \term{infinite}\index{Infinite} iff there is a bijection from $X$ to a proper subset of $X$.  $X$ is \term{finite} iff it is not infinite.
	\begin{rmk}
		The keyword here is \emph{proper}---there is a bijection from every set $X$ to some subset of $X$, namely $X\subseteq X$ itself.
	\end{rmk}
\end{dfn}
Before getting to the natural numbers themselves, let's discuss a couple of interesting properties about infinite sets.
\begin{prp}{}{prpA.5.25}
	Let $X$ be a set and define
	\begin{equation}
		\collection{F}_X\ceqq \left\{ S\subseteq X:S\text{ is finite.}\right\} .
	\end{equation}
	Then, if $X$ is infinite, then $\abs{X}=\abs{\collection{F}_X}$.
	\begin{rmk}
		In words, for infinite sets, the cardinality of the set itself is the same as the cardinality of its collection of finite subsets.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove this yourself.
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{}{prpA.5.27}
	Let $\collection{F}$ be an infinite collection of finite sets.  Then,
	\begin{equation}
		\abs*{\bigcup _{F\in \collection{F}}F}=\abs{\collection{F}}.
	\end{equation}
	\begin{rmk}
		In words, if $\kappa$ is an infinite cardinal, the union of $\kappa$ many finite sets still has cardinality $\kappa$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove this yourself.
		\end{exr}
	\end{proof}
\end{prp}
And now finally:
\begin{dfn}{Natural numbers}{}
	The \term{natural numbers}\index{Natural numbers}, $\N$, are defined as
	\begin{equation}
	\N \index[notation]{$\N$}\coloneqq \left\{ \abs{X}:X\in \Obj (\Set )\text{ is finite.}\right\} .
	\end{equation}
	In words, the natural numbers are precisely the cardinals of finite sets.
	\begin{rmk}
		Some people take the natural numbers to not include $0$.  This is a bit silly for a couple of reasons.  First of all, if you think of the natural numbers as cardinals, as we are doing here, then $0$ has to be a natural number as it is the cardinality of the empty-set.  Furthermore, as we shall see in the next subsection, it makes the algebraic structure of $\N$ slightly nicer because $0$ acts as an additive identity.  Indeed, I am not even aware of a term to describe the sort of algebraic object $\N$ would be if it did not contain $0$.  Finally, regardless of your convention, you already have a symbol to denote $\{ 1,2,3,\ldots \}$, namely $\Z ^+$:\footnote{Of course, at this point in the next, we technically don't know what any of these symbols mean.  For the purposes of motivating a convention, however, I have no qualms about pretending you are not completely ignorant.}  having the symbol $\N$ denote the same is an inefficient use of notation.
	\end{rmk}
\end{dfn}

As a corollary of \cref{thm1.1.34}, we immediately have the following.
\begin{crl}{}{crlA.5.27}
	$\coord{\N ,\leq}$ is a well-ordered set.
\end{crl}

\subsection{Countability}

The cardinality of the natural numbers is special:  it turns out that the cardinality of the natural numbers is the smallest infinite cardinal.
\begin{prp}{}{exr2.1.1}
	Let $\kappa$ be an infinite cardinal.  Then, $\abs{\N}\leq \kappa$.
	\begin{rmk}
		Phrased differently ,note that the contrapositive easily implies the following.
		\begin{important}
			If $\kappa$ is a cardinal with $\kappa \leq \abs{\N}$, then either $\kappa =\abs{\N}$ or $\kappa$ is finite.
		\end{important}
	\end{rmk}
	\begin{proof}
		Let $K$ be any set such that $|K|=\kappa$.  Recall that (\cref{dfn1.1.23}) to show that $|\N |\leq \kappa$ requires that we produce an injection from $\N$ into $K$.  We construct an injection $f\colon \N \rightarrow K$ inductively.  Let $k_0\in K$ be arbitrary and let us define $f(0)\coloneqq k_0$.  If $K\setminus \{ k_0\}$ were empty, then $K$ would not be infinite, therefore there must be some $k_1\in K$ distinct from $k_0$, so that we may define $f(1)\coloneqq k_1$.  Continuing this process, suppose we have defined $f$ on $0,\ldots ,n\in \N$, and wish to define $f(n+1)$.  If $K\setminus \left\{ f(0),\ldots ,f(n)\right\}$ were empty, then $K$ would be finite.  Thus, there must be some $k_{n+1}\in K$ distinct from $f(0),\ldots ,f(n)$.  We may then define $f(n+1)\coloneqq k_{n+1}$.  The function produced must be injective because, by construction, $f(m)$ is distinct from $f(n)$ for all $n<m$.  Hence, $|\N |\leq \kappa$.
	\end{proof}
\end{prp}
Thus, the cardinality of the natural numbers is the smallest infinite cardinality.  We give a name to this cardinality.
\begin{dfn}{Countability}{dfn2.2}
	Let $X$ be a set.  Then, $X$ is \term{countably-infinite}\index{Countably-infinite} iff $|X|=|\N |$.  $X$ is \term{countable} iff either $X$ is countably-infinite or $X$ is finite.  We write $\aleph _0\coloneqq |\N |$\index[notation]{$\aleph _0$}.
	\begin{rmk}
		It is not uncommon for people to use the term ``countable'' to mean what we call ``countably-infinite''.  They would would then just say ``countable or finite'' in cases that we would say ``countable''.
	\end{rmk}
\end{dfn}

Our first order of business is to decide what other sets besides the natural numbers are countably-infinite.
\begin{prp}{}{}
	The even natural numbers, $2\N$, are countably-infinite.
	\begin{proof}
		On one hand, $2\N \subseteq \N$, so that $|2\N |\leq \aleph _0$.  On the other hand, $2\N$ is infinite, and as we just showed that $\aleph _0$ is the smallest infinite cardinal, we must have that $\aleph _0\leq |2\N |$.  Therefore, by antisymmetry (Bernstein-Cantor-Schr\"{o}der Theorem, \cref{thm1.1.26}) of $\leq$, we have that $|2\N |=\aleph _0$.
	\end{proof}
\end{prp}
\begin{exr}{}{}
	Construct an explicit bijection from $\N$ to $2\N$.
\end{exr}
This is the first explicit example we have seen of some perhaps not-so-intuitive behavior of cardinality.  On one hand, our intuition might tell us that there are half as many even natural numbers as there are natural numbers, yet, on the other hand, we have just proven (in two different ways, if you did the exercise) that $2\N$ and $\N$ have the same number of elements!  This of course is not the only example of this sort of weird behavior.  The next exercise shows that this is actually quite general.
\begin{exr}{}{}
	Let $X$ and $Y$ be countably-infinite sets.  Show that $X\sqcup Y$ is countably-infinite.
	\begin{rmk}
		Note that this generalizes---see \cref{exr2.1.7}.
	\end{rmk}
\end{exr}
Thus, it is literally the case that $2\aleph _0=\aleph _0$.  A simple corollary of this is that $\Z$ is countably-infinite.
\begin{exr}{}{}
	Show that $|\Z |=\aleph _0$.
\end{exr}

You (hopefully) just showed that $2\aleph _0=\aleph _0$, but what about $\aleph _0^2$?
\begin{exr}{}{exr2.1.7}
	Let $\collection{X}$ be a countable indexed collection of countable sets.  Show that
	\begin{equation}
	\bigsqcup _{X\in \collection{X}}X
	\end{equation}
	is countable.
\end{exr}

\begin{prp}{}{}
	$\aleph _0^2=\aleph _0$.
	\begin{proof}
		For $m\in \N$, define
		\begin{equation}
		X_m\coloneqq \left\{ \coord{i,j}\in \N \times \N:i+j=m\right\} 
		\end{equation}
		Note that each $X_m$ is finite and also that
		\begin{equation}
		\N \times \N =\bigsqcup _{m\in \N}X_m.
		\end{equation}
		Therefore, by the previous exercise, $\abs{\N \times \N }\eqqcolon \aleph _0^2$ is countable, i.e., $\aleph _0^2\leq \aleph _0$.  As $\aleph _0$ is not finite, we must thus have that $\aleph _0^2=\aleph _0$ (\cref{exr2.1.1}).
	\end{proof}
\end{prp}
\begin{exr}{}{}
	Use Bernstein-Cantor-Schröder and the previous proposition to show that $|\Q |=\aleph _0$.
\end{exr}
This result might seem a bit crazy at first.  I mean, just `look' at the number line, right?  There's like bajillions more rationals than naturals.  Surely it can't be the case there there are no more rationals than natural numbers, can it?  Well, yes, in fact that can be, and in fact is, precisely the case---despite what your silly intuition might be telling you, there are no more rational numbers than there are natural numbers.

So, we've now done both $\Z$ and $\Q$, but what about $\R$?  At first, you might have declared it obvious that there are more real numbers than natural numbers, but perhaps the result about $\Q$ has now given you some doubt.  In fact, it \emph{does} turn out that there are more real numbers than there are natural numbers.  They key idea used to prove this is the following important famous result.
\begin{thm}{Cantor's Cardinality Theorem}{CantorsCardinalityTheorem}\index{Cantor's Cardinality Theorem}
	Let $X$ be a set.  Then, $|X|<|2^X|$.
	\begin{rmk}
		There is a good chance you may have heard of the term \term{Cantor's Diagonal Argument}\index{Cantor's Diagonal Argument}.  The argument here is a generalization of that (it's also `cleaner'), and so we don't present the Diagonal Argument itself.
	\end{rmk}
	\begin{rmk}
		We don't have need to give the details of how to show that $\abs{\R}>\abs{\N}$---these are better saved for an analysis course---but we can at least explain the vague idea.
		Of course, it suffices to show that $\abs{[0,1)}>\abs{\N}$.  To do that, you think of elements in $[0,1)$ as being defined in terms of their binary expansions, which allows you to associate to every element of $[0,1)$ a sequence of $0$s and $1$s.  Such a sequence in turn corresponds to a subset of $\N$---see \cref{exrA.1.26x}---and this then reduces the problem to show that $\abs{2^{\N}}>\abs{\N}$, but this is precisely the conclusion of Cantor's Cardinality Theorem!
	\end{rmk}
	\begin{proof}
		We must show two things:  (i)~$|X|\leq |2^X|$ and (ii)~$|X|\neq |2^X|$.
		
		The first, by definition, requires that we construct an injection from $X$ to $2^X$.  This, however, is quite easy.  We may define a function $X\rightarrow 2^X$ by $x\mapsto \{ x\}$.  This is of course an injection.
		
		The harder part is showing that $|X|\neq |2^X|$.  To show this, we must show that there is \emph{no} surjection from $X$ to $2^X$.  So, let $f\colon X\rightarrow 2^X$ be a function.  We show that $f$ cannot be surjective.  To do this, we construct a subset of $X$ that cannot be in the image of $f$.
		
		We define
		\begin{equation}
		S\coloneqq \left\{ x\in X:x\notin f(x)\right\} .
		\end{equation}
		We would like to show that $S$ is not in the image of $f$.  We proceed by contradiction:  suppose that $S=f(x_0)$ for some $x_0\in X$.  Now, we must have that either $x_0\in S$ or $x_0\notin S$.  If the former were true, then we would have that $x_0\notin f(x_0)=S$:  a contradiction.  On the other hand, in the latter case, we would have $x\in f(x_0)=S$:  a contradiction.  Thus, as neither of these possibilities can be true, there cannot be any $x_0\in X$ such that $f(x_0)=S$.  Thus, $S$ is not in the image of $f$, and so $f$ is not surjective.
	\end{proof}
\end{thm}

\cleardoublepage
\chapter{Basic category theory}\label{BasicCategoryTheory}

First of all, a disclaimer:  it is probably not best pedagogically speaking to start with even the very basics of category theory.  While in principle anyone who has the prerequisites for these notes knows everything they need to know to understand category theory, it may be difficult to understand the motivation for things without a collection of examples to work with in the back of your mind.  Thus, if anything in this section does not make sense the first time you read through it, you should not worry---it will only be a problem if you do not understand ideas here as they occur in the text.  In fact, it is probably perfectly okay to completely skip this section and reference back to it as needed.  In any case, our main motivation for introducing category theory in a subject like this is simply that we would like to have more systematic language and notation.

\section{What is a category?}

In mathematics, we study many different types of objects:  sets, preordered sets, monoids, rngs, vector spaces, topological spaces, schemes, etc..\footnote{No, you are not expected to know what all of these are.}  In all of these cases, however, we are not only concerned with the objects themselves, but also with maps between them that `preserve' the relevant structure.  In the case of a set, there is no extra structure to preserve, and so the relevant maps are \emph{all} the functions.  In contrast, however, for vector spaces, we will see that the relevant maps are not all the functions, but instead all \emph{linear} functions.  The idea then is to come up with a definition that deals with both the objects and the relevant maps, or morphisms, simultaneously.  This is the motivating idea of the definition of a category.
\begin{dfn}{Category}{Category}
A \term{category}\index{Category} $\cat{C}$ is
\begin{data}
\item a collection $\Obj (\cat{C})$\index[notation]{$\Obj (\cat{C})$}, the \term{objects}\index{Objects} of $\cat{C}$; together with
\item for each $A,B\in \Obj (\cat{C})$, a collection $\Mor _{\cat{C}}(A,B)$\index[notation]{$\Mor _{\cat{C}}(A,B)$}, the \term{morphisms}\index{Morphisms} from $A$ to $B$ in $\cat{C}$;\footnote{No, we do not require that $\Mor _{\cat{C}}(A,B)$ be a (small) set.  (This comment is really intended for those who have seen this definition elsewhere---often times authors fix a universe $U$, whose elements are referred to as the \emph{small sets}\index{Small set}, and in the definition of a category they require that the morphisms form small sets---we make no such requirement.)}
\item for each $A,B,C\in \Obj (\cat{C})$, a function $\circ :\Mor _{\cat{C}}(B,C)\times \Mor _{\cat{C}}(A,B)\rightarrow \Mor _{\cat{C}}(A,C)$ called \term{composition}\index{Composition};
\item and for each $A\in \Obj (\cat{C})$ a distinguished element $\id _{A}\in \Mor _{\cat{C}}(A,A)$, the \term{identity}\index{Identity (in a category)} of $A$;
\end{data}
such that
\begin{enumerate}
\item $\circ$ is `associative', that is, $f\circ (g\circ h)=(f\circ g)\circ h$ for all morphisms $f,g,h$ for which these composition make sense,\footnote{In case you're wondering, the quotes around ``associative'' are used because usually the word ``associative'' refers to a property that a binary operation has.  A binary operation on a set $S$ is, by definition, a function from $X\times X$ into $X$.  Composition however in general is a function from $X\times Y$ into $Z$ for $X\coloneqq \Mor _{\cat{C}}(B,C)$, $Y\coloneqq \Mor _{\cat{C}}(A,B)$ and $Z\coloneqq \Mor _{\cat{C}}(A,C)$, and hence not a binary operation.} and
\item $f\circ \id _A=f=\id _A\circ f$ for all $A\in \Obj (\cat{C})$.
\end{enumerate}
\begin{rmk}
We write
\begin{equation}
\Mor _{\cat{C}}\ceqq \bigsqcup _{A,B\in \Obj (\cat{C})}\Mor _{\cat{C}}(A,B)
\end{equation}\index[notation]{$\Mor _{\cat{C}}$}
for the collection of all morphisms in $\cat{C}$.
\end{rmk}
\begin{rmk}
The term \term{map}\index{Map} is often used synonymously with the term ``morphism'', though perhaps in a more casual manner.  For example, it is not uncommon to see people say ``linear map'' instead of ``map of vector spaces'' or ``map in the category of vector spaces''.
\end{rmk}
\begin{rmk}
If the category $\cat{C}$ is clear from context, we may simply write $\Mor (A,B)$\index[notation]{$\Mor (A,B)$}.
\end{rmk}
\begin{rmk}
We mentioned above that the morphisms relevant to vector spaces are the linear functions.  Of course, nothing about the definition of a category \emph{requires} this be the case---you could just as well consider the category whose objects are vector spaces and whose morphisms are \emph{all} functions---it just turns out that these weird examples aren't particularly useful.
\end{rmk}
\end{dfn}
The intuition here is that the objects $\Obj (\cat{C})$ are the objects you are interested in studying (for example, vector spaces), and for objects $A,B\in \Obj (\cat{C})$, the morphisms $\Mor _{\cat{C}}(A,B)$ are the maps relevant to the study of the objects in $\cat{C}$ (for example, linear functions from $A$ to $B$).  For us, it will usually be the case that every element of $\Obj (\cat{C})$ is a set equipped with extra structure (e.g.~a binary operation) and the morphisms are just the functions that `preserve' this structure (e.g.~homomorphisms).  In fact, there is a term for such categories---see \cref{ConcreteCategory}.

At the moment, this might seem a bit abstract because of the lack of examples.  As you continue through the main text, you will encounter more examples of categories, which will likely elucidate this abstract definition.  However, even already we have a couple basic examples of categories.
\begin{exm}{The category of sets}{exm1.2.2}\index{Category of sets}
The category of sets is the category $\Set$\index[notation]{$\Set$}
\begin{enumerate}
\item whose collection of objects $\Obj (\Set )$ is the collection of all sets,\footnote{See \cref{sbsA.1.1} for clarification as to what we actually mean by the phrase ``all sets''.};
\item with morphism set $\Mor _{\Set}(X,Y)$ precisely the set of all functions from $X$ to $Y$;
\item whose composition is given by ordinary function composition; and
\item whose the identities are given by the identity functions.
\end{enumerate}
\end{exm}
We also have another example at our disposal, namely the category of preordered sets.
\begin{exm}{The category of preordered sets}{}\index{Category of preordered sets}
The category of preordered sets is the category $\Pre$\index[notation]{$\Pre$}
\begin{enumerate}
\item whose collection of objects $\Obj (\Pre )$ is the collection of all preordered sets;
\item with morphism set $\Mor _{\Pre}(X,Y)$ precisely the set of all nondecreasing functions from $X$ to $Y$;
\item whose composition is given by ordinary function composition; and
\item whose identities are given by the identity functions.
\end{enumerate}
\end{exm}
The idea here is that the only structure on a preordered set is the preorder, and that the precise notion of what it means to `preserve' this structure is to be nondecreasing.  Of course, we could everywhere replace the word ``preorder'' (or its obvious derivatives) with ``partial-order'' or ``total-order'' and everything would make just as much sense.  Upon doing so, we would obtain the category of partially-ordered sets and the category of totally-ordered sets respectively.

We also have the category of magmas.
\begin{exm}{The category of magmas}{TheCategoryOfMagmas}\index{Category of magmas}
The category of magmas is the category $\Mag$\index[notation]{$\Mag$}
\begin{enumerate}
\item whose collection of objects $\Obj (\Mag )$ is the collection of all magmas;
\item with morphism set $\Mor _{\Mag}(X,Y)$ precisely the set of all homomorphisms from $X$ to $Y$;
\item whose composition is given by ordinary function composition; and
\item whose identities are given by the identity functions.
\end{enumerate}
\end{exm}
Similarly, the idea here is that the only structure here is that of the binary operation (and possibly an identity element) and that it is the homomorphisms which preserve this structure.  Of course, we could everywhere here replace the word ``magma'' with ``semigroup'', ``monoid'', ``group'', etc.~and everything would make just as much sense.  Upon doing so, we would obtain the categories of semigroups, the category of monoids, and the category of groups respectively.

Finally we have the category of rgs.
\begin{exm}{The category of rgs}{}\index{Category of rgs}
The category of rgs is the category $\Rg$\index[notation]{$\Rg$}
\begin{enumerate}
\item whose collection of objects $\Obj (\Rg )$ is the collection of all rgs;
\item with morphism set $\Mor _{\Rg}(X,Y)$ is precisely the set of all homomorphisms from $X$ to $Y$;
\item whose composition is given by ordinary function composition; and
\item and whose identities are given by the identity functions.
\end{enumerate}
\begin{rmk}
The same as before, we could have everywhere replaced the word ``rg'' with ``rig'', ``rng'', or ``ring''.  These categories are denoted $\Rig$\index[notation]{$\Rig$}, $\Rng$\index[notation]{$\Rng$}, and $\Ring$\index[notation]{$\Ring$} respectively.
\end{rmk}
\end{exm}

As mentioned previously, it should almost always be the case that the examples of categories we encounter are of this form, that is, in which the objects are ``sets equipped with extra structure'' and the morphisms are ``functions which `preserve' this structure''.  The term for such categories is \emph{concrete}.
\begin{dfn}{Concrete category}{ConcreteCategory}
Let $\cat{C}$ be a category.  Then, $\cat{C}$ is \term{concrete}\index{Concrete category} iff for all $A,B\in \Obj (\cat{C})$, $\Mor _{\cat{C}}(A,B)\subseteq \Mor _{\Set}(A,B)$.
\begin{rmk}
When defining categories, if the category happens to be concrete, we shall omit an explicit statement of the composition and identity, and instead will simply say that the category is concrete (e.g.~``The category of XYZ is the concrete category $\cat{C}$\textellipsis ''.).
\end{rmk}
\begin{rmk}
Warning:  Strictly speaking, this doesn't actually make sense as $A$ and $B$ are not actually sets.  Implicit in this is that we are additionally given a way of regarding objects of $\cat{C}$ as sets.  For example, in the case of the category of vector spaces, we regard a vector space as a set simply by ``forgetting'' the addition and scaling operations.  To better understand this, it might help to see an example of a nonconcrete category---see the following example.
\end{rmk}
\end{dfn}
While not terribly important for us, as you might now be wondering ``What could a nonconcrete category possibly look like?'', we present the following example.
\begin{exm}{A category that is not concrete}{}
Let $\coord{X,\leq}$ be a preordered set and define $\cat{C}_X$ to be the category
\begin{enumerate}
\item with collection of objects $\Obj (\cat{C}_X)\coloneqq X$;
\item with morphism set $\Mor _{\cat{C}_X}(x,y)$  taken to be a singleton iff $x\leq y$ and empty otherwise---in the case that $x\leq y$, let us write $x\rightarrow y$ for the unique element of $\Mor _{\cat{C}_X}(x,y)$;
\item with composition defined by $(y\rightarrow z)\circ (x\rightarrow y)\coloneqq x\rightarrow z$; and
\item with identity $\id _x\coloneqq x\rightarrow x$.
\end{enumerate}
\begin{exr}{}{}
Check that $\cat{C}_X$ is in fact a category.
\begin{rmk}
Note how the axiom of reflexivity corresponds to the identities and the axiom of transitivity corresponds to composition.
\end{rmk}
\end{exr}
\end{exm}

\section{Some basic concepts}

The real reason we introduce the definition of a category in notes like these is that it allows us to introduce consistent notation and terminology throughout the text.  Had we forgone even the very basics of categories, we would still be able to do the same mathematics, but the notation and terminology would be much more ad hoc.
\begin{dfn}{Domain and codomain}{}
Let $f\colon A\rightarrow B$ be a morphism in a category.  Then, the \term{domain}\index{Domain (of a morphism)} of $f$ is $A$ and the \term{codomain}\index{Codomain (of a morphism)} of $f$ is $B$.
\begin{rmk}
Of course, these terms generalize the notions of domain and codomain for sets.
\end{rmk}
\end{dfn}
\begin{dfn}{Endomorphism}{Endomorphism}
Let $\cat{C}$ be a category and let $A\in \Obj (\cat{C})$.  Then, an \term{endomorphism}\index{Endomorphism} is a morphism $f\in \Mor _{\cat{C}}(A,A)$.  We write $\End _{\cat{C}}(A)\coloneqq \Mor _{\cat{C}}(A,A)$\index[notation]{$\End _{\cat{C}}(A)$} for the collection of endomorphisms on $A$.
\begin{rmk}
In other words, ``endomorphism'' is just a fancy name for a morphism with the same domain and codomain.
\end{rmk}
\end{dfn}
\begin{dfn}{Isomorphism}{Isomorphism}
Let $f\colon A\rightarrow B$ be a morphism in a category.  Then, $f$ is an \term{isomorphism}\index{Isomorphism} iff it is invertible, i.e., iff there is a morphism $g\colon B\rightarrow A$ such that $g\circ f=\id _A$ and $f\circ g=\id _B$.  In this case, $g$ is an \term{inverse} of $f$.  The collection of all isomorphisms from $A$ to $B$ is denoted by $\Iso _{\cat{C}}(A,B)$\index[notation]{$\Iso _{\cat{C}}(A,B)$}.
\end{dfn}
\begin{exr}{}{Inverses are unique}
Let $f\colon A\rightarrow B$ be a morphism in a category and let $g,h \colon B\rightarrow A$ be two inverses of $f$.  Show that $g=h$.
\begin{rmk}
As a result of this exercise, we may denote \emph{the} inverse of $f$ by $f^{-1}$.\footnote{If inverses were not unique, then the notation $f^{-1}$ would be ambiguous:  what inverse would we be referring to?}
\end{rmk}
\end{exr}
\begin{exr}{}{exr2.1.3}
Show that a morphism in $\Set$ is an isomorphism iff it is bijective.
\end{exr}
\begin{exr}{}{}
Show that a morphism in $\Mag$ is an isomorphism iff (i)~it is bijective, (ii)~it is a homomorphism, and (iii)~its inverse is a homomorphism.
\end{exr}
\begin{exr}{}{exrA.2.11x}
Show that the inverse of a bijective homomorphism of magmas is itself a homomorphism.
\begin{rmk}
Thus, if you want to show a function is an isomorphism of magmas, in fact you only need to check (i)~and (ii)~of the previous exercise, because then you get (iii)~for free.  (Of course, essentially the very same thing happens in $\Rg$ as well.)
\end{rmk}
\end{exr}
\begin{dfn}{Isomorphic}{dfnA.2.10}
Let $A,B\in \Obj (\cat{C})$ be objects in a category.  Then, $A$ and $B$ are \term{isomorphic}\index{Isomorphic} iff there is an isomorphism from $A$ to $B$.  In this case, we write $A\cong _{\cat{C}}B$\index[notation]{$A\cong _{\cat{C}}B$}, or just $A\cong B$\index[notation]{$A\cong B$} if the category $\cat{C}$ is clear from context.
\end{dfn}
\begin{exr}{}{exrA.2.11}
Let $\cat{C}$ be a category.  Show that $\cong _{\cat{C}}$ is an equivalence relation on $\Obj (\cat{C})$.
\end{exr}
\begin{dfn}{Automorphisms}{}
Let $\cat{C}$ be a category and let $A\in \Obj (\cat{C})$.  Then, an \term{automorphism}\index{Automorphism} $f\colon A\rightarrow A$ is a morphism which is both an endomorphism and an isomorphism.  We write $\Aut _{\cat{C}}(A)\coloneqq \Iso _{\cat{C}}(A,A)$\index[notation]{$\Aut _{\cat{C}}(A)$} for the collection of automorphisms of $A$.
\begin{rmk}
The automorphisms of $A$ are often thought of as the \emph{symmetries} of $A$.
\end{rmk}
\end{dfn}
The following result can be seen as a reason why the concepts of monoid and group are so ubiquitous in mathematics.
\begin{prp}{}{prpB.2.9}
Let $\cat{C}$ be a category and let $A\in \Obj (\cat{C})$.  Then,
\begin{enumerate}
\item $\coord{\End _{\cat{C}}(A),\circ ,\id _A}$ is a monoid; and
\item $\coord{\Aut _{\cat{C}}(A),\circ ,\id _A,\blank ^{-1}}$ is a group.
\end{enumerate}
\begin{proof}
We leave this as an exercise.
\begin{exr}[breakable=false]{}{}
Prove this yourself.
\end{exr}
\end{proof}
\end{prp}

Finally, we end this section with a concrete example of isomorphism.
\begin{exm}{}{}
The category we work in is $\Grp$.  Thus, we are going to present an example of two different groups which are isomorphic in $\Grp$.

On one hand, we have $\coord{\Z /2\Z,+,0,-}$, which if you have been reading along in the appendix, should be relatively familiar to you by now.\footnote{Note that, we can regard $\Z /2\Z$ as a ring, explicitly $\coord{\Z /2\Z ,+,0,-,\cdot}$, but we don't.  We're \emph{forgetting} about the extra binary operation, and upon doing so, we obtain the group $\coord{\Z /2\Z ,+,0,-}$.}.  Regardless, however, we list the addition table for $\Z /2\Z$ for absolute concreteness.
\begin{equation}\label{eqnA.2.17}
\begin{array}{r|cc}
+ & 0 & 1 \\ \hline
0 & 0 & 1 \\
1 & 1 & 0
\end{array}
\end{equation}

On the other hand, let's define a group you haven't seen before, $C_2\coloneqq \{ 1,-1\}$ with binary operation defined by
\begin{equation}\label{eqnA.2.18}
\begin{array}{r|cc}
\cdot & 1 & -1 \\ \hline
1 & 1 & -1 \\
-1 & -1 & 1
\end{array}.
\end{equation}
(Feel free to check that this does indeed satisfy the axioms of a group (in fact, commutative group) if you like, but this is not so crucial .)

Now, the key to notice is the following: aside from a relabeling of symbols, \emph{the tables in \eqref{eqnA.2.17} and \eqref{eqnA.2.18} are identical}.  Explicitly, the relabeling is $0\mapsto 1$, $1\mapsto -1$, and $+\mapsto \cdot$.  The precise way of saying this is:  the function $\phi \colon \Z /2\Z \rightarrow C_2$ defined by $\phi (0)\coloneqq 1$ and $\phi (1)\coloneqq -1$ \emph{is an isomorphism in $\Grp$} (or, to say the same thing in slightly different language, \emph{is an isomorphism of groups}).

While not always literally true, depending on your category, I think at an intuitive level it is safe to think of two objects that are isomorphic as being `the same' up to a relabeling of the elements.  This is why, in mathematics, it is very common to not distinguish between objects which are isomorphic.  This would be like making a distinction between the two equations $x^2+5x-3=0$ and $y^2+5y-3=0$ in elementary algebra:  the name of the variable in question doesn't have any serious effect on the mathematics---it's just a name.
\end{exm}

We end this subsection with relatively tricky concepts, that of \emph{embedding} and \emph{quotient}.  Roughly speaking, you might say that ``embedding'' is the categorical generalization of the concept of a subset.  In a general category with objects $A$ and $B$, the statement $A\subseteq B$ just doesn't make sense---we need $A$ and $B$ to be sets to even posit the question ``Is $A$ a subset of $B$?''.  But even in concrete categories, where $A\subseteq B$ \emph{does} make sense, simply being a subset of an object is the `wrong' notion---see \cref{exmA.2.20}.  The basic idea which we want to make precise then is that, in addition to $A$ being a subset of $B$, the structure on $A$ is somehow the structure `inherited' from $B$.

The first thing to realize is that if we are to be categorical about things (by which I mean that the \emph{morphisms} are to play a central role), is that we shouldn't try to generalize the concept of ``subset'' to objects, but rather, to morphisms.  That is to say, the objective should be to figure out what it means for a \emph{morphism} to be an embedding.  So, let $f\colon A\rightarrow B$ be a morphism in a concrete category.\footnote{We take our category to be concrete because, to the best of my knowledge, there is no definition of embedding/quotient that is satisfactory for all (not-necessarily-concrete) categories.}  In order to be an embedding, $f$ has to be at the very least an embedding of the underlying sets, that is, $f$ should be injective.  We need more than this however:  if $C$ is another object `contained' in $A$, by which I mean there is a morphism $g\colon C\rightarrow A$, then, just as I can consider a subset of a subset as a subset,\footnote{That is, if $X$ is a subset of $Y$ and $Y$ is a subset of $Z$, then of course I can consider $X$ as a subset of $Z$ as well.} if $f$ is to be an embedding, I should be able to consider $C$ directly as being `contained' in $B$, that is, $f\circ g$ should be a morphism as well.  This is made precise with the following definition (as well as the `dual' concept of \emph{quotient}).
\begin{dfn}{Embedding and quotient}{EmbeddingAndQuotient}
Let $\cat{C}$ be a concrete category, let $A,B\in \Obj (\cat{C})$, and let $f\in \Mor _{\cat{C}}(A,B)$.
\begin{enumerate}
\item $f$ is an \term{embedding}\index{Embedding (category theory)} iff $f$ is injective and whenever a function $g\colon C\rightarrow A$ is such $f\circ g\in \Mor _{\cat{C}}(C,B)$ (with $C\in \Obj (\cat{C})$), it follows that $g\in \Mor _{\cat{C}}(C,A)$.
\item $f$ is a \term{quotient}\index{Quotient (category theory)} iff $f$ is surjective and whenever a function $g\colon B\rightarrow C$ is such that $g\circ f\in \Mor _{\cat{C}}(A,C)$ (with $C\in \Obj (\cat{C})$), it follows that $g\in \Mor _{\cat{C}}(B,C)$.
\end{enumerate}
\end{dfn}
Now that we have the precise definition in hand, we can \emph{prove} that being an injective morphism is not enough.
\begin{exm}{A nondecreasing injective function that is not an embedding}{exmA.2.20}
Define $X\coloneqq \{ A,B\}$ equipped with the trivial partial-order\footnote{That is, $A\leq A$, $B\leq B$, and nothing else.} and define $Y\coloneqq \{ 1,2\}$ with the only nontrivial relation being $1\leq 2$.

Define $f\colon X\rightarrow Y$ by $f(A)\coloneqq 1$ and $f(B)\coloneqq 2$.  If $x_1\leq x_2$ in $X$, then in fact we must have that $x_1=x_2$ (because it's the trivial order), and so of course $f(x_1)\leq f(x_2)$ (in fact, we have equality.  Thus, $f$ is nondecreasing.  It is also certainly injective (in fact, bijective).

We wish to show that $f$ is not an embedding.  Define $g\colon Y\rightarrow X$ by $g(A)\coloneqq 1$ and $g(B)\coloneqq 2$.  Then, $f\circ g=\id _Y$ is certainly nondecreasing (i.e.~a morphism in $\Pre$), but yet $g$ is not nondecreasing.  Hence, $f$ is not an embedding.
\begin{rmk}
Though you may be able to follow the proof, it's also important to understand why $f$ \emph{shouldn't} be an embedding.  That is to say, while it may be true that our definition has the property that $f$ is not an embedding, furthermore, any definition we might have come up with should have this property.  The reason is that, if you consider $X$ as a subset of $Y$ (via $f$), then the order on $Y$ would dictate that $A\leq B$ (because $f(A)\leq f(B)$), which is not the case.  In this case, the `structure' on $X$ is \emph{not} that inherited from $Y$ via $f$.
\end{rmk}
\end{exm}
On the other hand, we do have the following.
\begin{exr}{}{exrA.2.21}
Let $\cat{C}$ be either the category $\Set$, $\Rg$, or $\Mag$.
\begin{enumerate}
\item Show that a morphism in $\cat{C}$ is an embedding iff it is injective.
\item Show that a morphism in $\cat{C}$ is a quotient iff it is surjective.
\end{enumerate}
\end{exr}
\begin{exr}{}{exrA.2.22}
\begin{enumerate}
\item Show that a morphism $f$ in $\Pre$ is an embedding iff it is injective and has the property that $f(x_1)\leq f(x_2)$ iff $x_1\leq x_2$.
\item Show that a morphism $f$ in $\Pre$ is a quotient iff it is surjective and has the property that $f(x_1)\leq f(x_2)$ iff $x_1\leq x_2$.
\end{enumerate}
\end{exr}

\section{Functors and natural-transformations}

\subsection{Functors}

The motivating idea behind the definition of a category is we wanted a definition that would contain the data of both the objects under study and the \emph{morphisms} which `preserve' the relevant structure.\footnote{Of course, any choice of morphisms can work so long as they satisfy the axioms, but most of the examples we're interested in the morphisms will be taken to be the ones which ``preserve'' the structure, and furthermore, when dealing with general abstract categories, I find this to be my guiding intuition.}  We've just now introduced a new type of object, namely, a category, and so now what might ask ``What is the `right' notion of morphisms between categories?''.  Well, looking back at the definition (\cref{Category}), we see that there are four pieces of data (I) the objects, (II) the morphisms, (III) the composition, and (IV) the identity morphisms.  One thus comes up with the following definition of a `morphism of categories', what is called a \emph{functor}.
\begin{dfn}{Functor}{Functor}
	Let $\cat{C}$ and $\cat{D}$ be categories.  A \term{functor}\index{Functor} is
	\begin{data}
		\item a function $\functor{f}\colon \Obj (\cat{C})\rightarrow \Obj (\cat{D})$; together with
		\item for every $A,B\in \Obj (\cat{C})$, a function\footnote{Of course, this function depends on $A$ and $B$, but by abuse of notation we omit this dependence.}
		\begin{equation}
			\functor{f}\colon \Mor _{\cat{C}}(A,B)\rightarrow \Mor _{\cat{D}}(\functor{f}(A),\functor{f}(B))
		\end{equation}
	\end{data}
	such that
	\begin{enumerate}
		\item $\functor{f}(g\circ f)=\functor{f}(g)\circ \functor{f}(f)$ for all composable morphisms $f,g\in \Mor _{\cat{C}}$; and
		\item $\functor{f}(\id _A)=\id _{\functor{f}(A)}$ for all $A\in \Obj (\cat{C})$.
	\end{enumerate}
	\begin{rmk}
		Thus, a functor between two categories maps the objects to objects and the morphisms to the morphisms, and does so in such a way so as to ``preserve'' composition and the identities.
	\end{rmk}
\end{dfn}
\begin{exm}{The category of categories}{}
	The category of categories is the category $\Cat$
	\begin{enumerate}
		\item whose collection of objects $\Obj (\Cat )$ is the collection of all categories;
		\item with morphism set $\Mor _{\Cat}(\cat{C},\cat{D})$ precisely the set of all functors from $\cat{C}$ to $\cat{D}$;
		\item whose composition is given by ordinary function composition of both the functions on objects and morphisms;
		\item and whose identities are given by the functors for which both the functions on objects and morphisms is the identity function.
	\end{enumerate}
	\begin{rmk}
		Okay, so I won't blame you if you think this is total nonsense.  ``Category of categories''?  That's just asking for some sort of paradox.  The answer to this of course is that we're being sloppy.
		
		You'll recall during our discussion of Russell's Paradox (around \eqref{A.1.1}), we explained that our method of resolving the paradox was to fix some set $U$, the ``universe'', that satisfied certain properties which made it reasonable to do ``all of mathematics'' inside $U$.  Russel's Paradox was then resolved by concluding that the set in question simply was just not an element of $U$.
		
		Something nearly identity is going on here.  Implicitly, we pick a smallest ``universe'' we wish to work in $U_0$.  Then, whenever we say something like ``the category of all rgs'', it is implicit that all those rgs are coming from $U_0$.  Doing this will usually result in categories that are themselves not contained in $U_0$, but rather, a larger universe $U_1$.  Then, when we form ``the category of all categories'', it is implicit that all our categories are coming from $U_1$.  And again, the resulting category will not be in $U_1$.  But so what.  99\% of the time, that doesn't matter.  Yes, yes, technically we should be saying ``The category of all categories in $U_1$.'', but you can see how that would get really annoying really fast, and as it will never matter for us, we are sloppy.\footnote{And also give this remark because we feel guilty about being sloppy.}
	\end{rmk}
\end{exm}

When I think of functors intuitively, I think of them as some sort of mathematical ``constructions'', something that takes in objects of $\cat{C}$ and spits out objects of $\cat{D}$.  For example, the entire subject of algebraic topology originated as studying ``constructions'' that associate algebraic objects (like groups) to ``geometric'' objects (like topological spaces).  Unfortunately, our limited background gives us a limited supply of examples, but we do have some things.
\begin{exm}{The power-set functor}{}
	Consider the functor $\Set \rightarrow \Pos$ from the category of sets to the category of partially-ordered sets defined by
	\begin{subequations}
		\begin{align}
			\Obj (\Set )\ni X & \mapsto \coord{2^X,\subseteq}\in \Obj (\Pos ) \\
			\Mor _{\Set}(X,Y)\ni f & \mapsto f\in \Mor _{\Pos}(2^X,2^Y),
		\end{align}
	\end{subequations}
	where $f\in \Mor _{\Pos}(2^X,2^Y)$ is the function that sends $S\subseteq X$ to $f(Y)\subseteq Y$.
	\begin{exr}[breakable=false]{}{}
		Check that this defines a functor.
	\end{exr}
\end{exm}

The next example we would like to consider is the \emph{dual-space} functor $V\mapsto V^{\T}$ on the category of vector spaces.  Unfortunately, however, we run into a bit of a problem when trying to do this naively.  This functor on objects acts as $V\mapsto V^{\T}$, and so naturally on morphisms this functor acts as $T\mapsto T^{\T}$.  The definition of a functor, however, requires that, if $T\in \Mor _{\Mod{\K}}(V,W)$, then $T^{\T}\in \Mor _{\Mod{\K}}(V^{\T},W^{\T})$.  But it's not.  $T^{\T}$ is not a map from $V^{\T}$ to $W^{\T}$, but rather, a map from $W^{\T}$ to $V^{\T}$.  We get around this little hiccup by defining the \emph{opposite category}.
\begin{dfn}{Opposite category}{OppositeCategory}
	Let $\cat{C}$ be a category.  Then, the \term{opposite category}\index{Opposite category} of $\cat{C}$, $\cat{C}^{\op}$\index[notation]{$\cat{C}^{\op}$}, is defined by
	\begin{data}
		\item $\Obj (\cat{C}^{\op})\ceqq \Obj (\cat{C})$;
		\item for $A,B\in \Obj (\cat{C}^{\op})$,
		\begin{equation}
			\Mor _{\cat{C}^{\op}}(A,B)\ceqq \Mor _{\cat{C}^{\op}}(B,A);
		\end{equation}
		\item for $f,g\in \Mor _{\cat{C}^{\op}}$ composable,
		\begin{equation}
			f\circ _{\op}g\ceqq g\circ f;
		\end{equation}
		and
		\item for $A\in \Obj (\cat{C}^{\op})$, $\id _A\ceqq \id _A$.
	\end{data}
	\begin{rmk}
		In brief, $\cat{C}^{\op}$ has the same objects as $\cat{C}$, but the morphisms go in the \emph{opposite} direction.
	\end{rmk}
\end{dfn}
Such a construction might seem a bit silly, but it allows us to make the following convenient definition.
\begin{dfn}{Cofunctor}{Cofunctor}
	Let $\cat{C}$ and $\cat{D}$ be categories.  Then, a \term{cofunctor}\index{Cofunctor} from $\cat{C}$ to $\cat{D}$ is a functor $\cat{C}^{\op}\rightarrow \cat{D}$.
	\begin{rmk}
		We will often write things like ``Let $\functor{f}\colon \cat{C}\rightarrow \cat{D}$ be a cofunctor\textellipsis ''.  Strictly speaking, the domain category is $\cat{C}$, not $\cat{C}^{\op}$, but this notation is convenient for what will eventually be obvious reasons.
	\end{rmk}
	\begin{rmk}
		This is more commonly referred to as a \term{contravariant functor}\index{Contravariant functor} (in which case an `ordinary' functor is called a \term{covariant functor}\index{Covariant functor} for contrast).  I actually think this is pretty terrible terminology as in pretty much every other context in mathematics something with the prefix ``co'' is the ``dual'' thing, not the `primary' thing.
	\end{rmk}
\end{dfn}
\begin{prp}{}{}
	Let $\cat{C}$ and $\cat{D}$ be categories, let $\functor{f}\colon \Obj (\cat{C})\rightarrow \Obj (\cat{D})$ be a function, and for every $A,B\in \Obj (\cat{C})$ let $\functor{f}\colon \Mor _{\cat{C}}(A,B)\rightarrow \Mor _{\cat{D}}(\functor{f}(B),\functor{f}(A))$ be a function.  Then, this defines a cofunctor iff
	\begin{enumerate}
		\item $\functor{f}(g\circ f)=\functor{f}(f)\circ \functor{f}(g)$ for all composable morphisms $f,g\in \Mor _{\cat{C}}$; and
		\item $\functor{f}(\id _A)=\id _{\functor{f}(A)}$.
	\end{enumerate}
	\begin{rmk}
		Note that this is exactly the same as the definition of a functor (\cref{Functor}), except here the requirement is $\functor{f}(g\circ f)=\functor{f}(f)\circ \functor{f}(g)$ (instead of $\functor{f}(g\circ f)=\functor{f}(g)\circ \functor{f}(f)$).  In this sense, cofunctors are just functors which `flip' the order of composition.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
With this language in hand, we can now return to what is probably the most relevant example for us.
\begin{exm}{Dual-space}{}
	Let $\K$ be a cring and consider the \emph{co}functor $\Mod{\K}\rightarrow \Mod{\K}$ defined by
	\begin{subequations}
		\begin{align*}
			\Obj (\Mod{\K})\ni V & \mapsto V^{\T}\in \Obj (\Mod{\K}) \\
			\Mor _{\Mod{\K}}(V,W)\ni T & \mapsto T^{\T}\in \Mor _{\Mod{\K}}(W^{\T},V^{\T})
		\end{align*}
	\end{subequations}
	\begin{exr}[breakable=false]{}{}
		Check that this defines a functor.
	\end{exr}
\end{exm}

\subsection{Natural-transformations}\label{sbsB.2.2}

We can compose the dual-space functor with itself to obtain the ``double-dual-space'' functor $V\mapsto [V^{\T}]^{\T}$.  Recall that (\cref{DualityOfTheDual}), for every $\K$-module $V$, we actually have a linear-transformation $V\rightarrow [V^{\T}]^{\T}$, and in fact, this is an isomorphism in the context of finite-dimensional vector spaces.

On the other hand, if $V$ is a finite-dimensional vector space, then so is $V^{\T}$.  Furthermore, they have the same dimension, and hence are isomorphic.\footnote{Because they are both isomorphic to $\K ^d$ by taking coordinates (after choosing bases).}  The isomorphisms $V\cong V^{\T}$ and $V\cong [V^{\T}]^{\T}$ are fundamentally different, however.  The latter is what is called a \emph{natural isomorphism}.  To see more clearly how these isomorphisms are different, let us recall more explicitly what they are.

First of all, the map $V\rightarrow [V^{\T}]^{\T}$ is given by
\begin{equation}
	v\mapsto \braket{\blankdot ,v}.
\end{equation}
On the other hand, the isomorphism $V\rightarrow V^{\T}$ is more complicated.  Let $\{ b_1,\ldots ,b_d\}$ be a basis for $V$, so that the dual basis $\{ b_1^{\T},\ldots ,b_d^{\T}\}$ is a basis for $V^{\T}$.  Then,
\begin{equation}
	V\ni \alpha _1\cdot b_1+\cdots +\alpha _d\cdot b_d\mapsto \alpha _1\cdot b_1^{\T}+\cdots +\alpha _d\cdot b_d^{\T}\in V^{\T}
\end{equation}
is an isomorphism.  The significant thing to note here is that the definition of this morphism \emph{depended on an arbitrary choice} of a basis for $V$.

Thus, in order to define the morphism $V\rightarrow V^{\T}$ for every vector space $V$, you'll have to pick a basis for every single vector space, and there is no single ``natural'' way to do so.  The isomorphisms that I happen to use are almost certainly going to be different than the ones you choose to use.  On the other hand, there is \emph{no choice} when it comes to the isomorphism $V\rightarrow [V^{\T}]^{\T}$.  Intuitively, this isomorphism doesn't depend on $V$, the same definition works for every vector space, whereas the isomorphisms $V\rightarrow V^{\T}$ do depend on $V$.

The precise notion which distinguishes these two is that of a \emph{natural transformation}:  $V\rightarrow [V^{\T}]^{\T}$ will be a natural isomorphism, whereas $V\rightarrow V^{\T}$ will not be.
\begin{dfn}{Natural-transformation}{NaturalTransformation}
	Let $\cat{C}$ and $\cat{D}$ be functors, and let $\functor{f},\functor{g}\colon \cat{C}\rightarrow \cat{D}$ be functors.  Then, a \term{natural transformation}\index{Natural transformation} from $\functor{f}$ to $\functor{g}$ is, for every $A\in \Obj (\cat{C})$, a morphism $\eta _A\colon \functor{f}(A)\rightarrow \functor{f}(B)$, such that
	\begin{equation}
		\begin{tikzcd}
			\functor{f}(A) \ar[r,"\functor{f}(f)"] \ar[d,"\eta _A"'] & \functor{f}(B) \ar[d,"\eta _B"] \\
			\functor{g}(A) \ar[r,"\functor{g}(f)"'] & \functor{g}(B)
		\end{tikzcd}
	\end{equation}
	commutes for every morphism $f\in \Mor _{\cat{C}}(A,B)$ and $A,B\in \Obj (\cat{C})$.
	\begin{rmk}
		A \emph{diagram} in this sense of the word refers to a set of objects and morphisms between them indicated by drawing arrows for each morphism (from the domain to the codomain).  A path from one object to another (in the direction indicated by the arrows) then corresponds to the composition of the corresponding morphisms.  If you select any two objects in a given diagram, in general, there will be more than one path from the first to the second; the diagram is said to \emph{commute} iff all corresponding compositions agree.
			
		For example, in this case, the phrase ``the following diagram commutes'' should be understood as short-hand for the statement ``$\functor{g}(f)\circ \eta _A=\eta _B\circ \functor{f}(f)$''.
	\end{rmk}
	\begin{rmk}
		The entire natural-transformation is usually just denoted ``$\eta$'', in which case $\eta _A$ is referred to as the \term{component}\index{Component (of a natural-transformation)} of $\eta$ at $A$.
	\end{rmk}
	\begin{rmk}
		Admittedly, this definition is difficult to understand at first sight.  For one thing, it's not clear how this captures ``doesn't depend on $A$/$B$''.  I will do the best I can to explain.
		
		The idea is that, if I can define the morphisms $\eta _A$ in such a way that doesn't make use of anything special to this particular $A$, then it shouldn't matter whether I `go to a different object' and then apply the morphism, or if I first apply the morphism and then ``go to a different object''.  (Here, ``going to a different object'' corresponds to $\functor{f}(f)$ and $\functor{g}(f)$, and ``the morphism'' refers to $\eta _A$ and $\eta _B$.)
	\end{rmk}
\end{dfn}
It turns out that, in the following sense, natural-transformations should themselves be thought of as \emph{morphisms of functors}.
\begin{dfn}{Category of functors}{CategoryOfFunctors}
	Let $\cat{C}$ and $\cat{D}$ be categories.  Then, the \term{category of functors}\index{Category of functors} from $\cat{C}$ to $\cat{D}$, $\Mor _{\Cat}(\cat{C},\cat{D})$, is defined by
	\begin{data}
		\item $\Obj \left( \Mor _{\Cat}(\cat{C},\cat{D})\right) \ceqq \Mor _{\Cat}(\cat{C},\cat{D})$;
		\item $\Mor _{\Mor _{\Cat}(\cat{C},\cat{D})}(\functor{f},\functor{g})$ is the collection of natural-transformations from $\functor{f}$ to $\functor{g}$;
		\item composition of natural-transformations is defined componentwise; and
		\item $[\id _{\functor{f}}]_A\ceqq \id _A$.
	\end{data}
	\begin{rmk}
		Thus, the objects are functors, the morphisms are the natural-transformations, composition is done the way you would expect (componentwise), and the identity on $\functor{f}\in \Obj \left( \Mor _{\Cat}(\cat{C},\cat{D})\right)$ is the natural-transformation whose component as the object $A\in \cat{C}$ is $\id _A$.
	\end{rmk}
	\begin{rmk}
		We are abusing notation and using $\Mor _{\Cat}(\cat{C},\cat{D})$ to denote the collection of functors from $\cat{C}$ to $\cat{D}$ as well as the corresponding category.
	\end{rmk}
\end{dfn}
As in every category, we have a notion of isomorphism.  In the case of categories of functors, the following term is used.
\begin{dfn}{Natural-isomorphism}{NaturalIsomorphism}
	Let $\eta \colon \functor{f}\rightarrow \functor{g}$ be a natural-transformation of functors $\functor{f},\functor{g}\colon \cat{C}\rightarrow \cat{D}$.  Then, $\eta$ is a \term{natural-isomorphism}\index{Natural-isomorphism} iff $\eta$ is an isomorphism in $\Mor _{\Cat}(\cat{C},\cat{G})$.
	\begin{rmk}
		If we're thinking of functors as some sort of mathematical `constructions' (e.g.~taking the dual of a vector space), then, intuitively speaking, saying that ``$\functor{f}$ is naturally-isomorphic to $\functor{g}$'' is saying more than just ``$\functor{f}(A)$ is isomorphic to $\functor{g}(A)$ for all $A\in \Obj (\cat{C})$'', but rather that the entire ``constructions'' $A\mapsto \functor{f}(A)$ and $A\mapsto \functor{g}(A)$ are isomorphic.
	\end{rmk}
\end{dfn}
Fortunately, this is equivalent to what you would hope.
\begin{prp}{}{}
	Let $\eta \colon \functor{f}\rightarrow \functor{g}$ be a natural-transformation of functors $\functor{f},\functor{g}\colon \cat{C}\rightarrow \cat{D}$.  Then, $\eta$ is a natural-isomorphism iff $\eta _A\colon \functor{f}(A)\rightarrow \functor{g}(A)$ is an isomorphism for all $A\in \Obj (\cat{C})$.
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

We now finally return to the example of primary interest.
\begin{exm}{$V\rightarrow [V^{\T}]^{\T}$}{}
	Let $\K$ be a cring and consider the linear-transformation $\eta _V\colon V\rightarrow [V^{\T}]^{\T}$ defined by $v\mapsto \braket{\blankdot ,v}$.
	\begin{exr}[breakable=false]{}{}
		Show that this is a natural-transformation.
	\end{exr}
	Thus, by \cref{DualityOfTheDual}, it is in fact a natural-isomorphism in the category of finite-dimensional vector spaces.
\end{exm}

\cleardoublepage
\chapter{Results from ring theory}

Linear algebra is the study of vector spaces, which themselves are just $\F$-modules for $\F$ a division ring.  Thus, it should be expected, and is in fact the case, that some basic results about ring theory are used throughout the main text.  In order to avoid interrupting the flow with long tangents on not linear algebra, we have relegated most of these results to this appendix.

\section{Prerequisites}

In this section, we present a miscellany of results used in the sections to come.  You might consider skipping it for now and coming back as the results are referenced.

\begin{dfn}{Kernel (of a ring homomorphism)}{}
	Let $\phi \colon R\rightarrow S$ be a ring homomorphism.  Then, the \term{kernel}\index{Kernel (of a ring homomorphism)}, $\Ker (\phi )$\index[notation]{$\Ker (\phi )$}, is defined by
	\begin{equation}
		\Ker (\phi )\ceqq \{ x\in R:\phi (x)=0\} .
	\end{equation}
\end{dfn}
\begin{prp}{}{prpC.1.2}
	Let $\phi \colon R\rightarrow S$ be a ring homomorphism.  Then, $\Ker (\phi )\subseteq R$ is an ideal.
	\begin{rmk}
		Warning:  Unlike the case of modules, it is \emph{not} the case in general that $\Ima (\phi )\subseteq S$ is an ideal.  That said, see the following result.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{}{}
	Let $\phi \colon R\rightarrow S$ be a ring homomorphism.  Then,
	\begin{enumerate}
		\item the preimage under $\phi$ of a subring of $S$ is a subring of $\phi$;
		\item the preimage under $\phi$ of an ideal of $S$ is an ideal of $R$;
		\item the image under $\phi$ of a subring of $R$ is a subring of $S$; and
		\item the image under $\phi$ of every ideal of $R$ is an ideal of $S$ iff $\phi$ is surjective.Then, $\Ima (\phi )\subseteq S$ is a subring.
	\end{enumerate}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exr}{}{}
	Give an example of a ring homomorphism $\phi \colon R\rightarrow S$ for which $\phi (S)\subseteq R$ is \emph{not} an ideal.
\end{exr}

By and large, we're going to only be working with \emph{two-sided} ideals.  A ``two-sided ideal'' is just an ideal in the sense you (hopefully) learned back in \cref{IdealsAndQuotientRngs}.  The ``two-sided'' is used for emphasis to distinguish between \emph{left} and \emph{right} sided ideals.
\begin{dfn}{Left and right ideals}{LeftAndRightIdeals}
	Let $R$ be a ring and let $I\subseteq R$.  Then, $I$ is a \term{left (resp.~right) ideal}\index{Left ideal}\index{Right ideal} iff it is a submodule of $R$ regarded as a left (resp.~right) module over itself.
	\begin{rmk}
		See \cref{exm1.1.18x} to recall exactly what we are referring to when we regard a ring as a module over itself.  It's not hard---for example, when we say that ``$\R$ is a one-dimensional vector space over itself'' we're thinking of $\R$ as a module over itself.
	\end{rmk}
\end{dfn}
\begin{prp}{}{}
	Let $R$ be a ring and let $I\subseteq R$.
	\begin{enumerate}
		\item $I$ is a left ideal iff (i) it is a subrng and (ii) $r\in R$ and $i\in I$ implies that $r\cdot i\in I$.
		\item $I$ is a right ideal iff (ii) it is a subrng and (ii) $r\in R$ and $i\in I$ implies that $i\cdot r\in I$.
	\end{enumerate}
	\begin{rmk}
		Warning:  Note that it is not a subr\emph{i}ng!  Indeed, you should check that ideals (of any type) contain the multiplicative identity iff they are all of $R$.
	\end{rmk}
	\begin{rmk}
		Note that this is an exact analogue of what we say in the two-sided case (\cref{exrA.4.45})---in addition to being a subrng, you have respectively left and right ``absorption'' properties.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  This is no more difficult than just applying the submodule criterion (\cref{prp1.2.2}) to the left (resp.~right) module $R$.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{prp}

\begin{prp}{Product of ideals}{}
	Let $R$ be a ring and let $I,J\subseteq R$ be ideals.  Then, the \term{product}\index{Product of ideals} of $I$ and $J$, $IJ$, defined by
	\begin{equation}
	IJ\ceqq \left\{ \sum _{k=1}^mx_ky_k:m\in \N ,x_k\in I,y_k\in J\right\} ,
	\end{equation}
	is an ideal.
	\begin{rmk}
		For $n\in \N$, we define
		\begin{equation}
		I^n\ceqq \underbrace{I\cdots I}_n
		\end{equation}
		(as well as $I^0\ceqq R$).  Note how this is \emph{not} the same as $\left\{ \sum _{k=1}^mx_k^n:m\in \N ,x_k\in I\right\}$, which is in general smaller.
	\end{rmk}
	\begin{rmk}
		For noncommutative rings, there are also notions of left and right ideals,\footnote{Well, the notions exist in the commutative case as well, but there all three notions coincide.} in which case one might refer to what we have been simply calling ``ideals'' as ``two-sided ideals'' for emphasis.  While certainly some of the things we do could be adapted for the one-sided versions, throughout, ``ideal'' should be interpreted as meaning ``two-sided ideal''.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{Generated ideal}{GeneratedIdeal}
	Let $\K$ be a ring and let $S\subseteq \K$ be a subset.  Then, there is a unique ideal $\Ide{S}$\index[notation]{$\Ide{S}$}, the ideal \term{generated}\index{Generated ideal} by $S$, such that
	\begin{enumerate}
		\item $S\subseteq \Ide{S}$; and
		\item if $I$ is any other ideal containing $S$, it follows that $S\subseteq \Ide{S}$.
	\end{enumerate}
	Furthermore, explicitly,
	\begin{equation}
	\Ide{S}=\left\{ \sum _{s\in S}x_ssy_s:x_s,y_s\in \K \right\} \eqqc \K S\K .
	\end{equation}
	\begin{rmk}
		If $S\eqqc \{ s_1,\ldots ,s_m\}$ is finite, people typically write
		\begin{equation}
		\Ide{s_1,\ldots ,s_m}\ceqq \Ide (S).
		\end{equation}\index[notation]{$\Ide{s_1,\ldots ,s_m}$}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

\begin{thm}{Krull's Theorem}{KrullsTheorem}\index{Krull's Theorem}
	Let $R$ be a ring and let $I\subseteq R$ be a proper ideal.  Then, there is a maximal ideal $M\subseteq R$ containing $I$.
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  \nameref{ZornsLemma}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{thm}
\begin{dfn}{Principal ideal ring}{PrincipalIdealRing}
	Let $R$ be a ring.  Then, $R$ is a \term{left (resp.~right, resp.~two-sided) principal ideal ring}\index{Principal ideal ring} iff every nonzero left (resp.~right, resp.~two-sided) ideal is generated by a single element.
	\begin{rmk}
		It is much more common to see this condition stated only in the case where it is already assumed that $R$ is an integral domain, in which case people say \term{principal ideal domain}\index{Principla ideal domain instead}.  Without question, this term is far more common than ``principal ideal ring'', though of course it would be inappropriate if $R$ isn't actually an integral domain.
	\end{rmk}
	\begin{rmk}
		``Principal ideal domain'' is often abbreviated to \term{PID}\index{PID}.  It thus seems appropriate to abbreviate ``principal ideal ring'' to \term{PIR}\index{PIR}.
	\end{rmk}
	\begin{rmk}
		The primary example we have in mind are polynomial rings---see \cref{PolynomialsArePIDs}.
	\end{rmk}
	\begin{rmk}
		The zero ideal is generated by the empty-set, which of course doesn't have a single element, it has no elements---that's what ``empty'' means.
	\end{rmk}
\end{dfn}
\begin{prp}{}{}
	Let $R$ be a ring.  Then, if $R$ is a left or right PIR, then $R$ is a two-sided PIR.
	\begin{proof}
		Suppose that $R$ is a left or right PIR.  Without loss of generality, suppose that $R$ is a left PIR.  Let $I\subseteq R$ be a two-sided ideal.  Then, $I$ is in particular a left ideal, and so as $R$ is a left PIR, there is some $x\in R$ such that $I=Rx$.  However, as $I$ is two-sided, $RxR=IR=I$, and hence $x$ generates $I$ as a two-sided ideal as well.
	\end{proof}
\end{prp}

\section{Ideals and their quotients}

The primary goal of this section is to prove the ``dictionary'' \cref{thmC.1.9} that establishes a correspondence between properties of ideals $I$ and properties of their quotients $R/I$.  We begin with a discussion of relatively intuitive properties one might like a ring to have (one of which we have already encountered).  For reasons that won't be particularly apparent to us given our limited discussion, these properties, while more intuitive, are arguably not the `correct' notion in the noncommutative setting.  We thus introduce variants of these properties more suited to the noncommutative setting and check that they agree with the first definition if the ring is commutative.  To state these definitions, however, we must first define the corresponding properties of ideals.  Finally, we prove the dictionary itself.

\subsection{Some properties of rings}\label{sbsC.1.1}

As mentioned previously, all of the definitions given here come with the tiny caveat that they are arguably not the best definition, at least for noncommutative rings.

We have already defined what it means for a ring to be a \emph{division ring} (\cref{DivisionRing}) and to be \emph{integral} (\cref{dfnA.1.69}).  We reproduce the definitions here both for convenience and to stress it's relationship to the properties of reduced (\cref{ReducedRing}) and hyper-connected (\cref{HyperConnectedRing}).
\begin{dfn}{Division ring}{}
	Let $R$ be a ring.  Then, $R$ is a \term{division ring} iff every element of $R$ has a multiplicative inverse.
\end{dfn}
\begin{dfn}{Integral ring}{IntegralRing}
	Let $R$ be a ring.  Then, $R$ is \term{integral} iff $xy=0$ implies $x=0$ or $y=0$.
\end{dfn}
\begin{dfn}{Reduced ring}{ReducedRing}
	Let $R$ be a ring.  Then, $R$ is \term{reduced}\index{Reduced ring} iff $x^m=0$, $m\in \N$, implies $x=0$.
	\begin{rmk}
		In other words, $R$ is reduced iff the only nilpotent element is $0$.
	\end{rmk}
\end{dfn}
\begin{dfn}{Hyper-connected ring}{HyperConnectedRing}
	Let $R$ be a ring.  Then, $R$ is \term{hyper-connected}\index{Hyper-connected ring} iff whenever $xy=0$ and one of these elements is nonzero it follows that the other element is nilpotent.
	\begin{rmk}
		The name comes from the fact that the Zariski topology on the spectrum is hyper-connected\footnote{Or \emph{irreducible}, in algebraic geometer parlance.} iff $R$ is hyper-connected (for $R$ commutative).
	\end{rmk}
\end{dfn}
\begin{prp}{}{prpC.1.6}
	Let $R$ be a ring.
	\begin{enumerate}
		\item \label{prpC.1.6(i)}If $R$ is a division ring, then it is integral.
		\item \label{prpC.1.6(ii)}$R$ is integral iff it is reduced and hyper-connected.
	\end{enumerate}
	\begin{rmk}
		One can interpret this as saying that a ring can fail to be integral in one of two ways:  it can have nonzero nilpotents or it can fail to be hyper-connected.  The definition of hyper-connected given above is superficially quite unintuitive.  I tend to think of it as ``That property you need together with reducedness to guarantee the ring be integral.''.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

\subsection{Some properties of ideals}

\begin{dfn}{Maximal ideal}{MaximalIdeal}
	Let $R$ be a ring and let $I\subseteq R$ be a proper ideal.  Then, $I$ is \term{maximal}\index{Maximal ideal} iff $I$ is maximal among all proper ideals with respect to the partial-order of inclusion.
\end{dfn}
\begin{dfn}{Prime ideal}{PrimeIdeal}
	Let $R$ be a ring and let $I\subseteq R$ be a proper ideal.  Then, $I$ is \term{prime}\index{Prime ideal} iff $JK\subseteq I$, $J,K\subseteq R$ ideals, implies $J\subseteq I$ or $K\subseteq I$.
	\begin{rmk}
		The name comes from the fact that $\Z p\subseteq \Z$ is a prime ideal iff $p\in \Z$ is prime in the classical sense of the word ($p=0$ being an exception---$0\subseteq \Z$ is a prime ideal but $0\in \Z$ is not usually considered a prime integer).
	\end{rmk}
	\begin{rmk}
		You can remember that $R$ itself is excluded from being prime because it is excluded from being maximal (if it weren't, then it would be the only maximal ideal).  Indeed, $I=R$ is excluded from this entire list of classes of ideals, and so making the exclusion in all definitions (instead of just one) is convenient.
	\end{rmk}
\end{dfn}
\begin{dfn}{Radical ideal}{RadicalIdeal}
	Let $R$ be a ring and let $I\subseteq R$ be a proper ideal.  Then, $I$ is \term{radical}\index{Radical ideal} iff $J^m\subseteq I$, $J\subseteq R$ an ideal and $m\in \N$, implies $J\subseteq I$.
	\begin{rmk}
		The name comes from the following result.
	\end{rmk}
	\begin{rmk}
		Warning:  Some authors instead refer to this concept as that of a \term{semiprime ideal}\index{Semiprime ideal}.  For such authors, an ideal $I$ is ``radical'' iff $x^m\in I$ implies $x\in I$.  The two definitions agree in case $R$ is commutative, and as far as I can tell, the definition we have given above is the one that is better-behaved in the noncommutative setting, and so it is the one we make use of.\footnote{Of course, we could have just called it ``semiprime'', but I prefer ``radical'' as this is the more common of the two terms, at least outside of a strictly noncommutative setting.}
	\end{rmk}
\end{dfn}
\begin{thm}{Radical of an ideal}{}
	Let $R$ be a ring and let $I\subseteq R$ be a proper ideal.  Then, there is a unique radical ideal, the \term{radical}\index{Radical of an ideal}, $\sqrt{I}$, such that
	\begin{enumerate}
		\item $I\subseteq \sqrt{I}$ and
		\item if $J\subseteq R$ is another radical ideal that contains $I$, then $\sqrt{I}\subseteq J$.
	\end{enumerate}
	Furthermore, explicitly, we have
	\begin{equation}
		\sqrt{I}=\bigcap _{\substack{P\subseteq R\text{ prime} \\ I\subseteq P}}P.
	\end{equation}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  See \cite[Theorem 10.7]{Lam}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{thm}
\begin{dfn}{Primary ideal}{PrimaryIdeal}
	Let $R$ be a ring and let $I\subseteq R$ be a proper ideal.  Then, $I$ is \term{primary} iff $JK\subseteq I$, $J,K\subseteq R$ ideals, implies that either one of $J,K$ is contained in $I$ or one of $J,K$ is contained in $\sqrt{I}$.
\end{dfn}
\begin{prp}{}{prpC.1.4}
	Let $R$ be a ring and let $I\subseteq R$ be an ideal.
	\begin{enumerate}
		\item \label{prpC.1.4(i)}If $I$ is maximal, then it is prime.
		\item \label{prpC.1.4(ii)}$I$ is prime iff it is radical and primary.
	\end{enumerate}
	\begin{proof}
		\cref{prpC.1.4(i)} Suppose that $I$ is maximal.  Let $J,K\subseteq R$ be ideals such that $JK\subseteq I$.  If $J\subseteq I$, we're done.  Otherwise, there is some $j\in J\setminus I$.  Then, $RjR+I$ is an ideal that properly contains $I$, and so by maximality, $R=RjR+I$.  In particular, there are $a,b\in R$ and $i\in I$ such that $1=ajb+i$.  Let $k\in K$.  Then, $k=ajbk+ik\in JK+I\subseteq I+I\subseteq I$.  Thus, $K\subseteq I$, and so $I$ is prime.
		
		\blni
		\cref{prpC.1.4(ii)} $(\Rightarrow )$ Suppose that $I$ is prime.  We first check that $I$ is radical.  So, let $J\subseteq R$ be an ideal and suppose that $J^m\subseteq I$ for some $m\in \N$.  We can't have $m=0$ as $I$ is proper.  If $m=1$, we're done.  Otherwise, we can write
		\begin{equation}
			I\supseteq J^m\ceqq JJ^{m-1},
		\end{equation}
		whence we deduce that either $J\subseteq I$ or $J^{m-1}\subseteq I$.  In the former case, we are done.  In the latter case, we have $J^{m-1}\subseteq I$.  Continuing inductively, we eventually find that $J\subseteq I$, and so $I$ is radical.  We now check that $I$ is primary.  So, let $J,K\subseteq R$ be ideals and suppose that $JK\subseteq I$.  If $J\subseteq I$, we're done.  Otherwise, $K\subseteq I\subseteq \sqrt{I}$, and we are again done.

		$(\Leftarrow )$ Suppose that $I$ is radical and primary.  Let $J,K\subseteq R$ be ideals and suppose that $JK\subseteq I$.  If one of $J,K$ is contained in $I$, we're done.  Otherwise, as $I$ is primary, one of $J,K$ is contained in $\sqrt{I}$.  Without loss of generality, suppose that $J\subseteq \sqrt{I}$.  However, as $I$ is radical, $\sqrt{I}=I$, and hence $J\subseteq I$.  Thus, $I$ is prime.
	\end{proof}
\end{prp}

\begin{thm}{}{thmC.1.6}
	Let $R$ be a ring and let $I$ be an ideal.  Then, $R\supseteq J\mapsto J/I\subseteq R/I$ is a bijection from the set of ideals of $R$ that contain $I$ and the set of ideals of $R/I$.  Furthermore,
	\begin{enumerate}
		\item \label{thmC.1.6(i)}$J$ is maximal iff $J/I$ is maximal;
		\item \label{thmC.1.6(ii)}$J$ is prime iff $J/I$ is prime;
		\item \label{thmC.1.6(iii)}$J$ is radical iff $J/I$ is radical; and
		\item \label{thmC.1.6(iv)}$J$ is primary iff $J/I$ is primary.
	\end{enumerate}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

\subsection{Their noncommutative variants}

Though we call them the ``noncommutative variants'', of course they make just as much sense in commutative rings.  However, for commutative rings, they coincide (\cref{thmC.1.8}) with the more intuitive concepts defined in \cref{sbsC.1.1}, and so in that context it's usually preferable to work with the previous definitions.
\begin{dfn}{Simple ring}{SimpleRing}
	Let $R$ be a nonzero ring.  Then, $R$ is \term{simple}\index{Simple} iff the only ideals of $R$ are $0$ and $R$ itself.
	\begin{rmk}
		Warning:  This definition is exactly analogous to the definition of a simple module (\cref{SimpleModule}).  \emph{However}, there is a notion of semisimple ring and it is \emph{not} analogous to the definition of a semisimple module (\cref{SemisimpleModule}).  Because mathematicians are literally cancer when it comes to developing systematic terminology, the term ``semisimple ring'' refers to a ring that is semisimple when regarded as a left (equivalently, right) module over itself.
	\end{rmk}
\end{dfn}
\begin{dfn}{Prime ring}{PrimeRing}
	Let $R$ be a nonzero ring.  Then, $R$ is \term{prime}\index{Prime ring} iff $IJ=0$, $I,J\subseteq R$ ideals, implies $I=0$ or $J=0$.
	\begin{rmk}
		Note that this is just the statement that the zero ideal is prime.
	\end{rmk}
\end{dfn}
\begin{dfn}{Radical ring}{RadicalRing}
	Let $R$ be a nonzero ring.  Then, $R$ is \term{radical}\index{Radical ring} iff $I^m=0$, $I\subseteq R$ an ideal and $m\in \N$, implies $I=0$.
	\begin{rmk}
		Note that this is just the statement that the zero ideal is radical.
	\end{rmk}
	\begin{rmk}
		Warning:  Some authors refer to this concept as that of a \term{semiprime ring}\index{Semiprime ring}, essentially because such authors will instead use the term ``semiprime ideal'' in place of ``radical ideal''---see the remark in \cref{RadicalIdeal}.
	\end{rmk}
\end{dfn}
\begin{dfn}{Primary ring}{PrimaryRing}
	Let $R$ be a nonzero ring.  Then, $R$ is \term{primary}\index{Primary ring} iff $IJ=0$, $I,J\subseteq R$ ideals, implies that either one of $I,J$ is $0$ or one of $I,J$ is contained in $\sqrt{0}$.
	\begin{rmk}
		Note that this is just the statement that the zero ideal is primary.
	\end{rmk}
	\begin{rmk}
		Warning:  This terminology seems nonstandard, and in fact, upon looking it up, it seems to conflict with a usage of the term I had never seen before.  Oh well.
	\end{rmk}
\end{dfn}
\begin{prp}{}{prpC.1.7}
	Let $R$ be a ring.
	\begin{enumerate}
		\item \label{prpC.1.7(i)}If $R$ is simple, then it is prime.
		\item \label{prpC.1.7(ii)}$R$ is prime iff it is radical and primary.
	\end{enumerate}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
We now check that this definitions do indeed agree with the previous ones in the case that $R$ is commutative.
\begin{thm}{}{thmC.1.8}
	Let $\K$ be a cring.
	\begin{enumerate}
		\item \label{thmC.1.8(i)}$\K$ is simple iff it is a division ring.\footnote{Or, equivalently, a field.}
		\item \label{thmC.1.8(ii)}$\K$ is prime iff it is integral.
		\item \label{thmC.1.8(iii)}$\K$ is radical iff it is reduced.
		\item \label{thmC.1.8(iv)}$\K$ is primary iff it hyper-connected.
	\end{enumerate}
	\begin{proof}
		\cref{thmC.1.8(i)} $(\Rightarrow )$ Suppose that $\K$ is simple.  Let $x\in \K$ be nonzero.  $\K x\subseteq \K$ is then a nonzero ideal, and hence $\K x=\K$.  Thus, there is some $y\in \K$ such that $yx=1$.
		
		$(\Leftarrow )$ Suppose that $\K$ is a division ring.  Let $I\subseteq \K$ be a nonzero ideal.  Let $i\in I$ be nonzero.  $1=ii^{-1}\in I$, and so $x=1x\in I$ for all $x\in \K$, and hence $I=\K$.
		
		\blni
		\cref{thmC.1.8(iii)} $(\Rightarrow )$ Suppose that $\K$ is radical.  Let $x\in \K$ and suppose that $x^m=0$ for some $m\in \N$.  It follows that $(\K x)^m=0$, and so $\K x=0$, and in particular $x=0$.
		
		$(\Leftarrow )$ Suppose that $\K$ is reduced.  Let $I\subseteq \K$ be an ideal and suppose that $I^m=0$ for some $m\in \N$.  Let $i\in I$.  Them, $i^m=0$ as $i^m\in I^m$, and so $i=0$.
		
		\blni
		\cref{thmC.1.8(iv)} $(\Rightarrow )$ Suppose that $\K$ is primary.  Let $x,y\in \K$ and suppose that $xy=0$.  It follows that $(\K x)(\K y)=0$.  If $(\K x)=0$, then in particular $x=0$, and we're done.  Otherwise, we must have that $(\K y)^m=0$ for some $m\in \N$, and again, in particular, $y^m=0$.
		
		$(\Leftarrow )$ Suppose that $\K$ is hyper-connected.  Let $I,J\subseteq \K$ be ideals and suppose that $IJ=0$.  If $I=0$, we're done, so suppose this is not the case.  Then, there is some nonzero $i\in I$.  For every $j\in J$, we have that $ij=0$, and so as $i\neq 0$, there is some $m_j\in \N$ such that $j^{m_j}=0$.  Thus, $j\in \sqrt{0}$, and hence $J\subseteq \sqrt{0}$.
		
		\blni
		\cref{thmC.1.8(ii)} $\K$ is prime iff it is radical and primary (\cref{prpC.1.7}) iff it is reduced and hyper-connected (by the previous parts) iff it is integral (\cref{prpC.1.6}).
	\end{proof}
\end{thm}

\subsection{The dictionary}

We now finish with the ultimate goal of this section.
\begin{thm}{}{thmC.1.9}
	Let $R$ be a ring and let $I\subseteq R$ be an ideal.
	\begin{enumerate}
		\item \label{thmC.1.9(i)}$I$ is maximal iff $R/I$ is simple.
		\item \label{thmC.1.9(ii)}$I$ is prime iff $R/I$ is prime
		\item \label{thmC.1.9(iii)}$I$ is radical iff $R/I$ is radical.
		\item \label{thmC.1.9(iv)}$I$ is primary iff $R/I$ is primary.
	\end{enumerate}
	\begin{rmk}
		In particular, if $\K$ is commutative, by \cref{thmC.1.8}, we have
		\begin{enumerate}
			\item $I$ is maximal iff $\K I$ is a field.
			\item $I$ is prime iff $\K/ I$ is integral.
			\item $I$ is radical iff $\K /I$ is reduced.
			\item $I$ is primary iff $\K /I$ is hyper-connected.
		\end{enumerate}
	\end{rmk}
	\begin{proof}
		\cref{thmC.1.9(i)} $(\Rightarrow )$ Suppose that $I$ is maximal.  Every ideal of $R/I$ is of the form $J/I$ for an ideal $J\subseteq I$ containing $I$.  As $I$ is maximal, there are only two such ideals, $I$ and $R$.  Hence, the only ideals of $R/I$ are $I/I=0$ and $R/I$ itself.  Thus, $R/I$ is simple.
		
		$(\Leftarrow )$ Suppose that $R/I$ is simple.  Let $J\supseteq I$ be a proper ideal.  $J/I\subseteq R/I$ is an ideal, and so as $R/I$ is simple, either $J/I=0$ or $J/I=R/I$.  The latter cannot happen as $J$ is proper, and so $J/I=0$, that is, $J=I$.  Hence, $I$ is maximal.
		
		\blni
		\cref{thmC.1.9(ii)} $(\Rightarrow )$ Suppose that $I$ is prime.  Let $J/I,K/I\subseteq R/I$ be ideals such that $(J/I)(K/I)=0$.  It follows that $JK\subseteq I$, and hence as $I$ is prime, without loss of generality, $J\subseteq I$, so that $J/I=0$.
		
		$(\Leftarrow )$ Suppose that $R/I$ is prime.  Let $J,K\subseteq R$ be ideals such that $JK\subseteq I$.  It follows that $(J/I)(K/I)=0$, and hence as $R/I$ is prime, without loss of generality, $J/I=0$, so that $J\subseteq I$.
		
		\blni
		\cref{thmC.1.9(iii)} $(\Rightarrow )$ Suppose that $I$ is radical.  Let $J/I\subseteq R/I$ be an ideal such that $(J/I)^m=0$ for some $m\in \N$.  As $(J/I)^m=J^m/I$, it follows that $J^m=I$, and hence as $I$ is radical, $J\subseteq I$, so that $J/I=0$.
		
		$(\Leftarrow )$ Suppose that $R/I$ is radical.  Let $J\subseteq R$ be an ideal such that $J^m\subseteq I$ for some $m\in \N$.  As $J^m/I=(J/I)^m$, it follows that $(J/I)^m=0$, and hence as $R/I$ is radical, $J/I=0$, so that $J\subseteq I$.
		
		\blni
		\cref{thmC.1.9(iv)} $(\Rightarrow )$ Suppose that $I$ is primary.  Let $J/I,K/I\subseteq R/I$ be ideals such that $(J/I)(K/I)=0$.  It follows that $JK\subseteq I$, and hence as $I$ is primary, either one of $J,K$ is contained in $I$ or one of $J,K$ is contained in $\sqrt{I}$.  In the former case, without loss of generality, suppose that $J\subseteq I$, in which case $J/I=0$ and we are done.  Otherwise, without loss of generality, $J\subseteq \sqrt{I}$, so that $J/I\subseteq \sqrt{0}$.
		
		$(\Leftarrow )$ Suppose that $R/I$ is primary.  Let $J,K\subseteq R$ be ideals such that $JK\subseteq I$.  It follows that $(J/I)(K/I)=0$, and hence as $R/I$ is primary, either one of $J/I,K/I$ is $0$ or one of $J/I,K/I$ is contained in $\sqrt{0}$.  In the former case, without loss of generality, suppose that $J/I=0$, in which case $J\subseteq I$ and we are done.  Otherwise, without loss of generality, $J/I\subseteq \sqrt{0}$, so that $J\subseteq \sqrt{I}$.
	\end{proof}
\end{thm}

\subsection{Summary}

While this might seem like a lot, I claim that it can all be organized quite nicely.  First of all, we have three things:  properties of ideals, the corresponding properties of their quotient rings, and the `simpler' properties these are equivalent to in the commutative case.   Secondly, we also have implications among these properties.  In the table below, the properties in the first row imply the properties in the second row, and the properties in the second row are equivalent to the combination of the corresponding properties in the third and fourth rows.
\begin{tabu} to \linewidth {X[r]|X[c]|X[l]}
	Ideal $I$ & Quotient $R/I$ & $R/I$ for $R$ comm. \\ \hline
	Maximal & Simple & Field \\
	Prime & Prime & Integral \\
	Radical & Radical & Reduced \\
	Primary & Primary & Hyper-connected
\end{tabu}

\noindent
To clarify, an ideal $I$ has the property in the left-hand column iff $R/I$ has the property in the middle column.  Likewise, for crings, the properties in the middle column are equivalent to the properties in the right-hand column.

We also covered the result (\cref{thmC.1.6}) that states that ideals in $R/I$ correspond exactly to ideals in $R$ which contain $I$, and that all the properties discussed are preserved in both directions by this correspondence.

\section{The integral closure}

Let's take a step back for a moment and forget we know pretty much everything we know about mathematics.  In fact, let's temporarily forget everything except the natural numbers, $\N$.  So, we have the natural numbers in hand, and we write down an equation $x+m=n$, and then we try to solve it (because that's what mathematicians do, yeah?).  Sometimes we can:  if $x+3=5$, then $x=2$.  But eventually we run into a problem:  if $x+3=1$, what should $x$ be?

Uh oh.  For a moment there, you think you just broke math.  But now I come along, the clever man that I am, and I tell you about this super cool idea called the ``integers''.  Basically, you can solve all equations like this just by taking $\N$ and `adding in' all the solutions to the equations you can't solve.  Want to solve $x+5=0$ you say?  Great, I present unto you $-5$.  ``What's $-5$ you ask?''.  ``It's the solution to the equation $x+5=0$'', I reply.  You now have the integers, $\Z$.

It's that easy.  Want to solve an equation?  Cheat.  Pretend you have a solution, give it a name, add it to your set, et voilà:  you can now solve your equation.

Life goes on, things are good, you can solve your equations.  But then one day someone decides to ask this super insanely difficult question:  ``If $2x=1$, what is $x$?''.  You freak out, thinking you broke math again, but then recall my words of wisdom.  And so your mind brings $\frac{1}{2}$ into existence, the solution to the equation $2x=1$.  After a bit more thought, you realize you can perform similar feats of wizardry and solve all equations of the form $mx=n$.  You now have the rationals, $\Q$.

And now something mysterious happens.  Something something limits.  Something something least-upper bound property.  Something something \textellipsis .  You now have the real numbers, $\R$.\footnote{Passing from $\Q$ to $\R$ is about solving a different sort of `problem' that $\Q$ has that doesn't really have much to do with solving equations.}

Again, life is going well, until one day a heretic decides to ask the question ``If $x^2+1=0$, what is $x$?''.  Your fellow mathematicians call this heretic out on his\textellipsis heresy\textellipsis , but you, recalling my wisdom from days long past, realize that you \emph{can} solve such an equation:  all you have to do is make up the solution!

And so, you take $\R$, add this new solution to the mix, which you have quite creatively decided to call ``$\im$'', to obtain a new `number system'.  You now have the complex numbers, $\C$.

But something of a miracle has happened.  You set out to solve a single equation, $x^2+1=0$, but by adding \emph{just a single element}, you can now solve all polynomial equations!  In fact, you can even solve polynomial equations with coefficients in this weird new number system you just cooked up.

\horizontalrule

This passage from $\R$ to $\C$ is one we would like to generalize in this section.  Given a field $\F$, we would like to be able to solve polynomial equations in $\F$.  Of course, you can't always do that, and so the plan is to `enlarge' the field $\F$ to a new field $\A$ which contains all the solutions you needed.\footnote{In general, doing this will be quite a bit more complicated than going from $\R$ to $\C$.  That really was something of a miracle---it turns out that those fields which you can obtain an ``algebraic closure'' by adding just a single new element in this way have a name:  they are the \emph{real-closed fields}.}  This will be the \emph{algebraic closure} of $\A$.

It turns out however, that, in full generality (so for rings and not just fields), there is something that is better-behaved, but yet agrees with the algebraic closure in the case the ring happens to be a field.  This is the \emph{integral closure}.  The integral closure is the thing you obtain only by adding in roots to \emph{monic} polynomials, that is, polynomials whose leading coefficient is $1$.

We investigate both.  A priori, the algebraic closure is the one we'd be interested in (so we can solve \emph{all} polynomial equations, not just the monic ones), but a further investigation will reveal that the integral closure is the one you probably want to work with in general.  That's not a terribly big deal for us, however, as, like I said, the two notions wind up agreeing in the primary case of interest.

In any case, let us begin.

\subsection{Associative algebras}

Suppose we start with a field $\F$ which we enlarge to a field $\A$.  $\A$ is of course a ring, but it also has the structure of a vector space:  you can scale elements of $\A$ by elements of $\F$.  This gives $\A$ the structure of what is called an \emph{associative algebra}.

Intuitively, an associative algebra is like a module\footnote{Actually, a bimodule (\cref{KLBimodule}).} where you are allowed to ``multiply'' the elements of the module.  Equivalently, you might think of an associative algebra as a ring where you can scale elements.  That is, an associative algebra is something with both the structure of a bimodule and a ring for which the two structures are mutually compatible.
\begin{dfn}{$\K$-algebra}{KAlgebra}
	Let $\K$ be a ring.  Then, a \term{$\K$-algebra}\index{$\K$-algebra} is
	\begin{data}
		\item a ring $\coord{A,+,0,-,\mathlarger{\cdot},1}$; together with
		\item functions $\cdot _L\colon \K \times A\rightarrow A$ and $\cdot _R\colon A\times \K \rightarrow A$;
	\end{data}
	such that
	\begin{enumerate}
		\item \label{KAlgebra(i)}$\coord{A,+,0,-,\K ,\cdot _L,\cdot _R}$ is a $\K$-$\K$-bimodule;
		\item \label{KAlgebra(ii)}
		\begin{equation}
			\alpha \cdot _L(a_1\mathlarger{\cdot}a_2)=(\alpha \cdot _La_1)\mathlarger{\cdot}a_2;
		\end{equation}
		\item \label{KAlgebra(iii)}
		\begin{equation}
			(a_1\mathlarger{\cdot}a_2)\cdot _R\alpha =a_1\mathlarger{\cdot}(a_2\cdot _R\alpha );
		\end{equation}
		\item \label{KAlgebra(iiix)}
		\begin{equation}
			(a_1\cdot _R\alpha )\mathlarger{\cdot}a_2=a_1\mathlarger{\cdot}(\alpha \cdot _La_2)
		\end{equation}
		and
		\item \label{KAlgebra(iv)}
		\begin{equation}
			\alpha \cdot _La=a\cdot _R\alpha
		\end{equation}
		if $a\in A$ is central;
	\end{enumerate}
	for all $a_1,a_2\in A$ and $\alpha \in \K$.
	\begin{rmk}
		Essentially what these axioms state is (i) ``associativity'' holds between elements of $\K$ and elements of $A$,\footnote{Three axioms needed to deal with the three cases of the scalar appearing on the left, on the right, or in the middle.  Note that the case of two scalars and one element of $A$ is part of the definition of a bimodule.}, and (ii) central elements commute with scalars.
	\end{rmk}
	\begin{rmk}
		An \term{associative algebra}\index{Associative algebra} is a $\K$-algebra for some $\K$.  This term is thus used when you don't want to have to specify the ground ring.  The ``associative'' here refers to the associativity of the ring multiplication.  If you drop this hypothesis (in which case the definition becomes a bit more cumbersome to state because $\mathlarger{\cdot}$ is then no longer a ring multiplication), you obtain what is simply an \term{algebra}.  For example, Lie algebras are algebras that are generally not associative.  Thus, the term ``algebra'' by itself should be avoided if one really means ``associative algebra''.  On the other hand, if we say ``$\K$-algebra'', for us it is implicit that the algebra is associative.
	\end{rmk}
	\begin{rmk}
		The reason we require the structure of a bimodule (instead of just a left or right module) is motivated by the fact that doing things this way gives us another standard equivalent definition---see \cref{thmC.1.5}.
	\end{rmk}
\end{dfn}
\begin{dfn}{$\K$-algebra homomorphism}{KAlgebraHomomorphism}
	Let $A$ and $B$ be $\K$-modules and let $\phi \colon A\rightarrow B$ be a function.  Then, $\phi$ is a \term{$\K$-algebra homomorphism} iff it both a ring homomorphism and $\K$-$\K$-linear.
\end{dfn}
\begin{exm}{The category of $\K$-algebras}{}
	Let $\K$ be a ring.  Then, the category of $\K$-algebras is the concrete category $\Alg{\K}$\index[notation]{$\Alg{\K}$}
	\begin{enumerate}
		\item whose collection of objects $\Obj (\Alg{\K})$ is the collection of all $\K$-algebras; and
		\item with morphism set $\Mor _{\Alg{\K}}(A,B)$ precisely the set of all $\K$-algebra homomorphisms from $A$ to $B$.
	\end{enumerate}
	\begin{rmk}
		Just as we have the category of left $\K$-algebras $\Alg{\K}$, we also have that category of right $\K$-algebras $\Alg*{\K}$\index[notation]{$\Alg*{\K}$} defined similarly.
	\end{rmk}
\end{exm}
While, in my opinion anyways, ``ring in which you can scale elements'' is by far the more intuitive way to think about the concept, there is an equivalent formulation which some authors take as the definition.
\begin{thm}{}{thmC.1.5}
	Let $\K$ be a ring.
	\begin{enumerate}
		\item \label{thmC.1.5(i)}Let $A$ be a ring and let $\phi \colon \K \rightarrow A$ be a ring homomorphism.  Then, $\coord{A,\cdot _{L,\phi},\cdot _{R,\phi}}$ is a $\K$-algebra, where $\cdot _{L,\phi}\colon \K \times A\rightarrow A$ and $\cdot _{R,\phi}\colon A\times \K \rightarrow A$ are defined by
		\begin{equation}
			\alpha \cdot _{L,\phi}a\ceqq \phi (\alpha )a\text{ and }a\cdot _{R,\phi}\alpha \ceqq a\phi (\alpha ).
		\end{equation}
		\item \label{thmC.1.5(ii)}Let $\coord{A,\cdot _L,\cdot _R}$ be a $\K$-$\K$-algebra.  Then, $\phi \colon \K \rightarrow A$, defined by
		\begin{equation}
			\alpha \cdot _L1\eqqc \phi _{\cdot _L,\cdot _R}(\alpha )\ceqq 1\cdot _R\alpha ,
		\end{equation}
		is a ring homomorphism.
	\end{enumerate}
	Furthermore, these two constructions are inverse to each other.
	\begin{rmk}
		Given a ring homomorphism $\phi \colon \K \rightarrow A$, $\phi$ will be referred to as the \term{structure morphism}\index{Structure morphism (of a $\K$-algebra)} of the corresponding $\K$-algebra structure on $A$.
	\end{rmk}
	\begin{rmk}
		This result says that we could have equivalently defined a $\K$-algebra to be a ring homomorphism $\K \rightarrow A$ into some ring $A$.  While I think this is an unintuitive way to think about things, it \emph{is} important.\footnote{Indeed, I used this characterization to determine what axioms I wanted to use for the definition of a $\K$-algebra (in the noncommutative case, there is single standard definition).}  In particular, you should note that ring homomorphisms allow you to define $\K$-algebras in this way.
	\end{rmk}
	\begin{rmk}
		In particular, if $\K$ and $\L$ are rings with $\K \subseteq \L$, then $\L$ obtains the structure of a $\K$-algebra via the inclusion map $\K \hookrightarrow \L$.
	\end{rmk}
	\begin{rmk}
		This result is \emph{roughly} analogous to our equivalent characterization of $R$-modules (\cref{prpD.1.3}).
	\end{rmk}
	\begin{proof}
		\cref{thmC.1.5(i)} For the sake of brevity, we shall simply write $\cdot \ceqq \cdot _{L,\phi}$, and by abuse of notation, $\cdot \ceqq \cdot _{R,\phi}$ as well.  We first check that this gives $A$ the structure of a left $\K$-module.  So, let $\alpha _1,\alpha _2\in \K$ and $a\in A$.  Then,
		\begin{equation}
			\begin{split}
			(\alpha _1\alpha _2)\cdot a & \ceqq \phi (\alpha _1\alpha _2)a=\phi (\alpha _1)(\phi (\alpha _2)a) \\
			& \eqqc \alpha _1\cdot (\alpha _2\cdot a).
			\end{split}
		\end{equation}
		We also have $1\cdot a\ceqq \phi (1)a=1a=a$.  As for the distributivity axioms of a $\K$-module, we have
		\begin{equation}
			\begin{split}
				(\alpha _1+\alpha _2)\cdot a & \ceqq \phi (\alpha _1+\alpha _2)a \\
				& =\left( \phi (\alpha _1)+\phi (\alpha _2)\right) a \\
				& =\phi (\alpha _1)a+\phi (\alpha _2)a \\
				& \eqqc \alpha _1\cdot a+\alpha _2\cdot a
			\end{split}
		\end{equation}
		and
		\begin{equation}
			\begin{split}
				\alpha \cdot (a_1+a_2) & \ceqq \phi (\alpha)(a_1+a_2)=\phi (\alpha )a_1+\phi (\alpha )a_2 \\
				& \eqqc \alpha \cdot a_1+\alpha \cdot a_2.
			\end{split}
		\end{equation}
		Thus, $\cdot$ does indeed give $A$ the structure of a left $\K$-module.  Similarly, it has the structure of a right $\K$-module.  Finally, to see that $A$ is in fact a $\K$-$\K$-bimodule, note that
		\begin{equation}
			(\alpha _1\cdot a)\cdot \alpha _2\ceqq \footnote{This uses the associativity of the ring multiplication.}\ceqq \phi (\alpha _1)a\phi (\alpha _2)\eqqc \alpha _1\cdot (a\cdot \alpha _2).
		\end{equation}
		
		We next check \cref{KAlgebra(ii)} of \cref{KAlgebra}.  So, let $\alpha \in \K$ and $a_1,a_2\in A$.  Then,
		\begin{equation}
			\alpha \cdot (a_1a_2)\ceqq \phi (\alpha )(a_1a_2)=(\phi (\alpha )a_1)a_2\eqqc (\alpha \cdot a_1)a_2
		\end{equation}
		\cref{KAlgebra(iii)} follows similarly.  As for \cref{KAlgebra(iiix)}, we see that
		\begin{equation}
			\begin{split}
			(a_1\cdot \alpha )a_2 & \ceqq (a_1\phi (\alpha ))a_2=\footnote{Because the ring multiplication is associative.}a_1(\phi (\alpha )a_2) \\
			& \eqqc a_1(\alpha \cdot a_2).
			\end{split}
		\end{equation}
		
		Finally, we check \cref{KAlgebra(iv)}.  So, let $\alpha _1,\alpha _2\in \K$ and let $a\in A$ be central.  Then,
		\begin{equation}
			\alpha \cdot a\ceqq \phi (\alpha )a=a\phi (\alpha )\ceqq a\cdot \alpha .
		\end{equation}
		
		\blni
		\cref{thmC.1.5(ii)} For the sake of brevity, we shall simply write $\phi \ceqq \phi _{\cdot _L,\cdot _R}$.  $\phi$ preserves addition because scaling distributes over addition in $\K$.  As for multiplication, let $\alpha ,\beta \in \K$.  Then,
		\begin{equation}
			\begin{split}
				\phi (\alpha \beta ) & \ceqq (\alpha \beta )\cdot 1=\alpha \cdot (\beta \cdot 1)\eqqc \alpha \cdot \phi (\beta ) \\
				& =\alpha \cdot (1\phi (\beta ))=(\alpha \cdot 1)\phi (\beta )\eqqc \phi (\alpha )\phi (\beta ).
			\end{split}
		\end{equation}
		And of course, $\phi (1)\ceqq 1\cdot 1=1$.
		
		\blni
		It remains to check that these constructions are inverse to one another.
		\begin{equation}
			\begin{split}
			\alpha \cdot _{L,\phi _{\cdot _L,\cdot _R}}a & \ceqq \phi _{\cdot _L,\cdot _R}(\alpha )a\ceqq (\alpha \cdot _L1)a \\
			& =\alpha \cdot _L(1a)=\alpha \cdot _La.
			\end{split}
		\end{equation}
		Similarly for $\cdot _R$.
		
		In the other direction,
		\begin{equation}
			\phi _{\cdot _{L,\phi},\cdot _{R,\phi}}(\alpha )\ceqq \alpha \cdot _{L,\phi}1\ceqq \phi (\alpha )1=\phi (\alpha ).
		\end{equation}
	\end{proof}
\end{thm}
\begin{dfn}{Extension}{Extension}
	Let $\K$ be a ring.  Then, an \term{extension}\index{Extension} of $\K$ is a $\K$-algebra $\L$ such that $\K \subseteq \L$ and whose structure morphism is given by the inclusion $\K \hookrightarrow \L$.
\end{dfn}
\begin{exm}{Rings are $\Z$-algebras}{}
	Let $R$ be a ring.
	\begin{exr}[breakable=false]{}{}
		Show that there is a \emph{unique} ring homomorphism $\Z \rightarrow R$.
	\end{exr}
	$R$ thus obtains the structure of a $\Z$-algebra using the previous result.  It turns out that scaling (on both the left and right) $r\in R$ by $m\in \Z ^+$ is nothing more than $r$ added to itself $m$ times.
	
	\begin{rmk}
		Just as commutative groups are `the same as' $\Z$-modules (\cref{exm1.1.22}), rings are `the same as' $\Z$-algebras.
	\end{rmk}
\end{exm}
\begin{exm}{}{}
	\begin{enumerate}
		\item $\R$ is an infinite-dimensional $\Q$-algebra.
		\item $\C$ is a $2$-dimensional $\R$-algebra.
		\item $\H$ is a $2$-dimensional $\C$-algebra and a $4$-dimensional $\R$-algebra. 
	\end{enumerate}
\end{exm}

Polynomials furnish another natural collection of $\K$-algebras.

\subsection{Polynomials}

\begin{prp}{Polynomial algebra}{PolynomialAlgebra}
	Let $\K$ be a ring.  Define $\cdot \colon \K ^{\infty}\times \K ^{\infty}\rightarrow \K ^{\infty}$ by
	\begin{equation}
		[a\cdot b]_k\ceqq \sum _{i+j=k}a_ib_j.
	\end{equation}
	Then, $\K [x]\ceqq \coord{\K ^{\infty},+,0,-,\cdot ,1}$, the \term{polynomial algebra}\index{Polynomial algebra (single variable)} in a single variable with coefficients in $\K$, is a $\K$-algebra, where addition and both left and right scaling are defined componentwise, $0$ is the sequence of all $0$s, and $1\ceqq \coord{1,0,0,\ldots }$.
	\begin{rmk}
		We suggestively write
		\begin{equation}
			\begin{multlined}
				a_0+a_1x+\cdots +a_{m-1}x^{m-1}+a_mx^m \\ \ceqq \coord{a_0,a_1,\ldots ,a_{m-1},a_m,0,0,\ldots }.
			\end{multlined}
		\end{equation}
		With this notation, addition, multiplication, and scaling all work how you think they would with $x$ commuting with everything.
	\end{rmk}
	\begin{rmk}
		Elements of $\K [x]$ are \term{polynomials}\index{Polynomial} (in a single variable with coefficients in $\K$).
	\end{rmk}
	\begin{rmk}
		Warning:  Elements in $\K [x]$ are \emph{not} functions---see \cref{sss1.1.1}.  That said, it is not uncommon to use notation suggestive of such.  For example, it is not uncommon to write $p(x)\in \K [x]$ for a polynomial.  In fact, the polynomial is just $p$, but it can be useful to write $p(x)$ for the polynomial just as one writes $f(x)$ to denote a function, even though technically the function is just $f$.
	\end{rmk}
	\begin{rmk}
		For $p(x)=a_0+\cdots +a_mx^m\in \K [x]$, $a_m\neq 0$, the \term{degree}\index{Degree (of a polynomial)} of $p$, $\deg (p)$\index[notation]{$\deg (p)$}, is defined by $\deg (p)\ceqq m$.   $a_m$ is the \term{leading coefficient}\index{Leading coefficient} of $p$.  $a_0$ is the \term{constant term}\index{Constant term} of $p$.  $p$ is \term{monic}\index{Monic} iff the leading coefficient is $1$.
	\end{rmk}
	\begin{rmk}
		We also make use of that $\K$-algebra $[x]\K$.  As a $\K$-algebra, $[x]\K$\index[notation]{$[x]\K$} is identical to $\K [x]$.  However, notationally, elements in $[x]\K$ are written as
		\begin{equation}
			a_0+xa_1+\cdots +x^{m-1}a_{m-1}+x^ma_m.
		\end{equation}
		This of course is just a matter of notation.  What actually \emph{does} change, however, are the corresponding evaluations maps---see \cref{PolynomialFunction}.
	\end{rmk}
	\begin{rmk}
		Recall (\cref{exm1.1.18}) that $\K ^{\infty}$ is the set of all $\K$-valued sequences that are eventually $0$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
One important fact is that you can ``divide'' polynomials with coefficients in a division ring.
\begin{thm}{Division Algorithm}{DivisionAlgorithm}\index{DivisionAlgorithm}
	Let $\F$ be a division ring and let $f,g\in \F [x]$ with $g\neq 0$.
	\begin{enumerate}
		\item There are unique $q,r\in \F [x]$ such that $f=qg+r$ and $\deg (r)<\deg (g)$.
		\item There are unique $q,r\in \F [x]$ such that $f=gq+r$ and $\deg (r)<\deg (g)$.
	\end{enumerate}
	\begin{rmk}
		$q$ is the \emph{quotient} and $r$ is the \emph{remainder}.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
As mentioned in the definition of the property (\cref{PrincipalIdealRing}), polynomial rings furnish examples of PIDs.
\begin{prp}{$\F [x]$-is a left and right PID}{PolynomialsArePIDs}
	Let $\F$ be a division ring.  Then, $\F [x]$ is a left and right PIR.
	\begin{rmk}
		This is certainly not true if $\F$ is not a division ring.  For example, the ideal $\Ide{x,y}$ generated by $x,y\in \left[ \C [x]\right] [y]$ is not generated by a single element.\footnote{Can you prove this?}
	\end{rmk}
	\begin{rmk}
		$\F [x]$ is of course integral, but we don't use the term principal ideal \emph{domain} because that suggests commutativity.
	\end{rmk}
	\begin{proof}
		We prove that $\F [x]$ is a left PIR.  That it is a right PIR is similar.  So, let $I\subseteq \F [x]$ be a nonzero left ideal.  Let $g\in I$ be nonzero of minimum degree.  Let $f\in I$.  By the \nameref{DivisionAlgorithm}, there are unique $q,r\in \F [x]$ such that $f=qg+r$ and $\deg (r)<\deg (g)$.  As $I$ is a left ideal, $qg\in I$, and hence $r=f-qg\in I$.  However, as $g$ was chosen to have minimum degree among nonzero elements of $I$ and $\deg (r)<\deg (g)$, this implies that $r=0$, that is, $f=qg\in \Ide{g}$.  Hence, $I\subseteq \Ide{g}$, and so $I=\Ide{g}$.
	\end{proof}
\end{prp}
\begin{exr}{}{}
	Is it true that $\F [x]$ a left and right PID implies that $\F$ is necessarily a division ring?
\end{exr}

Polynomial algebras have an important ``universal property''.
\begin{thm}{}{thmC.1.8x}
	Let $\K$ be a ring and let $A$ be a $\K$-algebra.  Then, for every $a\in A$ central, there is a unique $\K$-algebra homomorphism $\phi \colon \K [x]\rightarrow A$ such that $\phi (x)=a$.
	\begin{rmk}
		Intuitively, a homomorphism $\K [x]\rightarrow A$ is completely determined by where it sends $x$ (though you do have to ensure that you map it to a central element as $x$ is itself central).  This is not unlike how linear-transformations are completely determined by where they send the elements in a basis.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

Not only is this useful in its own right, it suggests a convenient way to define polynomial algebras in multiple variables.
\begin{thm}{Polynomial algebras in multiple variables}{PolynomialAlgebraMultiple}
	Let $X$ be a set.  Then, there is a unique $\K$-algebra, the \term{polynomial algebra}\index{Polynomial algebra} in the variables $X$ with coefficients in $\K$, $\K [X]$\index[notation]{$\K [X]$}, with $X\subseteq \K [X]$ that has the property that for every $\K$-algebra $A$ and function $f\colon X\rightarrow A$ with central image there is a unique $\K$-algebra homomorphism $\phi \colon \K [X]\rightarrow A$ such that $\restr{\phi}{X}=f$.
	\begin{rmk}
		Intuitively, this is the statement that any $\K$-algebra homomorphism $\K [X]\rightarrow A$ is determined solely by where you send the elements of $X$.
		
		For example, for $X=\{ x,y\}$, a $\K$-algebra homomorphism $\K [X]\rightarrow A$ is determined by a choice of two elements of $A$, say $a_x,a_y\in A$.  This then determines the image of every polynomial.  For example, the polynomial $3xy-16x^2+3y$ is mapped to $3a_xa_y-16a_x^2+3a_y$.
	\end{rmk}
	\begin{rmk}
		For the purpose of (hopefully) increasing clarity, we are actually being sloppy here.  When we say that $\K [X]$ is ``unique'', what we actually mean is that $\K [X]$ is ``unique up to unique isomorphism'' in the sense that, if $A$ is some other $\K$-algebra with $X\subseteq A$ that satisfies this property, then there is a unique isomorphism (of $\K$-algebras) $\A \rightarrow A$.
		
		Similarly, when we write $\A \subseteq A$, we don't \emph{literally} mean that $\A$ is a subset of $A$, but rather, that there is a unique embedding of $\K$-algebras $\A \rightarrow A$.
	\end{rmk}
	\begin{rmk}
		The single variable polynomial algebras are a special case of this:  $\K [x]\ceqq \K [\{ x\} ]$.  Thus, we could have just started with this result as the definition of all polynomial algebras, but I think this might be a bit difficult to understand without seeing the single variable case first, and in particular, the ``universal property'' stated in \cref{thmC.1.8x}.
	\end{rmk}
	\begin{rmk}
		In general, if $X\eqqc \{ x_1,\ldots ,x_m\}$ is a finite set, we write $\K [x_1,\ldots ,x_m]\ceqq \K [X]$\index[notation]{$\K [x_1,\ldots ,x_m]$}.
	\end{rmk}
	\begin{rmk}
		So, for example, $-3x^3yz^2+xy-2y+x+4z-6\in \Z [x,y,z]$.
	\end{rmk}
	\begin{rmk}
		As with the single variable case, we have a corresponding $\K$-algebra of ``right polynomials $[X]\K$ which, as a $\K$-algebra, is the same as $\K [X]$, but will be distinguished from $\K [X]$ by how the polynomials become polynomial functions---see \cref{PolynomialFunction}.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

\subsubsection{Polynomials vs.~polynomial functions}\label{sss1.1.1}

Given a polynomial $p\in \K [x]$, we obtain a corresponding function $\K \rightarrow \K$ defined by $x\mapsto p(x)$, where $p(x)\in \K$ is obtained by ``plugging-in'' the value $x\in \K$ into $p$ and evaluating.  For example, if $\K \ceqq \R$ and $p\ceqq 3x^2-5$, then $p(-2)\ceqq 3(-2)\cdot (-2)-5=7$.  The distinction between this function and the original polynomial is a subtle one.  Elements of $\K [x]$ are merely ``formal''\footnote{I'm not quite sure why this word is used.  I think it's a bit misleading, but the word ``formal'' in this context essentially means that the thing is to be mindlessly manipulated as symbols and nothing more.  For example, I can talk about the ``formal'' power series $1+2x+3x^2+4x^3+\cdots$ even though the resulting expression might not make sense when I plug in particular values of $x$---here, $x$ is just a symbol, not a number.} polynomials, and not functions themselves.

To see the difference between the concepts, take $\K \ceqq \Z /2\Z$ and consider the two polynomials $p\ceqq 0$ and $q\ceqq x+x^2=x(x-1)$.  These are manifestly different polynomials---the coefficients are different---but yet they both define the same function on $\Z /2\Z$ as $q(0)=0\cdot 1=0$ and $q(1)=1\cdot 0=0$.

We now make the distinction between a polynomial and a polynomial function precise by actually giving the definition of the latter.
\begin{dfn}{Polynomial function}{PolynomialFunction}\index{Polynomial function}
	Let $\K$ be a ring.
	\begin{enumerate}
		\item A \term{left polynomial function} is a function $\K \rightarrow \K$ of the form $\alpha \mapsto p(\alpha )$ for $p\in \K [x]$.
		\item A \term{right polynomial function} is a function $\K \rightarrow \K$ of the form $\alpha \mapsto p(\alpha )$ for $p\in [x]\K$.
	\end{enumerate}
	\begin{rmk}
		To clarify what is meant by $p(\alpha )$, write $p(x)=a_0+\cdots +a_mx^m\in \K [x]$.  Then,
		\begin{equation}
			p(\alpha )\ceqq a_0+\cdots +a_m\alpha ^m.
		\end{equation}
		Similarly, for $p(x)=a_0+\cdots +x^ma_m\in [x]\K$,
		\begin{equation}
			p(\alpha )\ceqq a_0+\cdots +\alpha ^ma_m.
		\end{equation}
	\end{rmk}
\end{dfn}

\horizontalrule

We mentioned after \cref{exm4.3.7} that, for a given linear operator, there is a polynomial, called the characteristic polynomial, of which all the eigenvalues are a root, and that essentially the reason that the matrix given in \cref{exm4.3.7} was diagonalizable over $\C$ but not over $\R$ was because the characteristic polynomial in this case had complex but not real roots.  This suggests that the property we want to impose on the ground division ring has something to do with roots of polynomials.  So, before anything else, let's be absolutely clear what we mean by the term ``root''.
\begin{cnv}{Two-sided}{TwoSided}
	In what follows, many definitions have left and right versions (because of the possible noncommutativity).  We then have a corresponding \term{two-sided}\index{Two-sided} version:  ABC is two-sided XYZ iff ABC is left XYZ and right XYZ.
	
	Furthermore, if we every drop these phrases, it should be understood that we are referring to the \emph{two-sided} version (e.g.~if we just say ``XYZ'' instead of ``two-sided XYZ'').
\end{cnv}
\begin{dfn}{Relative root}{Root}
	Let $\L$ be a $\K$-algebra and let $x_0\in \L$.\footnote{Recall that as $\K$-algebras, $\K [x]\cong [x]\K$, and so we may freely regard a polynomial in one as a polynomial in the other.  These two algebras only differ in how we ``plug-in'' values, as explained in the remarks below.}
	\begin{enumerate}
		\item For $p\in \K [x]$, $x_0$ is a \term{right root}\index{Right root} of $p$ iff $p(x_0)=0$.
		\item For $p\in [x]\K$, $x_0$ is a \term{left root}\index{Left root} of $p$ iff $(x_0)p=0$.
		\item For $\K [x]\ni p\in [x]\K$, $x_0$ is a \term{two-sided root}\index{Two-sided root} of $p$ iff it is both a left and right root of $p$.
	\end{enumerate}
	\begin{rmk}
		``Relative'' refers to the fact that we are only concerned with the roots of $p$ that lie in $\L$, so this is ``relative to $\L$''---see \cref{AbsoluteRoot} for an ``absolute version''.
	\end{rmk}
	\begin{rmk}
		Write $p(x)=a_0+a_1x+\cdots +a_mx^m$.  Then,
		\begin{equation}
			p(x_0)\ceqq a_0+a_1x_0+\cdots +a_mx_0^m.
		\end{equation}\index[notation]{$p(x_0)$}
		On the other hand, the corresponding polynomial in $[x]\K$ is $p(x)=a_0+xa_1+\cdots +x^ma_m$, in which case
		\begin{equation}
			(x_0)p\ceqq a_0+x_0a_1+\cdots +x_0^ma_m
		\end{equation}\index[notation]{$(x_0)p$}
		Note that in general these are \emph{not} the same.  
		
		That said, I find the notation $(x_0)p$ to be rather awkward and I shall avoid it unless strictly necessary (note how above I still wrote $p(x)=a+xa_1+\cdots +x^ma_m$---I suppose it might be better to denote this by $(x)p$, but as I said, that looks very awkward to me). 
	\end{rmk}
	\begin{rmk}
		Thus, the definition was probably exactly what you thought it would be, but we gave it just to be absolutely sure that noncommutativity didn't change the definition in any serious way.  Be warned, however:  though it didn't change the definition, it \emph{will} change the behavior of roots.  For example, we no longer have $[pq](x_0)=p(x_0)q(x_0)$, and so we can no longer conclude that $x_0$ is a root of $pq$ if it is a root of $p$.  (It will be true that $x_0$ is a root of $pq$ if it's a root of $q$, though hopefully this makes it clear we have to be more careful with our proof than you might have previously thought.)
	\end{rmk}
	\begin{rmk}
		The reason to involve two rings ($\L$ and $\K$) instead of just one is because, for example, we would like to be able to say that $\im \in \C$ is a root of $x^2+1\in \R [x]$ even though $\im \notin \R$.
	\end{rmk}
\end{dfn}
\begin{prp}{}{prp4.4.174}
	Let $\F$ be a division ring and let $x_0\in \F$.
	\begin{enumerate}
		\item $x_0$ is a right root of $p\in \F[x]$ iff there is some $q\in \F [x]$ such that $p(x)=q(x)(x-x_0)$.
		\item $x_0$ is a left root of $p\in [x]\F$ iff there is some $q\in [x]\F$ such that $p(x)=(x-x_0)q(x)$.
	\end{enumerate}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  See \cite[Proposition 16.2]{Lam}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{prp}
\begin{exm}{A degree $2$ polynomial with infinitely many roots}{}
	The polynomial $x^2+1$ has infinitely many roots in $\H$.\footnote{Of course, it has none in $\R$ and $2$ in $\C$.}
	\begin{exr}[breakable=false]{}{}
		Check this.
	\end{exr}
	\begin{exr}[breakable=false]{}{}
		On the other hand, show that $x^2+1$ doesn't have \emph{any} central roots.
	\end{exr}
	\begin{rmk}
		For fields, it is well-known (perhaps even to you) that a polynomial of degree $m$ can have at most $m$ roots.  Thus, this phenomenon is perhaps quite startling:  in what is arguably the simplest of all noncommutative division rings, we have a polynomial of degree $2$ with infinitely many roots!\footnote{Of course, we can't do this with degree less than $2$.}  Instead, this classical result generalizes to the statement that the set of all roots of $p$ belong to at most $m$ \emph{conjugacy classes} in $D$.\footnote{You don't need to know what ``conjugacy class'' means.  This is just a comment for those that already know and those that are curious enough to look up the details.  By the way, this theorem has a name:  it's called the \emph{Gordon-Motzkin Theorem}\index{Gordon-Motzkin Theorem}.}  Furthermore, it \emph{is} true that either (i) there are infinitely many roots or (ii) the number of roots is less than $m$.  For example, even over the most noncommutative division rings, you can't find a degree $3$ polynomial with $6$ roots:  the only possibilities are $0$, $1$, $2$, $3$, and infinitely many roots.
	\end{rmk}
\end{exm}

\subsection{Algebraic and integral}

\begin{dfn}{Algebraically closed}{AlgebraicallyClosed}
	Let $\K$ be a ring.  Then, $\K$ is \term{algebraically closed}\index{Algebraically closed} iff every nonconstant polynomial with coefficients in $\K$ can be written as a product of linear factors.
\end{dfn}
\begin{dfn}{Integrally closed}{IntegrallyClosed}
	Let $\K$ be a ring.  Then, $\K$ is \term{integrally closed}\index{Integrally closed} iff every nonconstant monic polynomial with coefficients in $\K$ can be written as a product of monic linear factors.
\end{dfn}
\begin{mdf}{Weakly algebraically closed}{WeaklyAlgebraicallyClosed}
	Let $\K$ be a ring.  Then, $\K$ is \term{XYZ weakly algebraically closed}\index{Weakly algebraically closed} iff every nonconstant $p\in \K [x]$ has an XYZ root in $\K$.
	\begin{rmk}
		Recall that (\cref{TwoSided}) XYZ can stand for either ``right'', ``left'', or ``two-sided''---this applies throughout though we won't remind you again.\footnote{I imagine that would get slightly annoying\textellipsis}
	\end{rmk}
	\begin{rmk}
		The reason why this is \emph{right} weakly algebraically closed (and not left) is that the statement that $\alpha \in \K$ is a root of $p$ is equivalent to the statement that $p(x)=q(x)(x-\alpha )$ for some $q\in \K [x]$\footnote{This is \emph{not} in general equivalent to the statement that $p(x)=(x-\alpha )r(x)$ for some $r\in \K [x]$ if $\K$ is noncommutative.}---see the following result \cref{prp4.4.174}.  Similarly for left weakly algebraically closed.
	\end{rmk}
	\begin{rmk}
		Warning:  $\K$ being (two-sided) algebraically closed is \emph{not} the same as every nonconstant polynomial having a (two-sided) root.  According to \cref{TwoSided}, $\K$ being (two-sided) algebraically closed means that it is both left algebraically closed and right algebraically closed.  Thus, every nonconstant polynomial would have to have a right and a left root, but these roots don't necessarily have to coincide (that is, be a (two-sided) root).
	\end{rmk}
	\begin{rmk}
		Of course, algebraically closed implies weakly algebraically closed.
	\end{rmk}
\end{mdf}
\begin{mdf}{Weakly integrally closed}{WeaklyIntegrallyClosed}
	Let $\K$ be a ring.  Then, $\K$ is \term{XYZ integrally closed}\index{Weakly integrally closed} iff every nonconstant monic $p\in \K [x]$ has a XYZ root in $\K$.
	\begin{rmk}
		Warning:  Don't confuse this with the term ``integrally closed-domain''!\footnote{This means that it is an integral domain that is integrally closed in its field of fractions, as opposed to just ``absolutely'' integrally closed, not relative to another ring.}
	\end{rmk}
	\begin{rmk}
		You should see \cref{exmC.2.20} to perhaps better understand the difference between the integrally closed and algebraically closed.
		
		For me anyways, algebraically closed is the more intuitive of the two, but this is perhaps just because I was taught it much earlier.  One reason for the introduction of ``integrally closed'' when one already has the notion of ``algebraically closed'' is that the former is in general more well-behaved than the latter.  For example, integral elements form a ring (\cref{RelativeIntegralClosure}) whereas the algebraic elements do not necessarily (\cref{exmC.2.24}).
	\end{rmk}
	\begin{rmk}
		Note that in the common case of interest where $\K$ is a field, the notions of ``integral'' and ``algebraic'' are equivalent---see \cref{prpC.2.18}.
	\end{rmk}
	\begin{rmk}
		Warning:  $\K$ being (two-sided) integrally closed is \emph{not} the same as every nonconstant monic polynomial having a (two-sided) root.  According to \cref{TwoSided}, $\K$ being (two-sided) integrally closed means that it is both left integrally closed and right integrally closed.  Thus, every nonconstant monic polynomial would have to have a right and a left root, but these roots don't necessarily have to coincide (that is, be a (two-sided) root).
	\end{rmk}
	\begin{rmk}
		Of course, integrally closed implies weakly integrally closed.
	\end{rmk}
\end{mdf}
\begin{prp}{}{prpC.2.19}
	Let $\K$ be a ring.  Then, the following are equivalent.
	\begin{enumerate}
		\item $\K$ is integrally closed.
		\item $\K$ is right weakly integrally closed.
		\item $\K$ is left weakly integrally closed.
		\item $\K$ is two-sided weakly integrally closed.
	\end{enumerate}
	\begin{rmk}
		Warning:  The analogous result for ``algebraic'' should be false, though it is true if $\K$ is a field---see \cref{prpC.2.20}.
	\end{rmk}
	\begin{rmk}
		Hence, we need only use the term ``integrally closed''.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  \href{http://stacks.math.columbia.edu/tag/0DCK}{The Stacks Project}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{}{prpC.2.20}
	Let $\F$ be a field.  Then, the following are equivalent.
	\begin{enumerate}
		\item $\F$ is algebraically closed.
		\item $\F$ is right weakly algebraically closed.
		\item $\F$ is left weakly algebraically closed.
		\item $\F$ is two-sided weakly algebraically closed.
		\item $\F$ is integrally closed.
		\item $\F$ is right weakly integrally closed.
		\item $\F$ is left weakly integrally closed.
		\item $\F$ is two-sided weakly integrally closed.
	\end{enumerate}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exr}{}{}
	Can you find a counter-example showing that \cref{prpC.2.19} fails when you replace ``integrally'' with ``algebraically''?
\end{exr}
\begin{exm}{}{}
	Note that $\C$ is algebraically closed but $\R$ is not.  $\H$ is also (left and right) algebraically closed.
\end{exm}
\begin{exr}{}{}
	Can you find an example of a division ring that is left algebraically closed but not right algebraically closed (or vice versa)?
\end{exr}
Of course, not everything is algebraically closed or even integrally closed (for example, the real numbers).  As being algebraically closed is such a useful property, it is of interest to `enlarge' a given field to something that is algebraically closed.  The basic idea is to take your field and just `throw in' all solutions to polynomial equations, not unlike one obtains $\Q$ from $\Z$ by ``throwing in'' all solutions to equations of the form $mx=n$.  The field obtained in this way is the \emph{algebraic closure} of the original field.  Before we make this intuition precise, however, we must first discuss some preliminaries.
\begin{mdf}{Algebraic element}{AlgebraicElement}
	Let $\L$ be a $\K$-algebra and let $\alpha \in \L$.  Then, $\alpha$ is \term{XYZ algebraic}\index{Algebraic element} over $\K$ iff there is a nonconstant polynomial $p\in \K [x]$ such that $\alpha$ is an XYZ root of $p$.
	
	$\alpha$ is \term{XYZ transcendental}\index{Transcendental element} over $\K$ iff it is not XYZ algebraic over $\K$.
\end{mdf}
\begin{mdf}{Integral element}{IntegralElement}
	Let $\L$ be a $\K$-algebra and let $\alpha \in \L$.  Then, $\alpha$ is \term{XYZ integral}\index{Integral element} over $\K$ iff there is a nonconstant monic polynomial $p\in \K [x]$ such that $\alpha$ is an XYZ root of $p$.
\end{mdf}
\begin{exm}{}{}
	$\sqrt{2}\in \R$ is algebraic over $\Q$ (it is a root of the polynomial $x^2-2\in \Q [x]$).
	
	$\uppi \in \R$ is not algebraic over $\Q$.\footnote{For example, by the Lindemann-Weierstrass Theorem, if you enjoy swatting your flies with sledgehammers (I know I do).}
\end{exm}
\begin{mpr}{Integral=algebraic over \\ division rings}{prpC.2.18}
	Let $\F$ be a division ring, let $A$ be an $\F$-algebra, and let $x\in A$.  Then, $x$ is XYZ integral over $\F$ iff it is XYZ algebraic over $\F$.
	\begin{rmk}
		Similarly, a division ring is XYZ integrally closed iff it is XYZ algebraically closed.
	\end{rmk}
	\begin{proof}
		$(\Rightarrow )$ Immediate from the definitions.
		
		\blni
		$(\Leftarrow )$ Suppose that $x$ is right algebraic over $\F$.  By definition, there is a nonconstant polynomial $p\in \F [x]$ such that $p(x)=0$.  Denote the leading coefficient of $p$ by $a_m$.  Multiplying $p(x)=0$ on the left by $a_m^{-1}$ shows that $[a_m^{-1}p](x)=0$.  As $a_m^{-1}p$ is monic, this shows that $x$ is left-integral.
	\end{proof}
\end{mpr}
\begin{exm}{Algebraic but not integral}{exmC.2.20}
	$\frac{1}{2}\in \Q$ is algebraic but not integral over $\Z$:  it is a root of the polynomial $2x-1$, but it is not the root of any monic polynomial with integer coefficients.
\end{exm}
\begin{mdf}{Algebraic $\K$-algebra}{AlgebraicKAlgebra}
	Let $\L$ be a $\K$-algebra.  Then, $\L$ is \term{XYZ algebraic}\index{Algebraic $\K$-algebra} iff every element of $\L$ is XYZ algebraic over $\K$.
	\begin{rmk}
		We may instead say that $\L$ is \term{algebraic} over $\K$.
	\end{rmk}
	\begin{rmk}
		If $\L$ is an extension of $\K$, then people will say that ``$L$ is an \term{XYZ algebraic extension}\index{Algebraic extension} of $\K$'' instead of that ``$\L$ is an XYZ algebraic $\K$-algebra''.
	\end{rmk}
\end{mdf}
\begin{mdf}{Integral $\K$-algebra}{IntegralKAlgebra}
	Let $\L$ be a $\K$-algebra.  Then, $\L$ is \term{XYZ integral}\index{Integral $\K$-algebra} iff every element of $\L$ is XYZ integral over $\K$.
	\begin{rmk}
		We may instead say that $\L$ is \term{integral} over $\K$.
	\end{rmk}
	\begin{rmk}
		If $\L$ is an extension of $\K$, then people will say that ``$L$ is an \term{XYZ integral extension}\index{Integral extension} of $\K$'' instead of that ``$\L$ is an XYZ integral $\K$-algebra''.
	\end{rmk}
\end{mdf}
\begin{mdf}{Algebraic closure}{AlgebraicClosurex}
	Let $\L$ be a $\K$-algebra.  Then, $\L$ is an \term{XYZ algebraic closure}\index{Algebraic closure} of $\K$ iff it is algebraically closed and XYZ algebraic over $\K$.
\end{mdf}
\begin{mdf}{Integral closure}{IntegralClosure}
	Let $\L$ be a $\K$-algebra.  Then, $\L$ is an \term{XYZ integral closure}\index{Integral closure} of $\K$ iff it is integrally closed and XYZ integral over $\K$.
\end{mdf}
\begin{exm}{}{}
	\begin{enumerate}
		\item $\H$ is an algebraic extension of $\C$.
		\item $\C$ is an algebraic extension of $\R$.
		\item $\R$ is not an algebraic extension of $\Q$.
	\end{enumerate}
\end{exm}
\begin{prp}{Relative integral closure}{RelativeIntegralClosure}
	Let $A$ be a commutative $\K$-algebra.  Then,
	\begin{equation}
		\left\{ x\in A:x\text{ is integral over }\K \text{.}\right\}
	\end{equation}
	is an integral $\K$-subalgebra of $A$, the \term{integral closure}\index{Relative integral closure} of $\K$ in $A$.
	
	Furthermore,
	\begin{enumerate}
		\item if $A$ is integrally closed, then integral closure of $\K$ in $A$ is integrally closed; and
		\item if $\K$ and $A$ are fields, then the integral closure of $\K$ in $A$ is a field.
	\end{enumerate}
	\begin{rmk}
		Warning:  The analogous result for algebraic fails---see \cref{exmC.2.24}.  Indeed, this is one motivation for preferring integral over algebraic.  That said, it seems like it will hold for integral crings,\footnote{Different meaning of the word ``integral''.  In fact, just off the top of my head I can think of four different meanings of that word.  Like I said, mathematicians are terrible when it comes developing sensible terminology.} though we won't use this fact---see \href{https://math.stackexchange.com/questions/759345/algebraic-vs-integral closure-of-a-ring?rq=1}{math.stackexchange}.
	\end{rmk}
	\begin{rmk}
		``Relative''refers to the fact that this is the integral closure of $\K$ \emph{in $A$}---see \cref{AlgebraicClosure} for the ``absolute'' integral closure.  
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exr}{}{}
	Is the previous result true if $A$ is noncommutative?
\end{exr}
\begin{exm}{The algebraic elements need not form a ring}{exmC.2.24}
	We leave this as an exercise.
	\begin{exr}[breakable=false]{}{}
		Find such a counter-example.
		\begin{rmk}
			Hint:  See \href{https://mathoverflow.net/questions/163749/algebraic closure-of-a-ring-is-not-a-ring}{mathoverflow}.
		\end{rmk}
	\end{exr}
\end{exm}
\begin{thm}{Rings have weakly algebraically closed extensions}{thmC.2.25}
	Let $\K$ be a (commutative) ring.  Then, there exists a (commutative) weakly algebraically closed extension $A$ of $\K$.
	
	Furthermore, if $\K$ is field, then $A$ itself taken to be a field.
	\begin{rmk}
		Note that $A$ is integrally closed by \cref{prpC.2.19}.
	\end{rmk}
	\begin{proof}
		\Step{Introduce notation}
		Define
		\begin{equation}
			\mcal{F}\ceqq \left\{ f\in \K [x]:f\text{ nonconstant.}\right\} ,
		\end{equation}
		\begin{equation}
			X\ceqq \left\{ x_f:f\in \mcal{F}\right\} ,
		\end{equation}
			and
		\begin{equation}
			\L \ceqq \K [X].
		\end{equation}
		Intuitively, we have a ``variable'' $x_f$ for each monic $f\in \K [X]$ and $\L$ is the polynomial algebra in those variables.  Now define
		\begin{equation}
			I\ceqq \left\{ \sum _{f\in \mcal{F}}g_ff(x_f)h_f:g_f,h_f\in \L \right\} ,
		\end{equation}
		the ideal ``generated by'' the $f(x_f)$s.
		
		\Step{Construct a $\K$-algebra that contains a root for every nonconstant $f\in \K [x]$}
		Now define
		\begin{equation}\label{eqnC.2.31}
			\K _1\ceqq \L /I.
		\end{equation}
		$\K _1$ is a $\K$-algebra, and furthermore, it contains a root of every nonconstant $f\in \K [x]$, namely $x_f+I\in \K _1$.\footnote{This is a root because, as $f(x_f)\in I$ (from the definition of $I$), $f(x_f+I)=f(x_f)+I=0$ in $\L /I$.}
		
		\Step{Repeat this process inductively}
		Performing the same construction, we obtain a $\K _1$-algebra $\K _2$ that contains a root of every nonconstant $f\in \K _1[x]$.  Note that $\K _2$ can also be considered a $\K$-algebra via the composite structure map $\K \rightarrow \K _1\rightarrow \K _2$.  Proceeding inductively, we obtain a sequence of $\K$-algebra $\{ \K _m:m\in \N \}$ such that $\K _{m+1}$ contains roots of every nonconstant $f\in \K [x]$.\footnote{Take $\K _0\ceqq \K$.}
		
		\Step{Define the weakly algebraically closed $\K$-algebra}
		Now define\footnote{This definition doesn't literally make sense as we don't literally have that $\K _m\subseteq \K _{m+1}$.  Instead, what is actually meant is the disjoint union of the $\K _m$s modulo the equivalence relation that identifies elements in $\K _m$ with their images in $\K _{m+1}$.}
		\begin{equation}
			\K _{\infty}\ceqq \bigcup _{m\in \N}\K _m.
		\end{equation}
		
		\Step{Check that $\K _{\infty}$ is indeed weakly algebraically closed}
		We claim that $\K _{\infty}$ is algebraically closed.  So, let $f\in \K _{\infty}[x]$ be nonconstant.  Then, as $f$ has finitely many terms, there is some $m\in \N$ such that $f\in \K _m[x]$.  But we know that $f$ then has a root in $\K _{m+1}$, and hence in $\K _{\infty}$.  Thus, $\K _{\infty}$ is algebraically closed.
		
		\Step{Check that $\K _{\infty}$ is commutative if $\K$ is}
		Looking at the above proof, we see that $\L$ is commutative if $\K$ is, and hence $\K _1$ is commutative if $\K$ is.  Thus, each $\K _m$ is commutative if $\K$ is, and hence so is $\K _{\infty}$.
		
		\Step{Check that $\K _{\infty}$ is an extension}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Finish this step.
		\end{exr}
		
		\Step{Check the extra properties if $\K$ is a field}
		Suppose that $\K$ is a field.  We show in this case that $1\notin I$.  If it were, we could write
		\begin{equation}
			g_1f_1(x_{f_1})+\cdots +g_mf_m(x_{f_m})=1
		\end{equation}
		for $g_k\in \L$.  For convenience, let us write $x_k\ceqq x_{f_k}$, and enumerate any remaining variables that appear in any $g_k$ as $x_{m+1},\ldots ,x_n$, so that this equation then reads
		\begin{equation*}
			g_1(x_1,\ldots ,x_n)f_1(x_1)+\cdots +g_m(x_1,\ldots ,x_n)f_m(x_m)=1.
		\end{equation*}
		Define $A_1\ceqq \K [x]/I_1$, where $I_1\ceqq \{ af_1(x)b:a,b\in \K \}$, the ideal ``generated'' by $f_1$.  $A_1$ is a $\K$-algebra that contains a root of $f_1$ (namely $x+I_1\in A_1$), and furthermore, the image of $1\in \K$ under the quotient map $\K \rightarrow A_1$ is nonzero, for if it were, we would have $1=af_1(x)b$ for some $a,b\in \K$, in which case $f_1(x)$ would be a constant polynomial, a contradiction.  We may now define $A_2\ceqq A_1[x]/I_2$, where $I_2\ceqq \{ af_2(x)b:a,b\in \K _1\}$.  As before, $A_2$ is a $\K$-algebra that now contains roots of both $f_1$ and $f_2$, and furthermore, the image of $1\in \K$ under the map $\K \rightarrow A_2$ is nonzero.  Proceeding inductively, we obtain a $\K$-algebra $A_m$ that contains roots of $f_1,\ldots ,f_m$ and $1\neq 0$ in $A_m$.  Plugging in these roots for $x_1,\ldots ,x_m$ respectively (and anything for the remaining variables), the above equation reduces to $0=1$ in $A_m$:  a contradiction.\footnote{If $\K$ were noncommutative, then these terms wouldn't just be of the form $g_kf_k$, but rather, $g_kf_kh_k$, and now, the result we get by plugging in the root $\alpha _k$ is not necessarily equal to $g_k(\alpha _k)f_k(\alpha _k)h_k(\alpha _k)$.  This seems to break the proof for $\K$ noncommutative.}  Thus, $1\notin I$.
		
		We just showed that $1\notin I$, and so in particular $I\subseteq \L$ is a proper ideal.  It is thus contained in a maximal ideal $M$, in which case let us replace our definition of $\K _1$ in \eqref{eqnC.2.31} by
		\begin{equation}
			\K _1\ceqq \L /M.
		\end{equation}
		$\K _1$ now has all the properties it did before, except now it is additionally a field.  Proceeding inductively, we find that each $\K _m$ is a field.  Then, for $x\in \K _{\infty}$ nonzero, we have that $x\in \K _m$ for some $m\in \N$, in which case $x$ has an inverse in $\K _m$, and hence in $\K _{\infty}$.  Thus, $\K _{\infty}$ is likewise a field.
	\end{proof}
\end{thm}
\begin{exr}{}{}
	In the above result, if $\K$ is a division ring, can we still take $A$ to be an extension of $\K$?  Can we take $A$ to be simple?
\end{exr}
\begin{thm}{Integral closure}{AlgebraicClosure}
	Let $\K$ be a cring.  Then, there is a unique commutative integral closure $\A _{\K}$\index[notation]{$\A _{\K}$} of $\K$ which is an extension of $\K$.
	
	Furthermore, if $\K$ is a field, then $\A _{\K}$ itself is a field.
	\begin{rmk}
		While it is true (\cref{thmC.2.25}) that any ring $\K$ has an algebraically closed $\K$-algebra, it is \emph{not} true that rings, even commutative ones, have algebraic closures.  Essentially this boils down to the fact that the algebraic elements need not form a ring (\cref{exmC.2.24}), whereas the integral ones do (\cref{RelativeIntegralClosure}).
	\end{rmk}
	\begin{rmk}
		For the purpose of (hopefully) increasing clarity, we are actually being sloppy in a couple of places here.  First of all, when we say ``unique'', what we actually mean is that $\A$ is ``unique up to isomorphism''\footnote{But not unique isomorphism!} in the sense that, if $A$ is some other integrally closed $\K$-algebra that satisfies this property, then there is an isomorphism (of $\K$-algebras) $\A \rightarrow A$.
	\end{rmk}
	\begin{rmk}
		In the important case $\K$ is a field, there is no distinction between integral and algebraic (\cref{prpC.2.18}), and so in this context people almost always just say ``algebraic closure''.
	\end{rmk}
	\begin{rmk}
		Warning:  As far as I know, the integral closure does not satisfy any universal property---see \href{https://math.stackexchange.com/questions/1033209/universal-property-of-the-algebraic closure-of-a-field}{math.stackexchange}.  As such, while $\A _{\K}$ is unique up to isomorphism, it is not unique up to \emph{unique} isomorphism.  For example, it's also true that there is only one, up to isomorphism, $d$-dimensional vector space over $\R$---would you carelessly make no distinction between $\R ^d$ and $\R [x]_{d-1}$ as a result?  Probably not most of the time.  Keep this in mind any time you want to say ``\emph{the} integral closure''---it probably won't break anything, but, as I said, you should at least keep this in mind.
	\end{rmk}
	\begin{proof}
		\Step{Existence}
		By \cref{thmC.2.25}, there is a commutative integrally closed $\K$-algebra $A$.  Take $A$ to be a field if $\K$ is.  Define
		\begin{equation}
			\A _{\K}=\left\{ x\in A:x\text{ is integral over }\K \text{.}\right\} .
		\end{equation}
		By \cref{RelativeIntegralClosure}, $\A _{\K}$ is still a commutative integrally closed $\K$-algebra, which is a field if $\K$ is.  By construction, it is integral over $\K$, and hence an integral closure of $\K$.
		
		\Step{Uniqueness}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  Show that any integral extension $A$ of $\K$ embeds in $\A _{\K}$ by using Zorn's Lemma to find a map into $\A _{\K}$ with maximal domain---see \href{http://math.stanford.edu/~conrad/121Page/handouts/algclosure.pdf}{math.stanford.edu}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{thm}
\begin{exr}{}{}
	Do noncommutative rings have (left/right) integral closures?  What about noncommutative division rings in particular?  
\end{exr}

One of the most significant implications of the existence of an algebraic closure is that they permit us to make the following definitions.
\begin{dfn}{Absolute root}{AbsoluteRoot}
	Let $\F$ be a field, let $\A _{\F}$ be an algebraic closure of $\F$, and let $p\in \F [x]$.  Then, a \term{root}\index{Root} of $p$ is a root of $p$ in $\A _{\F}$.
	\begin{rmk}
		Warning:  Note that this \emph{does} technically depend on a choice of algebraic closure.  Thus, if we ever use the word ``root'' in this sense, it should be implicitly understood that we chose an algebraic closure before hand.  As algebraic closures are unique up to isomorphism, this seldom will make a difference (at least for us), but it's still good to keep in mind.
	\end{rmk}
	\begin{rmk}
		Of course, $\A _{\F}$ exists by the previous result.  Note that we need $\F$ to be a field, otherwise we are only guaranteed that only \emph{monic} polynomials have roots.
	\end{rmk}
\end{dfn}
\begin{prp}{Generated field}{GeneratedField}
	Let $\F$ be a field, let $\A$ be an algebraic closure of $\F$, and let $S\subseteq \A$.  Then, there exists a unique subfield $\F (S)$\index[notation]{$\F (S)$} of $\A$, the subfield \term{generated}\index{Generated subfield} by $S$, such that
	\begin{enumerate}
		\item $S\subseteq \F (S)$; and
		\item if $\K \subseteq \A$ is any other subfield containing $S$, it follows that $\F (S)\subseteq \K$.
	\end{enumerate}
	\begin{rmk}
		If $S\eqqc \{ s_1,\ldots ,s_m\}$ is finite, people typically write
		\begin{equation}
			\F (s_1,\ldots ,s_m)\ceqq \F (S).
		\end{equation}\index[notation]{$\F (s_1,\ldots ,s_m)$}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

\subsection{Summary}

We started out with the objective of being able to solve polynomial equations over a given field.  This gives us the concept of \emph{algebraic closure} (\cref{AlgebraicClosurex}), though we found that the related concept of \emph{integral closure} (\cref{IntegralClosure}) was better-behaved in general (\cref{RelativeIntegralClosure,exmC.2.24}), and in any case agree with the algebraic closure for division rings (\cref{prpC.2.18}).

To talk about such things, we needed to discuss $\K$-algebras (\cref{KAlgebra}), algebraic and integral $\K$-algebras (\cref{AlgebraicKAlgebra,IntegralKAlgebra}, and what it meant for these algebras to be algebraically closed and integrally closed (\cref{AlgebraicallyClosed,IntegrallyClosed}).

Finally, we proved that every cring has a unique integral closure that is an extension and a field if the cring was (\cref{AlgebraicClosure}).

Of course, there were other things, but these are the bullet-points.

\section{Perfect fields}

To be honest, the intuition for what are called \emph{perfect fields} is hard for me to describe.  For us, the motivation is simple:  (i) every field we encounter in these notes (with exception of the one I construct below simply for the purpose of producing a field that is not perfect) is perfect and (ii) this is the most general sufficient condition on the ground field I am aware of in order for the \namerefpcref{JordanChevalleyDecomposition} to hold.  It is important for you to know the statement of Jordan-Chevalley, and it's also important to know that it's going to work for a very large class of fields, almost certainly a class large enough that it contains every field you've ever seen before if you're learning linear-algebra for the first time.  On the other hand, it is not important for you to know the details of the theory of perfect fields.

So I basically just told you that you shouldn't care about the details about perfect fields, only that essentially everything you'll encounter in the notes is perfect, and so the Jordan-Chevalley Decomposition can be applied.  If you're one of those pesky students\footnote{That's a joke.  Such students are the best type of students.} that likes to know everything ``beyond the scope of this course'', that's probably not very satisfying, so let me try a little bit harder to motivate perfect fields.

The proof of Jordan-Chevalley uses Galois theory, and, in some sense, perfect fields make Galois theory `work'.  The largest extension of a given field that is Galois its what's called its \emph{separable closure}, which is contained in the algebraic closure.  However, the algebraic closure is more intuitive\footnote{At least for me.}, and so it would be nice if Galois theory would work over the algebraic closure.  Unfortunately, that's not always the case.  In fact, a field is \emph{perfect} iff the separable closure and algebraic closure coincide.  Thus, in this sense, perfect fields are precisely the fields in which Galois theory ``works'' over the algebraic closure.

Anyways, let's get started.
\begin{dfn}{Separable polynomial}{SeparablePolynomial}
	Let $\F$ be a field and let $p\in \F [x]$.  Then, $p$ is \term{separable}\index{Separable polynomial} iff the number of distinct roots of $p$ is equal to $\deg (p)$.
	\begin{rmk}
		Note that we need to work over a field here as we are implicitly making using of an algebraic closure---see \cref{AbsoluteRoot}.
	\end{rmk}
	\begin{rmk}
		Intuitively, we think of all the roots of $p$ having ``multiplicity $1$''.  Sometimes people say that ``$p$ has distinct roots''.
	\end{rmk}
	\begin{rmk}
		I'm not quite sure where the word ``separable'' comes from.  Perhaps it comes from the fact that, in an algebraic closure, you can `separate' $p$ into \emph{distinct} linear factor, though I think this is misleading as you can ``separate'' any polynomial into linear factors, they just won't necessarily be distinct.
	\end{rmk}
\end{dfn}
\begin{dfn}{Perfect field}{PerfectField}
	Let $\F$ be a field.  Then, $\F$ is \term{perfect}\index{Perfect field} iff every irreducible polynomial $p\in \F [x]$ is separable.
	
	$\F$ is \term{imperfect}\index{Imperfect field} iff it is not perfect.
	\begin{rmk}
		For a ring $R$ and $x\in R$ not a unit, $x$ is \term{irreducible}\index{Irreducible (in a ring)} iff whenever $x=ab$, it follows that either $a$ or $b$ is a unit.\footnote{Recall that (\cref{dfnA.1.33}) a unit is an element with a multiplicative inverse.}
		
		This definition of irreducible is one generalization of the concept of a prime integer to a general ring (there are others).
	\end{rmk}
\end{dfn}
There are several conditions one may impose on a field that are equivalent to being perfect.
\begin{thm}{}{thmC.4.2}
	Let $\F$ be a field.  Then, the following are equivalent.
	\begin{enumerate}
		\item $\F$ is perfect.
		\item Either $\Char (\F )=0$, or $\Char (\F )\eqqc p$ and $\F \ni x\mapsto x^p\in \F$ is a ring automorphism.
		\item Either $\Char (\F )=0$, or $\Char (\F )\eqqc p$ and $\F \ni x\mapsto x^p\in \F$ is surjective.
	\end{enumerate}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
Most fields you have probably encountered are perfect.
\begin{prp}{}{prpC.4.3}
	Let $\F$ be a field.  Then, $\F$ is perfect if any of the following are true.
	\begin{enumerate}
		\item $\F$ is algebraically closed.
		\item $\Char (\F )=0$.
		\item $\F$ is finite.
	\end{enumerate}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
Of course, there are examples of imperfect fields.
\begin{exr}{The Freshman's Dream}{}
	Let $\F$ be a field with $p\ceqq \Char (\F )\neq 0$.  Show that
	\begin{equation}
		(x+y)^p=x^p+y^p
	\end{equation}
	for all $x,y\in \F$.
	\begin{rmk}
		Hint:  Use the Binomial Theorem and argue that the binomial coefficients must vanish (except of course for the first and last ones, both of which are just $1$).
	\end{rmk}
\end{exr}
\begin{exm}{An imperfect field}{AnImperfectField}
	Let $p\in \Z ^+$ be prime, write $\F _p\ceqq \Z /p\F$, and define $\F \ceqq \F _p(t)$.\footnote{For any cring $\K$, $\K (x)$\index[notation]{$\K (x)$} is the \emph{field of fractions} of $\K [x]$.  That is, $\K (x)$ is constructed from $\K [x]$ in the same way that $\Q$ is constructed from $\Z$.}
	
	We claim that $\F$ is imperfect.  To show this, we check that $x^p-t\in \F [x]$ is irreducible but does not have distinct roots.
	\begin{exr}[breakable=false]{}{}
		Show that $x^p-t\in \F [x]$ is irreducible.
	\end{exr}
	If $x_0$ is a root of $x^p-t$, then $x_0^p-t=0$, and hence $t=x_0^p$.  But then
	\begin{equation}
	x^p-t=x^p-x_0^p=(x-x_0)^p,
	\end{equation}
	and hence $(x-x_0)^p$ divides $x^p-t$.  As $p\geq 2$, $x^p-t$ cannot have distinct roots.
	\begin{rmk}
		This example is quite unimportant for us, and so you shouldn't worry if you don't understand everything going on here.
	\end{rmk}
\end{exm}

\section{(Semi)simplicity}

There is a fundamental concept in the theory of modules called \emph{simplicity}.  Indeed, the concept is sufficiently fundamental that direct analogues\footnote{And nonanalogues *cough*Lie algebras*cough*.} appear throughout algebra.
\begin{dfn}{Simple module}{SimpleModule}
	Let $V$ be an $R$-module.  Then, $V$ is \term{simple}\index{Simple module} iff the only submodules of $V$ are $0$ and $V$ itself.
	\begin{rmk}
		Sometimes the term \term{irreducible}\index{Irreducible module} is used as a synonym for ``simple''.  As far as I can tell, ``irreducible'' seems to be the preferred term in the context of representation theory.  I will use both, depending on what feels most `natural' to me in a given context.
	\end{rmk}
	\begin{rmk}
		You should compare this with the definition of indecomposability (\cref{Indecomposable}).
	\end{rmk}
\end{dfn}
\begin{prp}{}{prpC.2.2}
	Let $V$ be an $R$-module.  Then, if $V$ is irreducible, $V$ is indecomposable.
	\begin{proof}
		Suppose that $V$ is irreducible.  Let $U,W\subseteq V$ be submodules such that $V=U\oplus W$.  As $V$ is irreducible, either $U=0$ or $U=V$.  In the former case, we are done.  In the latter case, we must have $W=0$, and we are again done.
	\end{proof}
\end{prp}
\begin{exm}{An indecomposable module that is not irreducible}{exmC.1.2}
	Define
	\begin{equation}
		N\ceqq \begin{bmatrix}0 & 1 \\ 0 & 0\end{bmatrix}
	\end{equation}
	and use $\N$ to regard $\C ^2$ as a $\C [x]$-module as in \cref{exm1.1.34}.
	
	\begin{exr}[breakable=false]{}{}
		Show that
		\begin{equation}
			\Span \left( \begin{bmatrix}1 \\ 0\end{bmatrix}\right) 
		\end{equation}
		is $N$-invariant.  Deduce that $\C ^2$ is not irreducible as a $\C [x]$-module.
	\end{exr}
	\begin{exr}[breakable=false]{}{}
		Show that $\C ^2$ is not an indecomposable $\C [x]$-module.
	\end{exr}
\end{exm}

We had a term (\cref{prp4.5.17}), \emph{complete-decomposability}, for a module that can be written as a direct-sum of indecomposable modules.  So to, we have an analogous term for irreducible modules.
\begin{dfn}{Semisimple module}{SemisimpleModule}
	Let $V$ be an $R$-module.  Then, $V$ is \term{semisimple}\index{Semisimple module} iff $V$ can be written as the direct-sum of simple submodules.
	\begin{rmk}
		Just as the term ``irreducible'' is used as a synonym for ``simple'', the term \term{completely-reducible}\index{Completely-reducible module} is used as a synonym for ``semisimple''.
	\end{rmk}
	\begin{rmk}
		A linear operator $T\colon V\rightarrow V$ on a $\K$-module $V$ is \term{semisimple}\index{Semisimple operator} iff the $\K [x]$-module defined by the action of $T$ is semisimple.
	\end{rmk}
	\begin{rmk}
		By \cref{prpC.2.2}, we immediately see that completely-reducible implies completely-indecomposable, but that the converse fails (\cref{exmC.1.2}).
	\end{rmk}
	\begin{rmk}
		Warning:  As previously mentioned in the definition of simple ring (\cref{SimpleRing}), there is a definition of ``semisimple ring'', but it is not analogous to ``semisimple module'' in the way that ``simple ring'' is analogous to ``simple module''.
	\end{rmk}
\end{dfn}
\begin{thm}{Fundamental Theorem of Semisimple Modules}{FTSemisimple}
	Let $V$ be a $\K$-module.  Then, the following are equivalent.
	\begin{enumerate}
		\item \label{FTSemisimple(i)}$V$ is semisimple.
		\item \label{FTSemisimple(ii)}$V$ is a sum of simple submodules.
		\item \label{FTSemisimple(iii)}For every submodule $W\subseteq V$, there is a submodule $U\subseteq V$ such that $V=U\oplus W$.
	\end{enumerate}
	\begin{rmk}
		In other words, \cref{FTSemisimple(iii)} says that $V$ is semisimple iff every submodule has a complement.
	\end{rmk}
	\begin{rmk}
		Warning:  Again, the name of this result is nonstandard.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
Our interest in semisemplicity arises from the fact that semisimple operators are in a certain sense a generalization of diagonalizable operators.
\begin{thm}{Semisimple (linear operator)}{semisimple_linear_operator}
	Let $\K$ be a field, let $\A$ be an algebraic closure of $\K$, let $V$ be a finite-dimensional vector space over $\K$, write $V^{\A}\ceqq \A \otimes _{\K}V$, and let $T\colon V\rightarrow V$ be a linear operator.  Then, the following are equivalent.
	\begin{enumerate}
		\item \label{semisimple_linear_operator.i}The minimal polynomial of $T$ is separable.
		\item \label{semisimple_linear_operator.ii}$T\colon V^{\A}\rightarrow V^{\A}$ is diagonalizable.
		\item \label{semisimple_linear_operator.iii}The $\K [x]$-module $V_T$ defined by $T$ is semisimple.
		\item \label{semisimple_linear_operator.iv}Every $T$-invariant subspace has a $T$-invariant complement.
	\end{enumerate}
	\begin{rmk}
		If these conditions are satisfied, then we say that $T$ is \term{semisimple}\index{Semisimple (linear operator)}.
	\end{rmk}
	\begin{proof}
		$(\cref{semisimple_linear_operator.i}\Rightarrow \cref{semisimple_linear_operator.ii})$ Suppose that the minimal polynomial of $T$ is separable.  Denote this minimal polynomial by $p$ and write $p(x)=(x-\lambda _1)\cdots (x-\lambda _m)$, with $\lambda _1,\ldots ,\lambda _m$ all distinct.  By the Jordan Canonical Form Theorem, we can pick a basis $\mcal{B}$ for $V^{\A}$ such that $[T]_{\mcal{B}}=D+N$ where $D$ is a matrix with $\lambda _1,\ldots ,\lambda _m$ along the diagonal (each listed a number of times equal to their algebraic multiplicity as roots of the characteristic polynomial), $N$ a matrix of all $0$s except for possibly some $1$s above the super-diagonal, and $DN=ND$, where $d\coloneqq \dim (V^{\A})$  Plugging $D+N$ into $p$, we see that if $N\neq 0$, then $p(D+N)\neq 0$:  a contradiction.  Therefore, $T\colon V^{\A}\rightarrow V^{\A}$ is diagonalizable.
		
		\blankline
		\noindent
		$(\cref{semisimple_linear_operator.ii}\Rightarrow \cref{semisimple_linear_operator.i})$ Suppose that $T\colon V^{\A}\rightarrow V^{\A}$ is diagonalizable.  Let $\lambda _1,\ldots ,\lambda _m$ denote the distinct eigenvalues of $T$.  Then, $(T-\lambda _1)\cdots (T-\lambda _m)=0$, and so the minimal polynomial of $T$ cannot have roots of multiplicity more than $1$, i.e.~is separable.
		
		\blankline
		\noindent
		$(\cref{semisimple_linear_operator.i}\Rightarrow \cref{semisimple_linear_operator.iii})$ Suppose that the minimal polynomial of $T$ is separable.  Let $\F$ be the splitting field of the minimal polynomial of $T$ over $\K$.  As the minimal polynomial of $T$ is separable, the extension $\K \hookrightarrow \F$ is Galois.  
		
		As $\F$ contains the roots of the minimal polynomial of $T$, $T\colon V_{\F}\rightarrow V_{\F}$ must have an eigenvalue $\lambda$.  Let $e\in V_{\K (\lambda )}\subseteq V_{\F}$ be a corresponding eigenvector.  The orbit of $\lambda$ under the Galois group consist of precisely the roots of the minimal polynomial of $\lambda$.  In particular, the number of elements in the orbit of $\lambda$ is exactly the degree $d$ of the minimal polynomial of $\lambda$ (recall that Galois groups transitively permute the roots of irreducible polynomials).  Thus, $W'\coloneqq \Span \left\{ \sigma (e):\sigma \in \Gal (\F /\K )\right\} \subseteq V_{\F}$ has dimension $d$.  So, let $\{ b_1,\ldots ,b_d\} \subseteq V$ be such that $e=b_1+\lambda b_2+\cdots +\lambda ^{d-1}b_d$,\footnote{As $e\in V_{\Q (\lambda )}$, for any basis $\{ b_1,\ldots ,b_D\}$ of $V$, we can write $e=\sum _{i=0}^{d-1}\sum _{k=1}^Dc_{ik}\lambda ^ib_k=b_0'+\lambda b_1'+\cdots +\lambda ^{d-1}b_{d-1}'$ for some $c_{ik}\in \K$, where we have defined $b_i'\coloneqq \sum _{k=1}^Dc_{ik}b_k\in V$.} and define $W\coloneqq \Span \{ b_1,\ldots ,b_d\} \subseteq V$, so that $W_{\F}=W$.  As $T(e)=\lambda e$, it follows that\footnote{Let $\{ e,\sigma _1(e),\ldots ,\sigma _{d-1}(e)\}$ be a basis for $W'$.  We have that $\sigma _k(e)=b_1+\sigma _k(\lambda )b_1+\sigma _k(\lambda ^{d-1})b_d$ for each $\sigma _k$ ($\sigma _0\coloneqq \id$).  These $d$ equations allow us to uniquely solve for each $b_j$ in terms of the $\sigma _k(e)$s (the matrix of coefficients has to be invertible in order for the $\sigma _k(e)$s to be linearly-independent).  Applying $T$ to this equation gives $d$ equations of the form $\sigma _k(\lambda )\sigma (e)=T(b_1)+\sigma _k(\lambda )T(b_2)+\cdots +\sigma _k(\lambda ^{d-1})T(b_d)$.  Thus, similarly, the $T(b_j)$s are uniquely determined by these $d$ equations, and so, if we can find some expression for each $T(b_j)$ that satisfies these equations, then it must be the correct one.  On the other hand, as all the $d$ equations are obtained from the first by applying $\sigma _k$s, if we can find expressions for each $T(b_j)$ that satisfies the equation $\lambda e=T(b_1)+\lambda T(b_2)+\cdots +\lambda ^{d-1}T(b_d)$, then they must be the correct ones!} $T(b_1)=-c_0b_d$, $T(b_2)=b_1-c_1b_d$, $T(b_3)=b_2-c_2b_d$,\textellipsis, $T(b_d)=b_{d-1}-c_{d-1}b_d$, where $m_{\lambda}(x)=x^d+c_{d-1}x^{d-1}+\cdots +c_0$ is the minimal polynomial of $\lambda$.  (These equations ensure that $T(e)=\lambda e$).  Thus, we have that
		\begin{equation}
		[T]_{\mcal{B}}=\begin{bmatrix}0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \ddots & \vdots \\ -c_0 & -c_1 & -c_2 & \cdots & -c_{d-1}\end{bmatrix},
		\end{equation}
		where $\mcal{B}\coloneqq \{ b_1,\ldots ,b_d\}$.  Thus, $W\coloneqq \Span \{ b_1,\ldots ,b_d\}$ is a simple submodule of $V$ (note that $c_0\neq 0$).  Doing this for each orbit of eigenvalues under the Galois group, as $V$ is finite-dimensional, we can decompose $V$ into a direct sum of such simple submodules, so that $V$ is semisimple.
		
		\blankline
		\noindent
		$(\cref{semisimple_linear_operator.iii}\Rightarrow \cref{semisimple_linear_operator.ii})$ Suppose that the representation $\rho \colon \K \rightarrow \gl (V)$ defined by $\rho (1)\coloneq T$ is semisimple.  By the Jordan Canonical Form Theorem, we can write $T=S+N$ for unique linear maps $S,N\colon V\rightarrow V$ that satisfy (i) $S\colon V^{\A}\rightarrow V^{\A}$ is diagonalizable, (ii) $N$ is nilpotent, and (iii) $SN=NS$.  Thus, to prove the result, it suffices to show that $N=0$.  Write $V\coloneqq V_1\oplus \cdots \oplus V_m$, where each $V_k$ is a simple subrepresentation of $V$.  Thus, $T(V_k)\subseteq V_k$, and no nonzero proper subspace of $V_k$ has this property.  For each $k$, we can apply the Jordan Canonical Form Theorem again to obtain $\restr{T}{V_k}=S_k+N_k$.  From uniqueness, we must have that $S=S_1\oplus \cdots \oplus S_m$ and $N=S_1\oplus \cdots \oplus S_m$.  Thus, each $V_k$ is in fact invariant under both $S$ and $N$ as well.  Define $W_k\coloneqq N(V_k)\subseteq V_k$.  $W_k$ is of course invariant under $N$, and so $S$ and $N$ commute, it is also invariant under $S$, and hence invariant under $T$.  Therefore, either $W_k=V_k$ or $W_k=0$.  If $W_k\coloneqq N(V_k)=V_k$, then $N$ will not be nilpotent.  Therefore, we must have that $W_k\coloneqq N(V_k)=0$, whence it follows that $N=0$, as desired.
		
		\blankline
		\noindent
		$(\cref{semisimple_linear_operator.iii}\Rightarrow \cref{semisimple_linear_operator.iv})$ Suppose that the representation $\rho \colon\K \rightarrow \gl (V)$ defined by $\rho (1)\coloneqq T$ is semisimple.  Write $V=V_1\oplus \cdots \oplus V_m$ for simple subrepresentations $V_k\subseteq V$.  Let $W\subseteq V$ be a $T$-invariant subspace.  Then, reordering the $V_k$s if necessary, without loss of generality $W=V_1\oplus \cdots \oplus V_n$ for $n\leq m$.  Then, $V_{n+1}\oplus \cdots \oplus V_m$ is a $T$-invariant complement of $W$.
		
		\blankline
		\noindent
		$(\cref{semisimple_linear_operator.iv}\Rightarrow \cref{semisimple_linear_operator.iii})$ Suppose that every $T$-invariant subspace has a $T$-invariant complement.  We proceed by induction of $\dim (V)$.  If $\dim (V)=1$, then $V$ is in fact simple.  Now, suppose that the result is true for all representations of $\K$ defined by $1\mapsto T$ of dimension at most $d-1$, and let suppose that $\dim (V)=d$.  If $V$ itself is simple, we are done.  Otherwise, there is a proper nonzero $T$-invariant subspace $U\subset V$.  By hypothesis, $U$ has a $T$-invariant complement $W$, and so we have $V=U\oplus W$ as representations.  By the induction hypothesis, $U$ and $W$ are both semisimple, and so $V$ is likewise semisimple.
	\end{proof}
\end{thm}