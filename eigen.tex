\chapter{``Eigensuff''}

\section{Motivation}\label{sct4.1}

As mentioned at the very beginning, linear algebra is the study of vector spaces.  More accurately, it is the study of the \emph{category} of vector spaces.  Thus, not only are we interested in vector spaces, but we're also interested in studying linear-transformations.  In fact, linear-transformations are arguably more important than the vector spaces themselves.

We saw in the last chapter that, given choice of bases, we can associate matrices to linear-transformations.  This is incredibly useful as matrices are more concrete and amenable to (scary) things like computation.  However, there isn't just one matrix associated to a linear-transformation, but you get one matrix for every choice of bases.  Some of these matrices might be quite ugly, and others might be quite nice.  The motivating objective for this chapter is to try to find basis in which the associated matrix is \emph{nice}.

Perhaps the simplest matrix one might hope for is a \emph{diagonal} matrix.\footnote{It turns out that one can almost, but not quite, do this.  In any case, having this as an objective, even if it can't always be obtained, it sufficient for motivation.}  Let us investigate what it would require for our matrix to be diagonal.

So, let $V$ be a finite-dimensional vector space, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis for $V$, and let $T\colon V\rightarrow V$ be a linear operator.  Looking back to the defining theorem of $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ (\cref{CoordinatesLinearTransformation}), we see that the $k^{\text{th}}$ column of this matrix is given by
\begin{equation}
	\coordinates{T(b_k)}{\basis{B}},
\end{equation}
that is, the coordinates of the vector $T(b_k)\in V$ with respect to the basis $\basis{B}$.  To compute these coordinates (\cref{CoordinatesVector}), we write $T(b_k)$ as a linear combination of the elements of $\basis{B}$:
\begin{equation}\label{eqn4.1.2}
	T(b_k)=T\indices{^1_k}b_1+\cdots +T\indices{^d_k}b_d,
\end{equation}
so that
\begin{equation}
	\coordinates{T(b_k)}{\basis{B}}=\begin{bmatrix}T\indices{^1_k} \\ \vdots \\ T\indices{^d_k}\end{bmatrix}.
\end{equation}
Now, for a matrix to be diagonal, by definition, everything in the $k^{\text{th}}$ column should vanish except for possibly the $k^{\text{th}}$ entry in that column.  Thus, as $\coordinates{T(b_k)}{\basis{B}}$ is the $k^{\text{th}}$ column of $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$, if this matrix is to be diagonal, we had better have that $T\indices{^l_k}=0$ for $l\neq k$.  Plugging this back into \eqref{eqn4.1.2}, we find
\begin{equation}
	T(b_k)=T\indices{^k_k}b_k.
\end{equation}
We have found the following.
\begin{displayquote}
	If $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ is to be a diagonal matrix, it had better be the case that $T(b_k)$ is a scalar multiple of $b_k$ for all $b_k\in \basis{B}$.
\end{displayquote}

It is thus of interest to find a basis consisting of vectors $b$ such that $T(b)$ is a scalar multiple of $b$.  Such vectors have a name:  they are \emph{eigenvectors} of $T$.

\section{Basic definitions}

\begin{dfn}{Eigenspaces, eigenvalues, and eigenvectors}{Eigen}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $W\subseteq V$ be a nonzero subspace, and let $\lambda \in \K$.  Then, $W$ is a \term{$\lambda$-eigenspace}\index{Eigenspace} iff
	\begin{enumerate}
		\item \label{Eigen(i)}$\restr{T}{W}=\lambda \id _W$; and
		\item \label{Eigen(ii)}$W$ is maximal with this property.
	\end{enumerate}
	\begin{rmk}
		Such a $\lambda$ is referred to as an \term{eigenvalue}\index{Eigenvalue} of $T$.
	\end{rmk}
	\begin{rmk}
		Nonzero elements of $W$ are \term{eigenvectors}\index{Eigenvector} of $T$ with eigenvalue $\lambda$.
	\end{rmk}
	\begin{rmk}
		Warning:  The eigenspace is \emph{not} the set of eigenvectors---we must have $0\in W$ as $W$ is a subspace, but, by definition, $0\in W$ is forbidden from being an eigenvector.
	\end{rmk}
	\begin{rmk}
		Warning:  Eigenvalues need not be unique---see \cref{exm4.2.12}.  On the other hand, they certainly will be for vector spaces---see \cref{prp4.2.13}.
		
		Eigenvectors on the other hand are essentially never unique:  any scalar multiple of an eigenvector is another eigenvector with the same eigenvalue (because eigenspaces are in particular subspaces).
	\end{rmk}
	\begin{rmk}
		To clarify, \cref{Eigen(ii)} means the following.  If $U\subseteq V$ is a nonzero subspace with $U\supseteq W$ and for which there is some $\mu \in \K$ such that $\restr{T}{U}=\mu \id _U$, then $U=W$.
	\end{rmk}
	\begin{rmk}
		Recall that (\cref{exm1.1.36}) scaling by elements of $\K$ need not actually define a linear-transformation.  Thus, it is implicit in this condition that $\lambda \id _W\colon W\rightarrow W$ is actually a linear-transformation.  If $V$ is a vector space, this will actually force $\lambda$ to commute with everything in $\K$ (in which case scaling by $\lambda$ defines a linear-transformation on all of $V$)---see \cref{thm4.2.17}.
	\end{rmk}
	\begin{rmk}
		One reason to see we need the maximality condition is to get uniqueness of the eigenspace.  For example, consider the linear-transformation $\R ^2\rightarrow \R ^2$ defined by the matrix
		\begin{equation}
			\begin{bmatrix}2 & 0 \\ 0 & 2\end{bmatrix}.
		\end{equation}
		Without the maximality condition, every nonzero subspace would be considered a $2$-eigenspace.  But that's silly.  We want our definition to be so that, in this example, there is only one $2$-eigenspace, namely all of $\R ^2$.
	\end{rmk}
	\begin{rmk}
		If we allowed $W=0$, then we might have stupid things happening like every scalar being an eigenvalue of $T$.  Thus, unless you're a fan of stupid things, you're going to want $W=0$ here.
	\end{rmk}
	\begin{rmk}
		The \term{eigenspaces}, \term{eigenvalues}, and \term{eigenvectors} of a matrix are respectively the eigenspaces, eigenvalues, and eigenvectors of the associated linear-transformation.
	\end{rmk}
\end{dfn}
Before moving on, there is a common mistake that students make that we should clear up.
\begin{displayquote}
	Eigen\emph{vectors} cannot be $0$.  On the other hand, eigen\emph{values} can be.
	
	This is of course true just by definition.  The reason we exclude the zero vector from being an eigenvector however is because we would always have $T(0)=\lambda \cdot 0$ for all $\lambda \in \K$, not a particularly interesting condition.
\end{displayquote}

Our first order of business is to establish the uniqueness of eigenspaces, which will allow us in particular to utilize unambiguous notation to denote them.
\begin{prp}{Eigenspaces are unique}{prp4.2.3}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $\lambda \in \K$, and let $U,W\subseteq V$ be $\lambda$-eigenspaces of $T$.  Then, $U=W$.
	\begin{rmk}
		We denote the unique $\lambda$-eigenspace of $V$ by $\Eig _{\lambda ,T}$\index[notation]{$\Eig _{\lambda ,T}$}.  If $T$ is clear from context, we may simply write $\Eig _{\lambda}\ceqq \Eig _{\lambda ,T}$\index[notation]{$\Eig _{\lambda ,T}$}.
	\end{rmk}
	\begin{rmk}
		If ever we write ``$\Eig _{\lambda ,T}$'' without having explicitly said so, it should be assumed that $\lambda$ is an eigenvalue of $T$.
	\end{rmk}
	\begin{rmk}
		Warning:  Though eigenvalues themselves may still not be unique---we may still have $\Eig _{\lambda ,T}=\Eig _{\mu ,T}$ for $\lambda \neq \mu$.
	\end{rmk}
	\begin{rmk}
		Of course, $\lambda$-eigenspaces need not exist---this only says that, \emph{if} they do exist, they must be unique.
	\end{rmk}
	\begin{proof}
		$U+W$ is a subspace of $V$.  Furthermore, for $u+w\in U+W$, with $u\in U$ and $w\in W$, we have
		\begin{equation}
			T(u+w)=T(u)+T(w)=\lambda u+\lambda w=\lambda (u+w),
		\end{equation}
		and so $\restr{T}{U+W}=\lambda \id _{U+W}$.  As $U+W\supseteq U,W$, by maximality of eigenspaces, we have $U=U+W=W$.
	\end{proof}
\end{prp}
\begin{prp}{Eigenspaces are maximum with their defining property}{}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $W\subseteq V$ be a subspace, and let $\lambda \in \K$ be an eigenvalue of $V$.  Then, if $\restr{T}{W}=\lambda \id _W$, then $W\subseteq \Eig _{\lambda}$.
	\begin{rmk}
		A corollary of this is that, to show that $\lambda$ is an eigenvalue, it suffices to exhibit a single nonzero subspace $W$ such that $\restr{T}{W}=\lambda \id _W$.  For this result implies that $\Eig _{\lambda}\subseteq W$, so that if $W$ is nonzero, so is $\Eig _{\lambda}$, in which case $\lambda$ would be an eigenvalue.
	\end{rmk}
	\begin{proof}
		Suppose that $\restr{T}{W}=\lambda \id _W$.  Define
		\begin{equation}
			\collection{U}\ceqq \{ U\subseteq V\text{ a subspace}:\restr{T}{U}=\lambda \id _U\} 
		\end{equation}
		and
		\begin{equation}
			E\ceqq \sum _{U\in \collection{U}}U.
		\end{equation}
		Every element in $E$ can be written as a finite sum $u_1+\cdots +u_m$ where $T(u_k)=\lambda u_k$ for each $u_k$, and so
		\begin{equation}
			T(u_1+\cdots +u_m)=\lambda (u_1+\cdots +u_m).
		\end{equation}
		That is, $\restr{T}{E}=\lambda \id _E$.  $E$ is maximal with this property, and hence $E=\Eig _{\lambda}$.  Finally, note that $W\in \collection{U}$, and hence $W\subseteq E=\Eig _{\lambda}$. 
	\end{proof}
\end{prp}
\begin{exm}{An eigenspace with infinitely many distinct eigenvalues}{exm4.2.12}
	Define $V\ceqq \Z /2\Z$, regard it as a $\K \ceqq \Z$-module, and define $T\ceqq 0$.  Then, of course $\restr{T}{V}=0\id _V$, but also $\restr{T}{V}=2\id _V$.  (It is obviously maximal as this is the entire space!)  Thus, $V\subseteq V$ is an eigenspace of $T$ with eigenvalues $0,2\in \K$.  Of course, $2$ is not special---any even integer would have worked just as well, and so there are in fact infinitely many distinct eigenvalues!
\end{exm}
\begin{prp}{Eigenvalues are unique (in vector spaces)}{prp4.2.13}
	Let $V$ be a vector space and let $T\colon V\rightarrow V$ be linear.  Then, if $\Eig _{\lambda ,T}=\Eig _{\mu ,T}$, it follows that $\lambda =\mu$.
	\begin{proof}
		Suppose that $\Eig _{\lambda ,T}=\Eig _{\mu ,T}$.  There must be some nonzero $v\in \Eig _{\lambda ,T}=\Eig _{\mu ,T}$.  It follows that
		\begin{equation}
			\lambda v=T(v)=\mu v,
		\end{equation}
		and so $(\lambda -\mu )v=0$.  As $v\neq 0$ and the ground ring is a division ring, we must have that $\lambda -\mu =0$, that is, $\lambda =\mu$.
	\end{proof}
\end{prp}

If you've seen eigenvalues and eigenvectors before, you'll note that this is similar, but not identical to how they are usually defined.  The motivation for this slight deviation has to do with the fact that, even for vector spaces, the ground division ring $\F$ need not be commutative.  In this case, we would like eigenvalues to define linear-transformations by scaling (for reasons we'll see in a moment), even if $\K$ itself is not commutative.  The definition as stated above \emph{implies} that eigenvalues have to be central, and hence define linear-transformations by scaling.  Thus, we don't have to put this condition into the definition by hand---in a sense, we get it for free.

Another advantage it has is that it places the emphasis on eigen\emph{spaces} instead of eigen\emph{vectors}.  Eigenspaces are the more fundamental of the two concepts simply because they are unique---in general, there will be infinitely many eigenvectors with the same eigenvalue, and no one of them is more `correct' than the other, whereas for eigen\emph{spaces} there is no arbitrary choice to be made---there's only one thing to pick from.  In any case, it would be good to know that our presentation is equivalent to the usual one.
\begin{thm}{}{thm4.2.17}
	$V$ be a vector space over a division ring $\F$, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \F$.  Then,
	\begin{enumerate}
		\item \label{thm4.2.17(i)}$\lambda$ is an eigenvalue of $T$ iff $\lambda$ is central and there is some nonzero $v\in V$ such that $T(v)=\lambda v$, in which case $v$ is an eigenvector of $T$ with eigenvalue $\lambda$; and
		\item \label{thm4.2.17(ii)}if $\lambda$ is an eigenvalue, then
		\begin{equation}
			\Eig _{\lambda ,T}=\{ v\in V:T(v)=\lambda v\} =\Ker (T-\lambda ).
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		Recall (\cref{Rg}) that for $\lambda$ to be \emph{central} means that $\lambda$ commutes with everything.  For example, in the quaternions $\H$, $\im ,\jm ,\km \in \H$ are \emph{not} central, though $1\in \H$ is.
		
		Of course, if $\F$ is commutative (that is, if $\F$ is a field), then everything is central, and may ignore this part of the result.  Upon doing so, we obtain exactly the condition you are likely familiar with (assuming you've seen eigenvalues before, that is).
	\end{rmk}
	\begin{rmk}
		Perhaps the most relevant fact for us about central elements is that scaling by them defines linear-transformations.  That is, if $\alpha \in \F$ is central, then $V\ni v\mapsto \alpha \cdot v\in V$ is a linear-transformation.
	\end{rmk}
	\begin{rmk}
		In particular, note that, if $0$ is an eigenvalue, then its eigenspace is precisely $\Ker (T)$.  Thus, $T$ is injective iff $0$ is \emph{not} an eigenvalue.
	\end{rmk}
	\begin{proof}
		\cref{thm4.2.17(i)} $(\Rightarrow )$ Suppose that $\lambda$ is an eigenvalue of $T$.  As $\lambda$ is an eigenvalue, there is a nonzero subspace $\Eig _{\lambda}\subseteq V$, such that $T(v)=\lambda v$ for all $v\in \Eig _{\lambda}$.  In particular, there is \emph{some} nonzero $v\in V$ such that $T(v)=\lambda v$.  By definition, $v$ is an eigenvector with eigenvalue $\lambda$.
		
		We now check that $\lambda$ is central.  Let $\alpha \in \F$.  Then,
		\begin{equation}
			(\alpha \lambda )\cdot v=\alpha (\lambda v)=\alpha T(v)=T(\alpha v)=\footnote{If $v\in \Eig _{\lambda}$, $\alpha v\in \Eig _{\lambda}$ as well because this is a subspace.}\lambda (\alpha v)=(\lambda \alpha )\cdot v.
		\end{equation}
		As $v\neq 0$ and we are working over a division ring, it follows that $\alpha \lambda =\lambda \alpha$, that is, $\lambda$ is central.
		
		\blni
		$(\Leftarrow )$ Suppose that $\lambda$ is central and that there is some nonzero $v\in V$ such that $T(v)=\lambda v$.  Define
		\begin{equation}
			W\ceqq \{ v\in V:T(v)=\lambda v\} .
		\end{equation}
		As $\lambda$ is central, this is a subspace.  It is furthermore a nonzero subspace by hypothesis.  By definition, $\restr{T}{W}=\lambda \id _W$.
		
		To show maximality, let $U\supseteq W$ be a subspace such that $\restr{T}{U}=\lambda \id _U$.  We then of course have that $T(u)=\lambda u$ for all $u\in U$, that is, $U\subseteq W$, showing maximality.  Hence,
		\begin{equation}\label{eqn4.2.22}
			W\ceqq \{ v\in V:T(v)=\lambda v\} =\Eig _{\lambda ,T}.
		\end{equation}
		In particular, $\lambda$ is an eigenvalue of $T$.
		
		\blni
		\cref{thm4.2.17(ii)} Note that this has already been proven in \eqref{eqn4.2.22}.
	\end{proof}
\end{thm}

\begin{exm}{}{}
	Define $D\colon C^{\infty}(\R )\rightarrow C^{\infty}(\R )$ by $D(f)\ceqq f'$.  Then, $\lambda \in \C$ is an eigenvalue of $f$ iff $f$ is nonzero and $f'\eqqc D(f)=\lambda f$.  If this is true, it follows that $f(x)=C\exp (\lambda x)$ for some constant $C\in \C$, and hence that the function $x\mapsto \exp (\lambda x)$ is an eigenvector of $D$ with eigenvalue $\lambda$.  Thus, every $\lambda \in \C$ is an eigenvalue and
	\begin{equation}
		\Eig _{\lambda}=\Span (\exp (\lambda x)).
	\end{equation}
\end{exm}
\begin{exm}{}{}
	Let $T\colon \R ^2\rightarrow \R ^2$ be the linear-transformation that is counter-clockwise rotation about the origin by $\uppi /2$.  If $v\in \R ^2$ and $T(v)=\lambda v$ for some $\lambda \in \R$, then the rotate of $v$ would have to be parallel to $v$.  This is impossible unless $v=0$.  Thus, this linear-transformation has no eigenvalues.
\end{exm}
\begin{exm}{}{}
	Let $A$ be an upper-triangular matrix with entries in a field.  Then, the eigenvalues of $A$ are the scalars appearing on the diagonal of $A$.
	\begin{exr}[breakable=false]{}{}
		Check this.
	\end{exr}
\end{exm}

\begin{prp}{Eigenvectors with distinct eigenvalues are linearly-independent}{prp4.2.22}
	Let $V$ be a vector space, let $T\colon V\rightarrow V$ be linear, let $\lambda _1,\ldots ,\lambda _m$ be distinct eigenvalues of $T$, and let $v_k\in \Eig _{\lambda _k}$ be nonzero $1\leq k\leq m$.  Then, $\{ v_1,\ldots ,v_m\}$ is linearly-independent.
	\begin{rmk}
		When we investigate generalized-eigenvectors, we will encounter a strict generalization of this---see \cref{prp4.4.157}.
	\end{rmk}
	\begin{proof}
		Denote the ground division ring by $\F$.  We proceed by induction on $m$.  The $m=0$ case is immediate as eigenvectors are nonzero.  So, let $m\in \Z ^+$ and suppose the result is true $m-1$.  To prove the result for $m$, we proceed by contradiction:  suppose that $\{ v_1,\ldots ,v_m\}$ is linearly-dependent.  By \cref{prp1.2.28}, there is some $v_k$ such that
		\begin{equation}
			v_k\in \Span (v_1,\ldots ,v_{k-1}).
		\end{equation}
		Without loss of generality, we may assume that $k$ is the smallest such integer.  Thus, there are $\alpha _1,\ldots ,\alpha _{k-1}\in \F$ such that
		\begin{equation}\label{eqn4.2.25}
			v_k=\alpha _1\cdot v_1+\cdots +\alpha _{k-1}\cdot v_{k-1}.
		\end{equation}
		Applying $T$ to this equation yields
		\begin{equation}\label{eqn4.2.26}
			\lambda _kv_k=\alpha _1\lambda _1v_1+\cdots +\alpha _{k-1}\lambda _{k-1}v_{k-1}.
		\end{equation}
		Multiplying \eqref{eqn4.2.25} by $\lambda _k$ and subtracting the result from \eqref{eqn4.2.26} yields\footnote{Note that I am allowed to commute $\lambda _k$ past the $\alpha _i$s as $\lambda _k$ is central.}
		\begin{equation}
			0=\alpha _1(\lambda _1-\lambda _k)v_1+\cdots +\alpha _{k-1}(\lambda _{k-1}-\lambda _k).
		\end{equation}
		Now, by the induction hypothesis, it follows that $\alpha _i(\lambda _i-\lambda _k)=0$ for all $1\leq i\leq k-1$.  If some $\alpha _i\neq 0$, then it would follows that $\lambda _i=\lambda _k$:  a contradiction of the fact that the eigenvalues are distinct.  Thus, we must have that $\alpha _i=0$ for all $1\leq i\leq k-1$, whence it follows from \eqref{eqn4.2.25} that $v_k=0$, which itself is a contradiction (eigenvectors can't be $0$).  Thus, it must have been the case that $\{ v_1,\ldots ,v_m\}$ was linearly-independent, as desired.
	\end{proof}
\end{prp}
\begin{crl}{}{crl4.2.28}
	Let $V$ be a vector space, let $T\colon V\rightarrow V$ be linear, and let $\mcal{E}$ denote the set of eigenvalues of $T$.  Then, $\abs{\mcal{E}}\leq \dim (V)$.
	\begin{rmk}
		In words, the number of eigenvalues is at most the dimension.
	\end{rmk}
	\begin{proof}
		For every $\lambda \in \mcal{E}$, let $v_{\lambda}\in \Eig _{\lambda}$ be an eigenvector.  By the previous result, $\{ v_{\lambda}:\lambda \in \mcal{E}\}$ is linearly-independent, and so\footnote{By \cref{prp1.2.66}---this is the result that says linearly-independent sets have cardinality at most the cardinality of any spanning set.}
		\begin{equation}
			\abs{\mcal{E}}=\abs*{\{ v_{\lambda}:\lambda \in \mcal{E}\}}\leq \dim (V).
		\end{equation}
	\end{proof}
\end{crl}

Let $V$ be a vector space over a division ring $\F$ and let $T\colon V\rightarrow V$ be a linear-transformation.  In \cref{exm1.1.34}, we saw that we could make $V$ into an $\F [x]$-module, by having a polynomial $p$ act on $V$ by the $\F$-linear-transformation $p(T)$.  In the following result, we examine how such operators act on eigenvectors.
\begin{prp}{}{prp4.2.33}
	Let $V$ be a vector space over a division ring $\F$, let $T\colon V\rightarrow V$ be linear, let $p\in \F [x]$ be a polynomial, and let $\lambda \in \F$ be an eigenvalue of $T$ with eigenvector $v\in V$.  Then,
	\begin{equation}
		[p(T)](v)=p(\lambda )v.
	\end{equation}
	\begin{rmk}
		Indeed, note that you can view the equation $T(v)=\lambda v$ as the special case of this result for the polynomial $p(x)\ceqq x$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{crl}{}{crl4.2.36}
	Let $V$ be a vector space over a division ring $\F$, let $T\colon V\rightarrow V$ be linear, and let $p\in \F [x]$ be a polynomial.  Then, if $p(T)=0$, then $p(\lambda )=0$ for every eigenvalue $\lambda \in \F$ of $T$.
	\begin{rmk}
		This can have practical applications for actually \emph{computing} eigenvalues.  For example, if for whatever reason you know that $T^2-1=0$, then you know that $\lambda ^2-1=0$ for any eigenvalue $\lambda \in \F$ of $T$.  Thus, in this case, one could deduce that the only possible eigenvalues for $T$ were $\pm 1$.
	\end{rmk}
	\begin{proof}
		Suppose that $p(T)=0$.  Let $\lambda \in \F$ be an eigenvalue of $T$ with eigenvector $v\in V$.  By the previous result, we have that
		\begin{equation}
			0=[p(T)](v)=p(\lambda )v.
		\end{equation}
		As $v$ is nonzero, it follows that $p(\lambda )=0$.
	\end{proof}
\end{crl}
\begin{exm}{}{}
	For example, suppose that $3\in \C$ is an eigenvalue of $T$ with eigenvector $v\in V$, so that $T(v)=3v$.  If $p(x)\ceqq -5x^2+2x+1$, then we would have
	\begin{equation}
		[-5T^2+2T+\id ](v)\eqqc [p(T)](v)=p(3)v=[-5\cdot 9+2\cdot 3+1]v=-38v.
	\end{equation}
\end{exm}

\section{Diagonalization}

We started this chapter with the objective of finding a basis for which the coordinates of a given linear-transformation was a diagonal matrix.  We now have the tools to discuss this.
\begin{dfn}{Diagonalizable}{Diagonalizable}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, $T$ is \term{diagonalizable}\index{Diagonalizable} iff there is a basis $\basis{B}$ of $T$ such that $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ is a diagonal matrix.
	\begin{rmk}
		In this case, $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ is the \term{diagonalization}\index{Diagonalization} of $T$.
	\end{rmk}
	\begin{rmk}
		If $A$ is a matrix, we say that $A$ is \term{diagonalizable} iff the associated linear-transformation is diagonalizable.
	\end{rmk}
\end{dfn}
The whole point of discussing the ``eigenstuff'' was, as we saw in \crefnameref{sct4.1}, that the existence of eigenvalues was related to the diagonalizability of the linear-transformation.  We now make this precise.
\begin{thm}{Fundamental Theorem of Diagonalizability}{FundamentalTheoremOfDiagonalizability}
	Let $V$ be a finite-dimensional vector space, let $T\colon V\rightarrow V$ be linear, and let $\lambda _1,\ldots ,\lambda _m$ denote the eigenvalues of $T$.\footnote{Note that there are finitely many eigenvalues by \cref{crl4.2.28}.}  Then, the following are equivalent.
	\begin{enumerate}
		\item $T$ is diagonalizable.
		\item There is a basis of $V$ consisting of eigenvectors of $T$.
		\item
		\begin{equation}
			\dim (V)=\dim (\Eig _{\lambda _1})+\cdots +\dim (\Eig _{\lambda _m}).
		\end{equation}
	\end{enumerate}
	In this case, if $\basis{B}$ is a basis of eigenvectors of $V$ consisting of eigenvectors of $T$, then $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ is a diagonal matrix.
	\begin{rmk}
		There is another important condition equivalent to these three which we can't quite state yet as we don't know what direct-sums are.  In brief, it says that a linear operator is diagonalizable iff $V$ is the direct-sum of its eigenspaces---see the remark in \cref{prp4.4.13}.
	\end{rmk}
	\begin{rmk}
		In particular, as eigenspaces are of course at least one-dimensional, if $T$ has $\dim (V)$ distinct eigenvalues, then $T$ must be diagonalizable.  That said, the converse is not true---see \cref{exm4.3.4}.
	\end{rmk}
	\begin{rmk}
		Warning:  This name is nonstandard---there is no standard name for this result.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
\begin{exm}{}{}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & 6\end{bmatrix}.
	\end{equation}
	As $A$ is upper-triangular, the eigenvalues are the elements on the diagonal  $1$, $4$, and $6$.  Thus, as $A$ has $3$ distinct eigenvalues, it must be diagonalizable.
\end{exm}
Diagonalizability is yet another context where the ground division ring matters.
\begin{exm}{A matrix diagonalizable over $\C$ but not over $\R$}{exm4.3.7}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}.
	\end{equation}
	This matrix defines a linear-transformation $\R ^2\rightarrow \R ^2$ and also $\C ^2\rightarrow \C ^2$.  In either case, if $\lambda$ is an eigenvalue, then it must be the case that
	\begin{equation}
		\lambda ^2+1=0.
	\end{equation}
	\begin{exr}[breakable=false]{}{}
		Check this, that is, show that in both case $F=\R ,\C$, $\lambda \in \F$ is an eigenvalue iff $\lambda ^2+1=0$.
	\end{exr}

	Thus, if the ground field is $\R$, there are no eigenvalues.  On the other hand, if the ground field in $\C$, then there are two distinct eigenvalues, $\pm 1$.  Thus, according to \namerefpcref{FundamentalTheoremOfDiagonalizability}, this linear-transformation is diagonalizable over $\C$ but not over $\R$.
\end{exm}
The phenomenon we observed in the previous example is quite general, so it's worth it to take the time to point out its significance.
\begin{displayquote}
	As we will see later, it turns out that to every linear operator, there is an associated polynomial, the \emph{characteristic polynomial}, that has the property that a scalar is an eigenvalue iff it is a root of that polynomial. 
\end{displayquote}
In the previous example, the characteristic polynomial was $\lambda ^2+1$.  Thus, if the characteristic polynomial doesn't have any roots, the linear operator doesn't have any eigenvalues.  Thus, we observe the following.
\begin{displayquote}
	If order to guarantee that all linear operators have an eigenvalue, we're going to want to require all polynomials to have at least one root.
\end{displayquote}
The property that all polynomials have a root is an important one called \emph{algebraically-closed}, and we will return to it later.  For now, however, we resume discussion of examples regarding diagonalizability.
\begin{exm}{A diagonalizable linear-transformation with fewer eigenvalues than the dimension}{exm4.3.4}
	The identity $\R ^2\rightarrow \R ^2$ has a single eigenvalue ($1$), but yet it is diagonalizable (with respect to the standard basis).
\end{exm}
\begin{exm}{A matrix over $\C$ that is not diagonalizable}{}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}0 & 1 \\ 0 & 0\end{bmatrix}.
	\end{equation}
	There is only one eigenvalue, $0$.  The eigenspace is the null-space of $A-0=A$, which is just
	\begin{equation}
		\Span \left( \begin{bmatrix}1 \\ 0\end{bmatrix}\right) .
	\end{equation}
	As the sum of the dimensions of the eigenspaces is only $1$, $A$ cannot be diagonalizable.
\end{exm}
\begin{exr}{}{}
	Define $V\ceqq \{ f\in C^{\infty}(\R ):f''+f=0\}$ and define $D\colon V\rightarrow V$ by $D(f)\ceqq f'$.  Show that $D$ is diagonalizable by exhibiting a basis of $V$ with respect to which the coordinates of $D$ is a diagonal matrix.
\end{exr}

\subsection{Diagonalization of matrix linear-transformations}

While the concept of diagonalization makes sense for any linear-transformation, there is an extra question one might ask if the linear-transformation happens to be defined by a matrix:  what is the relationship between the original matrix and its diagonalization?  The answer to this question is given by the following result.
\begin{prp}{}{}
	Let $A$ be an $m\times m$ diagonalizable matrix with entries in a division ring $\F$ and let $\basis{B}$ be a basis of eigenvectors of $A$.  Then,
	\begin{equation}\label{eqn4.3.12}
		\coordinates{A}{\basis{B}\leftarrow \basis{B}}=\coordinates{\id}{\basis{B}\leftarrow \basis{S}}\coordinates{A}{\basis{S}\leftarrow \basis{S}}\coordinates{\id}{\basis{S}\leftarrow \basis{B}},
	\end{equation}
	where $\basis{S}$ is the standard basis of $\F ^m$.
	\begin{rmk}
		Note that $\coordinates{A}{\basis{S}\leftarrow \basis{S}}=A$ by \cref{prp3.2.100}.\footnote{I wrote \eqref{eqn4.3.12} using $\coordinates{A}{\basis{S}\leftarrow \basis{S}}$ instead of $A$ because I feel as if writing it this way makes it more `obviously' true.}
		
		Furthermore, $\coordinates{A}{\basis{B}\leftarrow \basis{B}}$ is the diagonalization of $A$ (by definition) and $\coordinates{\id}{\basis{B}\leftarrow \basis{S}}=\coordinates{\id}{\basis{S}\leftarrow \basis{B}}^{-1}$.  Thus, writing $D\ceqq \coordinates{A}{\basis{B}\leftarrow \basis{B}}$ and $P\ceqq \coordinates{\id}{\basis{S}\leftarrow \basis{B}}$, this equation is sometimes written more concisely (but perhaps less transparently) as
		\begin{equation}
			D=P^{-1}AP.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		Note that, by \cref{CoordinatesLinearTransformation}, the columns of $\coordinates{\id}{\basis{S}\leftarrow \basis{B}}$ are given by $\coordinates{b_k}{\basis{S}}$ for $b_k\in \basis{B}$.  However, by \cref{prp3.1.17},\footnote{This is the result that says the coordinates of a column vector with respect to the standard basis is just that original column vector.} this is just $b_k$ itself.  Thus,
		\begin{displayquote}
				\emph{$P\ceqq \coordinates{\id}{\basis{S}\leftarrow \basis{B}}$ is the matrix whose columns are the eigenvectors of $A$.}
		\end{displayquote}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result yourself.
		\end{exr}
	\end{proof}
\end{prp}

\section{Jordan Canonical Form}

Diagonalizable linear-transformations are great.  There's just one little itsy-bitsy problem:  not all linear-transformations are diagonalizable!  Not even in the context of finite-dimensional vector space over algebraically-closed\footnote{Whatever that means.} fields.  Fortunately, there is something that is almost as good, but \emph{always} exist (at least if the ground ring is sufficiently nice):  the \emph{Jordan Canonical Form} of a linear-transformation.

Before we get there however, we must first take a detour to discuss \emph{direct-sums} and \emph{invariant-subspaces}.  Very roughly speaking, the idea is to decompose a vector space into smaller pieces that are easier to study.  For example, if $b\in V$ is an eigenvector of $T$, then $\Span (b)$ the behavior of $T$ on this subspace is particularly easy to understand.  While we can't quite have the subspaces be that simple in general, we will find that we can still decompose our space into pieces which are not that much more complicated.

\subsection{Direct-sums}

The precise sense in which we are going to decompose vector spaces is called the \emph{direct-sum} and is defined as follows.
\begin{dfn}{Direct-sum}{DirectSum}
	Let $V$ be a $\K$-module, and let $\collection{W}$ be a collection of subspaces of $V$.  Then, $V$ is the \term{direct-sum}\index{Direct-sum} of the elements of $\collection{W}$ iff for every $v\in V$ there are unique $v^W\in W$ such that
	\begin{equation}\label{eqn4.4.2}
		v=\sum _{W\in \collection{W}}v^W.
	\end{equation}
	\begin{rmk}
		In this case, we write
		\begin{equation}
			V=\bigoplus _{W\in \collection{W}}W.
		\end{equation}\index[notation]{$\bigoplus _{W\in \collection{W}}W$}
	\end{rmk}
	\begin{rmk}
		We say that
		\begin{equation}
			V=\bigoplus _{W\in \collection{W}}W
		\end{equation}
		or just $\collection{W}$ is a \term{direct-sum decomposition}\index{Direct-sum decomposition} or \term{decomposition}\index{Decomposition} of $V$.
	\end{rmk}
	\begin{rmk}
		It is implicit in this that cofinitely many $v^W$s vanish, so that \eqref{eqn4.4.2} is secretly a finite sum.
	\end{rmk}
	\begin{rmk}
		As per usual, the arbitrary (nonfinite) case obfuscates the simplicity of this concept.  If $\collection{W}=\{ W_1,\ldots ,W_m\}$, then this definition reads:  $V$ is the \term{direct-sum} of $W_1,\ldots ,W_m$ iff for every $v\in V$ there are unique $v^1\in W,\ldots ,v^m\in W_m$ such that
		\begin{equation}
			v=v^1+\cdots +v^m,
		\end{equation}
		in which case we write
		\begin{equation}
			V=W_1\oplus \cdots \oplus W_m.
		\end{equation}\index[notation]{$W_1\oplus \cdots \oplus W_m$}
	\end{rmk}
	\begin{rmk}
		Note how this definition is similar in ways to that of a basis---see \cref{dfn4.4.6,exr4.4.9}.
	\end{rmk}
\end{dfn}
We will see that a given direct-sum decomposition is in many ways similar to the specification of a basis.  Indeed, there is a sense in which it is a generalization---see \cref{exr4.4.9}.  The notions of \emph{linear-independence} and \emph{spanning} generalized to the context of subspaces as well, and, as with bases, these concepts will help us understand direct-sum decompositions.
\begin{thm}{Span (of subspaces)}{}
	Let $V$ be a $\K$-module and let $\collection{W}$ be a collection of subspaces of $V$.  Then, there is a unique subspace of $V$, the \term{span}\index{Span (of subspaces)} of $\collection{W}$, $\Span (\collection{W})$\index[notation]{$\Span (\collection{W})$}, such that
	\begin{enumerate}
		\item $W\subseteq \Span (\collection{W})$ for all $S\in \collection{W}$; and
		\item if $U\subseteq V$ is other subspace containing each $W\in \collection{W}$, it follows that $\Span (\collection{W})\subseteq U$.
	\end{enumerate}
	Furthermore, explicitly,
	\begin{equation}
		\Span (\collection{W})=\sum _{W\in \collection{W}}W.
	\end{equation}
	\begin{rmk}
		Thus, if $\collection{W}=\{ W_1,\ldots ,W_m\}$, we have that
		\begin{equation}
			\begin{split}
				\Span (W_1,\ldots ,W_m) & \ceqq \Span (\collection{W}) \\
				& =W_1+\cdots +W_m.
			\end{split}
		\end{equation}
	\end{rmk}
	\begin{rmk}
		It is incredibly uncommon to see people to write $\Span (W_1,\ldots ,W_m)$, and instead, people simply write the more explicit $W_1+\cdots +W_m$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
\begin{dfn}{Spanning (subspace)}{}
	Let $V$ be a $\K$-module and let $\collection{W}$ be a collection of subspaces of $V$.  Then, $\collection{W}$ is \term{spanning}\index{Spanning (subspaces)} iff $\Span (\collection{W})=V$.
	\begin{rmk}
		Synonymously, we also say that $\collection{W}$ \term{spans}\index{Spans (of subspaces)} $V$.
	\end{rmk}
\end{dfn}
\begin{dfn}{Linear-independence (of subspaces)}{}
	Let $V$ be a $\K$-module and let $\collection{W}$  be a collection of subspaces of $V$.  Then, $\collection{W}$ is \term{linearly-independent}\index{Linearly-independent (subspaces)} iff for all $m\in \Z ^+$, $W_1,\ldots ,W_m\in \collection{W}$, $w_k\in W_k$,
	\begin{equation}
		w_1+\cdots +w_m=0\text{ implies }w_1=0,\ldots ,w_m=0.
	\end{equation}
	$\collection{W}$ is \term{linearly-dependent}\index{Linearly-depenent (subspaces)} iff it is not linearly-independent.
\end{dfn}
Now let us see in what sense these concepts are generalizations of the ones we met before.
\begin{prp}{}{exr4.4.9}
	Let $V$ be a $\K$-module and let $\basis{B}\subseteq V$.
	\begin{enumerate}
		\item $\basis{B}$ is spanning iff $\{ \Span (b):b\in \basis{B}\}$ is spanning.
		\item $\basis{B}$ is linearly-independent iff $\{ \Span (b):b\in \basis{B}\}$ is linearly-independent.
		\item $\basis{B}$ is a basis iff
		\begin{equation}\label{eqn4.4.35}
			V=\bigoplus _{b\in \basis{B}}\Span (b).
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		In finite dimensions, with $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$, \eqref{eqn4.4.35} reads
		\begin{equation}\label{eqn4.4.23}
			V=\Span (b_1)\oplus \cdots \oplus \Span (b_d).
		\end{equation}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
This suggests the following criterion for checking whether a vector space can be written as a direct-sum.
\begin{prp}{Criterion for direct-sums of an arbitrary collection of subspaces}{prp4.4.7}
	Let $V$ be a $\K$-module, let $\collection{W}$ be a collection of subspaces of $V$.  Then,
	\begin{equation}
		V=\bigoplus _{W\in \collection{W}}W
	\end{equation}
	iff $\collection{W}$ is linearly-independent and spans $V$.
	\begin{rmk}
		Again, the case where $\collection{W}\ceqq \{ W_1,\ldots ,W_M\}$ is clearer.  It says:
		\begin{equation}
			V=W_1\oplus \cdots \oplus W_m
		\end{equation}
		iff (i) $V=W_1+\cdots +W_m$ and (ii) the equation $0=w_1+\cdots +w_m$ with $w_k\in W_k$ implies every $w_k=0$.
	\end{rmk}
	\begin{proof}
		$(\Rightarrow )$ Suppose that
		\begin{equation}
			V=\bigoplus _{W\in \collection{W}}W.
		\end{equation}
		By definition, this means that for every $v\in V$ there are unique $v^W\in W$ such that
		\begin{equation}
			v=\sum _{W\in \collection{W}}v^W.
		\end{equation}
		In particular, there are \emph{some} sum $v^W$s, and so $V=\sum _{W\in \collection{W}}W$.  Thus, $\collection{W}$ spans $V$.  For linear-independence, suppose that
		\begin{equation}
			0=\sum _{W\in \collection{W}}v^W.
		\end{equation}
		As we also have
		\begin{equation}
			0=\sum _{v\in \collection{W}}0,
		\end{equation}
		uniqueness implies that $v^W$, and so $\collection{W}$ is linearly-independent, as desired.
		
		\blni
		$(\Leftarrow )$ Suppose that $\collection{W}$ is linearly-independent and spans $V$.  Let $v\in V$.  As $\collection{W}$ spans $V$, there are $v^W\in W$ such that
		\begin{equation}
			v=\sum _{W\in \collection{W}}v^W.
		\end{equation}
		To show uniqueness, suppose that there are other $u^W\in W$ such that
		\begin{equation}
			v=\sum _{W\in \collection{W}}u^W.
		\end{equation}
		Subtracting these two expressions for $v$, we find
		\begin{equation}
			0=\sum _{W\in \collection{W}}[v^W-u^W],
		\end{equation}
		whence $v^W=u^W$ by linear-independence, giving us uniqueness, as desired.
	\end{proof}
\end{prp}
The case of two subspaces is particularly important, and in this case the criterion simplifies, and so we state it separately.
\begin{crl}{Criterion for direct-sums of two spaces}{crl4.4.11}
	Let $V$ be a $\K$-module, and let $U,W\subseteq V$ be subspaces.  Then,
	\begin{equation}
		V=U\oplus W
	\end{equation}
	iff (i) $V=U+W$ and (ii) $U\cap W=0$.
	\begin{rmk}
		Warning:  The naive generalization of this criterion to more than two subspaces is not true.  That is, if $W_1,\ldots ,W_m\subseteq V$ are subspaces such that $V=W_1+\cdots +W_m$ and $W_k\cap W_l=0$ for $k\neq l$, it is \emph{not} necessarily the case that $V=W_1\oplus \cdots \oplus W_m$---see \cref{exm4.4.13}.
	\end{rmk}
	\begin{proof}
		From the previous result, it suffices to show that $U\cap W=0$ is true iff it is true that the equation $0=u+w$ with $u\in U$ and $w\in W$ implies $u=0=w$.
		
		\blni
		$(\Rightarrow )$ Suppose that $U\cap W=0$.  Let $u\in U$, $w\in W$, and suppose that $0=u+w$.  Then, $w=-u\in U$, and so $w\in U\cap W$, and so $w=0$.  We then have $u=-w=0$.
		
		\blni
		$(\Leftarrow )$ Suppose that the equation $0=u+w$ with $u\in U$ and $w\in W$ implies $u=0=w$.  Let $v\in U\cap W$.  We have that $0=v+(-v)$, and as $v\in U$ and $-v\in W$, it follows in particular that $v=0$, and hence $U\cap W=0$.
	\end{proof}
\end{crl}
There are also more practical criterion specific to the finite-dimensional case.
\begin{prp}{}{prp4.4.13}
	Let $V$ be a finite-dimensional vector space and let $W_1,\ldots ,W_m\subseteq V$ be subspaces such that
	\begin{equation}
		\dim (V)=\dim (W_1)+\cdots +\dim (W_m).
	\end{equation}
	\begin{enumerate}
		\item If $\{ W_1,\ldots ,W_m\}$ is spanning, then $V=W_1\oplus \cdots \oplus W_m$.
		\item If $\{ W_1,\ldots ,W_m\}$ are linearly-independent, then $V=W_1\oplus \cdots \oplus W_m$.
	\end{enumerate}
	\begin{rmk}
		Thus, if the sum of the dimensions of the $W_k$s add up to that of $V$, then you only need to check one of the two usual conditions, instead of both.
	\end{rmk}
	\begin{rmk}
		Note that a corollary of this (together with \cref{FundamentalTheoremOfDiagonalizability}) is that a linear operator $V\rightarrow V$ is diagonalizable iff $V$ is the direct-sum of its eigenspaces:
		\begin{equation}
			V=\Eig _{\lambda _1}\oplus \cdots \oplus \Eig _{\lambda _m}.
		\end{equation}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exm}{$W_1,W_2,W_3\subseteq V$ subspaces with trivial pairwise intersections and $V=W_1+W_2+W_3$ but $V\neq W_1\oplus W_2\oplus W_3$}{exm4.4.13}
	Define $V\ceqq \R ^2$, $W_1\ceqq \Span (\coord{1,0})$, $W_2\ceqq \Span (\coord{0,1})$, and $W_3\ceqq \Span (\coord{1,1})$.  Then, $W_1\cap W_2=0$, $W_1\cap W_3=0$, $W_2\cap W_3=0$, $W_1+W_2+W_3=V$, but the sum is not direct:  we can write both
	\begin{equation}
		\coord{1,1}=0+0+\coord{1,1}
	\end{equation}
	and
	\begin{equation}
		\coord{1,1}=\coord{1,0}+\coord{0,1}+0.
	\end{equation}
\end{exm}
\begin{exm}{}{}
	Define
	\begin{equation}
	U=\Span \left( \begin{bmatrix}1 \\ 0 \\ 0\end{bmatrix},\begin{bmatrix}0 \\ 1 \\ 0\end{bmatrix}\right) \text{ and }W=\Span \left( \begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}\right) .
	\end{equation}
	Then, $\R ^3=U\oplus V$.
	\begin{exr}[breakable=false]{}{}
		Check this.
	\end{exr}
\end{exm}
\begin{exm}{}{}
	Define
	\begin{equation}
	V\ceqq \left\{ f\in \Mor _{\Set}(\R ^2,\R ):\lim _{\abs{x}\to \infty}f(x)\text{ exists.}\right\} .
	\end{equation}
	Then,
	\begin{equation}
	V=\left\{ f\in V:f\text{ is constant.}\right\} \oplus \left\{ f\in V:\lim _{\abs{x}\to \infty}f(x)=0\right\} .
	\end{equation}
	To see this, note that we may write
	\begin{equation}
	f=\lim _{\abs{x}\to \infty}f(x)+(f-\lim _{\abs{x}\to \infty}f(x))
	\end{equation}
	for any $f\in V$.
	\begin{exr}[breakable=false]{}{}
		Check this in more detail.
	\end{exr}
\end{exm}

In a way roughly analogous to how we can extend linearly-independent subsets to bases, we can `extend' subspaces into direct-sum decompositions, at least for vector spaces
\begin{prp}{Subspaces of vector spaces have complements}{prp4.4.24}
	Let $V$ be a vector space and let $U\subseteq V$ be a subspace.  Then, there is a subspace $W\subseteq V$ such that $V=U\oplus W$.
	\begin{rmk}
		If $V$ is a $\K$-module and $U\subseteq V$ is a subspace, then a subspace $W\subseteq V$ such that $V=U\oplus W$ is a \term{complement}\index{Complement} of $U$.
	\end{rmk}
	\begin{rmk}
		Warning:  Complements are not unique, even for vector spaces---see \cref{exm4.4.6}.
	\end{rmk}
	\begin{rmk}
		Warning:  This will fail in general for $\K$-modules---see \cref{exm4.5.15}.
	\end{rmk}
	\begin{proof}
		Let $\basis{A}$ be a basis of $U$ and extend it to a basis $\basis{B}$ of $V$.  Define $\basis{C}\ceqq \basis{B}\setminus \basis{A}$ and $W\ceqq \Span (\basis{C})$.  We certainly have
		\begin{equation}
			V=\Span (\basis{B})=\Span (\basis{A}\cup \basis{C})=\Span (\basis{A})+\Span (\basis{C})=U+W.
		\end{equation}
		On the other hand, an element in the intersection $V\cap W$ must be able to be written as a linear-combination of both elements of $\basis{A}$ and elements of $\basis{C}$.  This would then yield a nontrivial linear-dependence relation among elements of $\basis{A}\cup \basis{C}=\basis{B}$ unless the element in the intersection were $0$.  Thus, we must have $U\cap W=0$, and hence $V=U\oplus W$ by \cref{crl4.4.11}.
	\end{proof}
\end{prp}

Direct-sums allow us to define \emph{projections}.
\begin{prp}{Projection}{Projection}
	Let $V$ be a $\K$-module, and let $\collection{W}$ be a collection of subspaces of $V$ such that $V=\bigoplus _{W\in \collection{W}}W$.  Then, for every $W_0\in \collection{W}$, there is a unique linear-transformation $\proj _{W_0}\colon V\rightarrow W_0$\index[notation]{$\proj _{W_0}$}, the \term{projection}\index{Projection} onto $W$ with respect to the decomposition $V=\bigoplus _{W\in \collection{W}}W$, such that $\proj _{W_0}(v)=v^{W_0}$, where $v=\sum _{W\in \collection{W}}v^W$ for unique $v^W\in W$.
	\begin{rmk}
		For example, if $V=U\oplus W$, for $v\in V$, we can write $v=u+w$ for unique $u\in U$ and $w\in U$.  In this case, $\proj _U(v)=u$.  (Similarly, $\proj _W(v)=w$.)
	\end{rmk}
	\begin{rmk}
		Warning:  $\proj _{W_0}$ is \emph{not} uniquely determined by $W_0$ itself---it depends on the other $W$s, on the entire decomposition---see \cref{exm4.4.6} for a concrete example of this.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result yourself.
		\end{exr}
	\end{proof} 
\end{prp}
\begin{exm}{$V=U\oplus W_1=U\oplus W_2$ for $W_1\neq W_2$}{exm4.4.6}
	Define $V\ceqq \R ^2$, $U\ceqq \Span (\coord{1,0})$, $W_1\ceqq \Span (\coord{0,1})$, and $W_2\ceqq \Span (\coord{1,1})$.
	\begin{exr}[breakable=false]{}{}
		Check that $V=U\oplus W_1$ and $V=U\oplus W_2$.
	\end{exr}

	Define $v\ceqq \coord{a,b}\in \R ^2$.  The decomposition of $v$ with respect to the decomposition $V=U\oplus W_1$ is given by
	\begin{equation}
		v\ceqq \begin{bmatrix}a \\ b\end{bmatrix}=\begin{bmatrix}a \\ 0\end{bmatrix}+\begin{bmatrix}0 \\ b\end{bmatrix}.
	\end{equation}
	On the other hand, the decomposition of $v$ with respect to the decomposition $V=U\oplus W_2$ is given by
	\begin{equation}
		v\ceqq \begin{bmatrix}a \\ b\end{bmatrix}=\begin{bmatrix}a-b \\ 0\end{bmatrix}+\begin{bmatrix}b \\ b\end{bmatrix}
	\end{equation}
	Hence,
	\begin{equation}
		\proj _U(w)=\begin{bmatrix}a \\ 0\end{bmatrix}
	\end{equation}
	with respect to the first decomposition, but
	\begin{equation}
		\proj _U(w)=\begin{bmatrix}a-b \\ 0\end{bmatrix}
	\end{equation}
	with respect to the second decomposition.
\end{exm}

\subsubsection{Coordinates with respect to direct-sum decompositions}

We saw in \cref{CoordinatesVector} that, given a basis of a $\K$-module $V$, we can define the \emph{coordinates} of elements of $v$ with respect to that basis.  In a similar way, we can also talk about the coordinates of elements of $V$ with respect to a direct-sum decomposition.   In fact, the definition of coordinates we learned before is a special case of this---see \cref{exr4.4.9}.

We saw before that the coordinates of an `abstract' vector was an element of $\K ^d$, $\K$ the ground ring.  Given a direct-sum decomposition $V=W_1\oplus \cdots \oplus W_m$, the first question we must address is ``What plays the role of $\K ^d$ in this context?''.  To see what the answer should be, recall (\cref{exr4.4.9}) that, given a basis $\basis{B}$ of $V$, $V$ can be written as the direct-sum of the subspaces $\{ \Span (b):b\in \basis{B}\}$.

Over a division ring $\F$, $\Span (b)\cong _{\Vect _{\F}}\F$ via the map $\F \ni \alpha \mapsto \alpha \cdot b\in \Span (b)$.  Thus, if $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ is a basis for the vector space $V$, after identifying each $\Span (b_k)$ with $\F$, \eqref{eqn4.4.23} reads
\begin{equation}
	V\cong \underbrace{\F \oplus \cdots \oplus \F}_d.
\end{equation}
On the other hand, we know from \cref{prp3.1.7} that the map that sends an ``abstract'' vector to its coordinates yields an isomorphism
\begin{equation}
	V\cong \F ^d\ceqq \underbrace{\F \times \cdots \times \F}_d.
\end{equation}
These two facts together suggest that in the general case $V=W_1\oplus \cdots \oplus W_d$, $W_1\times \cdots \times W_d$ should play the role that $\F ^d$ did with coordinates before.  That is, the map that sends an ``abstract'' vector to its coordinates should be a map $V\rightarrow W_1\times \cdots \times W_d$ (and with any luck, it will be an isomorphism).  This is exactly how we're doing to define things.  But first, we want to give $W_1\times \cdots \times W_d$ the structure of a vector space.
\begin{prp}{Product of modules}{}
	Let $\collection{V}$ be an indexed collection of $\K$-modules, and define $\prod _{V\in \collection{V}}\times \prod _{V\in \collection{V}}\rightarrow \prod _{V\in \collection{V}}$ and $\K \times \prod _{V\in \collection{V}}V\rightarrow \prod _{V\in \collection{V}}V$ respectively by
	\begin{subequations}
		\begin{align}
			[v_1+v_2]^V & \ceqq v_1^V+v_2^V \\
			[\alpha \cdot v]^V & \ceqq \alpha \cdot v^V.
		\end{align}
	\end{subequations}
	Then, $\prod _{V\in \collection{V}}V$ is a $\K$-module, the \term{product}\index{Product (of $\K$-modules)}, with this addition and scaling.
	\begin{rmk}
		$v\in \prod _{V\in \collection{V}}V$, that is, it is a function $\collection{V}\rightarrow \bigsqcup _{V\in \collection{V}}V$ whose value at $V\in \collection{V}$ is an element of $V$---see \cref{CartesianProductCollection}.  $\alpha \cdot v$ is a new function, one we are defining, and $[\alpha \cdot v]^V$ is its value at $v\in \collection{V}$.  Similarly for the definition of addition.
	\end{rmk}
	\begin{rmk}
		In words, addition and scaling are defined \emph{componentwise}.
	\end{rmk}
	\begin{rmk}
		In the finite case $\collection{V}\eqqc \{ V_1,\ldots ,V_m\}$, these definitions (in more suggestive notation) look like
		\begin{subequations}
			\begin{align}
				\coord{v_1,\ldots ,v_m}+\coord{w_1,\ldots ,w_m} & \ceqq \coord{v_1+w_1,\ldots ,v_m+w_m} \\
				\alpha \cdot \coord{v_1,\ldots ,v_m} & \ceqq \coord{\alpha \cdot v_1,\ldots ,\alpha \cdot v_m}.
			\end{align}
		\end{subequations}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  The proof is as easy as verifying the axioms of a $\K$-module---see \cref{RModule}.
			\end{rmk}
		\end{exr}
	\end{proof} 
\end{prp}

We are now ready define coordinates with respect to a direct-sum decomposition.
\begin{dfn}{Coordinates (of a vector with respect to a decomposition)}{dfn4.4.6}
	Let $V$ be a $\K$-module, let $V=\bigoplus _{W\in \collection{W}}W$ be a direct-sum decomposition of $V$, let $v\in V$, and write
	\begin{equation}
	v=\sum _{W\in \collection{W}}v^W
	\end{equation}
	for unique $v^W\in \collection{W}$.  Then, the \term{coordinates}\index{Coordinates (of a vector with respect to a decomposition)} of $v$ with respect to the decomposition $\collection{W}$, $\coordinates{v}{\collection{W}}$\index[notation]{$\coordinates{v}{\collection{W}}$}, is defined by
	\begin{equation}
	\coordinates{v}{\collection{W}}\ceqq \langle v^W:W\in \collection{W}\rangle \in \prod _{W\in \collection{W}}W.
	\end{equation}
	\begin{rmk}
		If $\collection{W}\eqqc \{ W_1,\ldots ,W_m\}$ is finite, where are unique $v^1\in W^1,\ldots ,v^m\in W^m$ such that
		\begin{equation}
		v=v^1+\cdots +v^m.
		\end{equation}
		Then, the coordinates of $v$ are given by the column vector
		\begin{equation}
		\coordinates{v}{\collection{W}}\ceqq \begin{bmatrix}v^1 \\ \vdots \\ v^m\end{bmatrix}\in \prod _{k=1}^mW_k.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		Note that this generalizes coordinates with respect to a basis (\cref{CoordinatesVector}), at least for vector spaces.  Indeed, if $\basis{B}$ is a basis for $V$, then we obtain a corresponding direct-sum decomposition
		\begin{equation}
		V=\bigoplus _{b\in \basis{B}}\Span (b)
		\end{equation}
		by the previous result.  If the ground ring $\F$ is a division ring, then $\Span (b)$ is isomorphic to the one-dimensional vector space $\F$, and after this identification, we can view the component $v^b\in \Span (B)$ in the sense of this definition as a scalar, which gives us $\coordinates{v}{\basis{B}}$.
	\end{rmk}
\end{dfn}
As before, this is an isomorphism if the direct-sum decomposition is finite.
\begin{prp}{$\coordinates{\blankdot}{\collection{W}}$ is linear and injective (and surjective in finite dimensions)}{}
	Let $V$ be a $\K$-module, and let $V=\bigoplus _{W\in \collection{W}}W$ be a direct-sum decomposition of $V$.  Then,
	\begin{equation}
		V\ni v\mapsto \coordinates{v}{\collection{W}}\in \prod _{W\in \collection{W}}W
	\end{equation}
	is linear and injective with image
	\begin{equation}
		\left\{ \coord{v^W:W\in \collection{W}}:v^W=0\text{ for cofinitely many }W\in \collection{W}\text{.}\right\} .
	\end{equation}
	\begin{rmk}
		In particular, if $\collection{W}\eqqc \{ W^1,\ldots ,W^m\}$ is finite, this map is surjective, and so this gives an isomorphism $V\rightarrow W^1\times \cdots \times W^m$.  In particular,
		\begin{equation}
			W^1\oplus \cdots \oplus W^m\cong W^1\times \cdots \times W^m.
		\end{equation}
		For this reason, people are not always careful to distinguish between the two, and quite often people will write $V\oplus W$ when technically they mean $V\times W$.  Indeed, I have even heard $V\times W$ called the ``external direct sum'' (in which case they referred to what we have been calling ``direct sum'' as ``internal direct sum'').
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove this yourself.
		\end{exr}
	\end{proof}
\end{prp}

As you might now expect, we obtain an analogous notion of coordinates of linear-transformations with respect to (finite) direct-sum decompositions.  As with coordinates of vectors, our first order of business is to determine what the analogue of matrices is in this context (just as we had to determine what the analogue of $\K ^d$ should be).  We will see that the answer is still essentially ``matrices'', but now matrices whose entries themselves are linear-transformations (similar to how one might think of an element of $W_1\times \cdots \times W_m$ as a ``column vector'' whose entries are themselves vectors).
\begin{dfn}{Matrix (of linear-transformations)}{}
	Let $m$ and $n$ be collections of $\K$-modules.  Then, an \term{$m\times n$ matrix}\index{Matrix (of linear-transformations)} is a function
	\begin{equation}
		m\times n\ni \coord{i,j}\mapsto A\indices{^i_j}\in \bigsqcup _{\substack{W\in m \\ V\in n}}\Mor _{\Mod{\K}}(V,W)
	\end{equation}
	such that $A\indices{^i_j}\in \Mor _{\Mod{\K}}(i,j)$.
	\begin{rmk}
		We make use of all of the suggestive notation and language as we did with matrices (of scalars) before.  One thing to note, however, is that in the equation defining matrix multiplication (\eqref{eqn1.1.47}), upon generalizing to this context, the multiplication denoted by juxtaposition should be interpreted there as composition in \emph{postfix notation}, that is, $B\indices{^k_j}A\indices{^i_k}\ceqq A\indices{^i_k}\circ B\indices{^k_j}$ (otherwise the composition wouldn't make sense in general).
	\end{rmk}
	\begin{rmk}
		We write $\Matrix _{m\times n}$\index[notation]{$\Matrix _{m\times n}$} for the set of all $m\times n$ matrices.  Note that we need not specify the ground ring as we did before (e.g.~as $\Matrix _{m\times n}(\K )$) because this is implicitly contained in the data given in $m$ and $n$---they are collections of \emph{$\K$}-modules.  In case $m=n$, we may write $\Matrix _m\ceqq \Matrix _{m\times m}$\index[notation]{$\Matrix _m$}.  As before, $\Matrix _{m\times n}$ has the structure of a (commutative) group always, it will additionally have the structure of a $\K$-module if $\K$ is commutative, and $\Matrix _m$ has the structure of a ring.
	\end{rmk}
	\begin{rmk}
		If the two collection of $\K$-modules are finite, say $\{ W^1,\ldots ,W^m\}$ and $\{ V^1,\ldots ,V^n\}$, then a $\{ W^1,\ldots ,W^m\} \times \{ V^1,\ldots ,V^n\}$ matrix should be thought of as a double-indexed array $A\indices{^i_j}$, $1\leq i\leq m$ and $1\leq j\leq n$, where $A\indices{^i_j}\colon V^j\rightarrow W^i$ is a linear-transformation from $V^j$ to $W^i$.  As before, this will be written
		\begin{equation}
			A=\begin{bmatrix}A\indices{^1_1} & \cdots & A\indices{^1_n} \\ \vdots & \ddots & \vdots \\ A\indices{^m_1} & \dots & A\indices{^m_n}\end{bmatrix}.
		\end{equation}
		We will see later that this will define a linear-transformation $V^1\times \cdots \times V^n\rightarrow W^1\times \cdots \times W^m$.  Indeed, that is somehow the point of matrices of linear-transformations---they make it easier to think about linear-transformations between products.
	\end{rmk}
	\begin{rmk}
		If $A\indices{^i_j}=0$ for $i\neq j$, we write
		\begin{equation}
		A\indices{^1_1}\oplus \cdots \oplus A\indices{^m_m}\ceqq \begin{bmatrix}A\indices{^1_1} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \dots & A\indices{^m_n}\end{bmatrix}.
		\end{equation}\index[notation]{$A\indices{^1_1}\oplus \cdots \oplus A\indices{^m_m}$}
		This matrix of linear-transformations is the \term{direct-sum}\index{Direct-sum (of linear-transformations)}.  Matrices of this form are referred to as \term{Block-diagonal}\index{Block-diagonal matrix}.\footnote{It is of course just a diagonal matrix in our old terminology, but in this context it is far more common to say ``block-diagonal'' to emphasize that the entries on the diagonal are not scalars but linear-transformations.}
	\end{rmk}
\end{dfn}
Before moving on, we introduce a convention that will prove quite convenient.

\paragraph{Identifying matrices with the linear-transformations they define}

Consider a linear-transformation $T\colon V\rightarrow W$.  In case $V=\K ^n$ and $W=\K ^m$, $V$ and $W$ have canonical bases (the standard bases), and so, in a sense, vectors and linear-transformations really are `the same as' column-vectors and matrices respectively.  Ordinarily this identification is arbitrary because it depends on a choice of basis, but not so in this case.  Thus, one can be sloppy and fail to make a distinction between a matrix and the linear-transformation it defines.  Previously, we didn't do this because (i) pedagogy and (ii) it wasn't really necessary.  Of course, it's still not \emph{strictly} necessary, but now it will prove to rather useful.

For example, it is quite cumbersome to say ``Consider the linear-transformation $\C ^3\rightarrow \C ^3$ defined by the follow matrix of linear-transformations
\begin{equation}
	\begin{bmatrix}T_A & 0 \\ 0 & T_B\end{bmatrix},
\end{equation}
where $T_A\colon \C \rightarrow \C$ is the linear-transformation defined by the matrix $A\ceqq \begin{bmatrix}5\end{bmatrix}$ and $T_B\colon \C ^2\rightarrow \C ^2$ is the linear-transformation defined by the matrix
\begin{equation}
	B\ceqq \begin{bmatrix}-3 & 1 \\ 0 & -3\end{bmatrix}.\text{''},
\end{equation}
especially when you consider that I could be sloppy and alternatively say ``Consider the following linear-transformation $\C ^3\rightarrow \C ^3$
\begin{equation}
	\begin{bmatrix}5 & 0 & 0 \\ 0 & -3 & 1 \\ 0 & 0 & -3\end{bmatrix}.\text{''}.
\end{equation}

Thus, hereafter, we do not guarantee to be careful about making distinctions between matrices and the linear-transformations they define---we trust that by this point in the notes the conceptual difference between the two is firmly cemented in your mind---though we will still make the distinction if it's convenient.  Related to this is that you should now consider definitions made for linear-transformations as also being `officially' made for matrices.  For example, we defined above the term ``block-diagonal matrix of linear-transformations''.  A \term{block-diagonal matrix} (of scalars) is one such that the matrix of linear-transformations with respect to the given direct-sum decomposition of the linear-transformation defined by the matrix is block-diagonal.  For example,
\begin{equation}
	\begin{bmatrix}1 & 2 & 3 & 0 & 0 \\ 4 & 5 & 7 & 0 & 0 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1\end{bmatrix}
\end{equation}
is a block-diagonal matrix of scalars, whereas
\begin{equation}
	\begin{bmatrix}1 & 2 & 3 & 0 & -13 \\ 4 & 5 & 7 & 0 & 0 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 1\end{bmatrix}
\end{equation}
is not.  Similarly, we may speak of direct-sums of matrices and so on.

\horizontalrule

We now turn to defining the coordinates of a linear-transformation with respect to direct-sum decompositions.  Our ability to do this will follow from what are in their own right important properties of direct-sum decompositions.  The first of these properties is that direct-sum decompositions of a vector space allow one to define a linear-transformation by defining what it does on all the ``summands''.  The second of these properties is the dual property involving mapping \emph{into} summands.  We state both results here.
\begin{thm}{Finite direct-sums are biproducts}{thm4.4.14}
	Let $U$ and $V$ be $\K$-modules, and let $V_1,\ldots ,V_m\subseteq V$ be subspaces such that $V=V_1\oplus \cdots \oplus V_m$.
	\begin{enumerate}
		\item \label{thm4.4.14(i)}Let $S_k\colon U\rightarrow V_k$ be linear.  Then, there is a unique linear-transformation $S\colon U\rightarrow V$ such that $S_k=\proj _{V_k}\circ S$ for $1\leq k\leq m$.
		\item \label{thm4.4.14(ii)}Let $T_k\colon V_k\rightarrow U$ be linear.  Then, there is a unique linear-transformation $T\colon V\rightarrow U$ such that $\restr{T}{V_k}=T_k$ for $1\leq k\leq m$.
	\end{enumerate}
	\begin{rmk}
		More explicitly, the first implies that
		\begin{equation}
			S(u)=S_1(u)+\cdots +S_m(u)
		\end{equation}
		for all $u\in U$.  Likewise, the second implies that
		\begin{equation}
			T(v_1+\cdots +v_m)=T_1(v_1)+\cdots +T_m(v_m)
		\end{equation}
		for $v_k\in V_k$.
	\end{rmk}
	\begin{rmk}
		You should draw an analogy between the property stated in \cref{thm4.4.14(i)} and the corresponding property for \emph{Cartesian products} of sets, namely, that you can define a function $Z\rightarrow X\times Y$ by specifying its ``components'' $Z\rightarrow X$ and $Z\rightarrow Y$.  You will learn when you study category theory in more depth that these both serve examples of \emph{limits}, in fact a special type of limit called a \emph{product} (one in the category of $\K$-modules and the other in the category of sets).
	\end{rmk}
	\begin{rmk}
		Dually, you should raw an analogy between the property stated in \cref{thm4.4.14(ii)} and the corresponding property for \emph{disjoint-unions} of sets, namely, that you can define a function $X\coprod Y\rightarrow Z$ by specifying its ``restrictions'' $X\rightarrow Z$ and $Y\rightarrow Z$.  You will learn when you study category theory in more depth that these both serve examples of \emph{colimits}, in fact a special type of colimit called a \emph{coproduct} (one in the category of $\K$-modules and the other in the category of sets).
	\end{rmk}
	\begin{rmk}
		The term \emph{biproduct} comes from the fact that $W_1\oplus \cdots \oplus W_m$ is both a product and a coproduct (though the definition is not quite that simple).
	\end{rmk}
	\begin{rmk}
		Warning:  While \cref{thm4.4.14(ii)} generalizes to arbitrary direct-sums, \cref{thm4.4.14(i)} \emph{fails} in this case---this is why we stuck to the case of finitely-many subspaces in this result.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			We leave this as an exercise.
		\end{exr}
	\end{proof}
\end{thm}
Combining these two properties, with both the domain and the codomain as a direct-sum, we obtain the following.
\begin{thm}{Coordinates of a linear-transformation (with respect to a decomposition)}{}
	Let $V$ and $W$ be $\K$-modules, let $\collection{V}\eqqc \{V_1,\ldots ,V_n\}$ and $\collection{W}\eqqc \{ W_1,\ldots ,W_m\}$ be collections of subspaces of $V$ and $W$ respectively such that $V=V_1\oplus \cdots \oplus V_n$ and $W=W_1\oplus \cdots \oplus W_m$, and let $T\colon V\rightarrow W$ be a linear-transformation.  Then, there is a unique $m\times n$ matrix, $\coordinates{T}{\collection{W}\leftarrow \collection{V}}$, the \term{coordinates}\index{Coordinates (of a linear-transformation with respect to a decomposition)} with respect to the decompositions $\collection{V}$ and $\collection{W}$, such that
	\begin{equation}
		\coordinates{T(v)}{\collection{W}}=\coordinates{T}{\collection{W}\leftarrow \collection{V}}\coordinates{v}{\collection{V}}
	\end{equation}
	for all $v\in V$.
	
	Furthermore, explicitly
	\begin{equation}
		\coordinates{T}{\collection{W}\leftarrow \collection{V}}=\begin{bmatrix}T\indices{^1_1} & \cdots & T\indices{^1_n} \\ \vdots & \ddots & \vdots \\ T\indices{^m_1} & \dots & T\indices{^m_n}\end{bmatrix},
	\end{equation}
	where we have defined
	\begin{equation}
		\proj _{W_i}\circ \restr{T}{V_j}\eqqc T\indices{^i_j}\ceqq \restr{\proj _{W_i}\circ T}{V_j}.
	\end{equation}
	\begin{rmk}
		Perhaps the most important thing for you to remember about this is an expression for $T$ in terms of the $T\indices{^i_j}$s:
		
		For $v\in V$, write $v=v^1+\cdots +v^n$ for unique $v^k\in V^k$.  Then,
		\begin{equation}
			T(v)=\left( \sum _{j=1}^nT\indices{^1_j}(v^j)\right) +\cdots +\left( \sum _{j=1}^nT\indices{^m_j}(v^j)\right) .
		\end{equation}
		That is, the $i^{\text{th}}$ coordinate of $T(v)$ with respect to the decomposition $W=W^1\oplus \cdots \oplus W^m$ is given by
		\begin{equation}
			T(v)^i=\sum _{j=1}^nT\indices{^i_j}(v^j).
		\end{equation}
	\end{rmk}
	\begin{rmk}
		To be clear, $\coordinates{v}{\collection{V}}\in V^1\times \cdots \times V^n$, $\coordinates{T(v)}{\collection{W}}\in W^1\times \cdots \times W^m$, and $\coordinates{T}{\collection{W}\leftarrow \collection{V}}\colon V^1\times \cdots \times V^n\rightarrow W^1\times \cdots \times W^m$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
As before, this yields an isomorphism from the space of linear-transformations to the space of matrices (of linear-transformations).
\begin{prp}{$\coordinates{\blankdot}{\basis{W}\leftarrow \basis{V}}$ is an isomorphism}{}
	Let $\K$ be a cring, let $V$ and $W$ be $\K$-modules, and let $V=V^1\oplus \cdots \oplus V^n$ and $W=W^m\oplus \cdots \oplus W^m$ be direct-sum decompositions of $V$ and $W$ respectively.  Then,
	\begin{equation}
		\Mor _{\Mod{\K}}(V,W)\ni T\mapsto \coordinates{T}{\collection{W}\leftarrow \collection{V}}\in \Matrix _{\collection{W}\times \collection{V}}
	\end{equation}
	is an isomorphism of $\K$-modules, where $\collection{V}\ceqq \{ V^1,\ldots ,V^n\}$ and $\collection{W}\ceqq \{ W^1,\ldots ,W^m\}$.
	\begin{rmk}
		In particular, we have that
		\begin{equation}
			\coordinates{T_1+T_2}{\collection{W}\leftarrow \collection{V}}=\coordinates{T_1}{\collection{W}\leftarrow \collection{V}}+\coordinates{T_2}{\collection{W}\leftarrow \collection{V}}
		\end{equation}
		and
		\begin{equation}
			\coordinates{\alpha \cdot T}{\collection{W}\leftarrow \collection{V}}=\alpha \cdot \coordinates{T}{\collection{W}\leftarrow \collection{V}}.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		If $\K$ weren't commutative, this instead would only be an isomorphism of groups.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

\subsection{Invariant subspaces}

At the beginning of the previous subsection, we explained that we sought to decompose $V$ into simpler pieces, with the hopes that this would enable us to associate a relatively simple matrix to a linear-transformation.  In this subsection, we investigate the \emph{types} of subspaces we would like to decompose $V$ into in order to achieve this:  \emph{invariant subspaces}.\footnote{In an ideal world, we could choose these smaller pieces to not just be $T$-invariant but in fact be eigenspaces, but of course, if we could do that, everything would be diagonalizable, and we wouldn't be discussing this in the first place.}  If we can decompose $V$ as a direct-sum of smaller $T$-invariant subspaces, then the form that the matrix of $T$ takes will simplify into a \emph{block-diagonal} matrix---see \cref{thm4.5.2}.

Before turning to the the definition of invariant subspace itself, we first investigate a new example of an $R$-module that will prove moderately useful in the things to come.
\begin{exm}{The $\K [x]$-module defined by a linear-trans-\\formation}{exm1.1.34}
	Let $\K$ be a ring.  Before we get to the module itself, note that, while we have previously only thought of $\K [x]$ as a $\K$-module, it also has the canonical structure of a ring:  the addition is the same and the multiplication now is just `ordinary' multiplication of polynomials.\footnote{This works just like you think it would, with the caveat that the coefficients are assumed to \emph{commute} with $x$, even if $\K$ is noncommutative.  For example, $(\alpha x)\cdot (\beta +x^2)=\alpha \beta x+\alpha x^2$.  This assumption is a reasonable one because, as you will see shortly, $x$ is going to act as if it were a linear-transformation, and linear-transformations commute with all scalars, no matter how noncommutative $\K$ might be.}  This is exactly analogous to how we can consider $\R$ to be both a vector space and a ring.
	
	Having noted that $\K [x]$ has the structure of a ring (so that ``$\K [x]$-module'' actually makes sense), let us turn to examining the $\K [x]$-module itself.  To define this $\K [x]$-module, we must first start with a $\K$-module $V$ and a $\K$-linear operator $T\colon V\rightarrow V$.
	
	We now define the $\K [x]$-module as follows:  The underlying set of vectors is the same as before, $V$; the addition is the same as it was before; but now scaling $\K [x]\times V\rightarrow V$ is defined by
	\begin{equation}
	(\alpha _0+\alpha _1x+\cdots +\alpha _mx^m)\cdot v\ceqq \alpha _0\cdot v+\alpha _1\cdot T(v)+\cdots +\alpha _m\cdot T^m(v).
	\end{equation}
	Intuitively, we require that $x\cdot v\ceqq T(v)$, and then the ``action'' of every other polynomial is determined in the obvious way (e.g.~$x^2\cdot v\ceqq T^2(v)$).
	\begin{exr}[breakable=false]{}{}
		Check that this actually satisfies the axioms of a $\K [x]$-module.
	\end{exr}
	\begin{rmk}
		If ever $V$ is a $\K$-module and $T$ is a $\K$-linear operator on $V$, if we say ``$V$ is a $\K [x]$-module'', it should be understood that the $\K [x]$-module structure we are referring to is the one defined in this example.
	\end{rmk}
\end{exm}
Regarding $V$ as a $\K [x]$-module in this way is just a fancy way of saying that we have defined what $p(T)$ means, $T\colon V\rightarrow V$ a $\K$-linear operator and $p$ a polynomial.  For example, if $p(x)\ceqq 3x^3-5x^2+2$, then $p(T)\colon V\rightarrow V$ is the $\K$-linear transformation
\begin{equation}
v\mapsto [p(T)](v)\ceqq [3T^3-5T^2+2](v)\ceqq 3T(T(T(v)))-5T(T(v))+2v.
\end{equation}
After having discussed eigenvalues, we can say a little more about these operators---see \cref{prp4.2.33}.

This example allows us to make the following definition.
\begin{dfn}{Invariant subspace}{InvariantSubspace}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be a $\K$-linear operator, and let $W\subseteq V$ be a subspace.  Then, $W$ is \term{$T$-invariant}\index{Invariant subspace} iff $W$ is a $\K [x]$-submodule of $V$.
	\begin{rmk}
		If $T$ is clear from context, we may simply say \term{invariant}.
	\end{rmk}
	\begin{rmk}
		This is succinct but obtuse---see the following proposition for a more down-to-earth characterization of what this means.
	\end{rmk}
	\begin{rmk}
		Recall that (\cref{Subspace}) ``submodule'' is synonymous with ``subspace''.  We use the term ``submodule'' here to emphasize the distinction between thinking of $V$ as a $\K$-module versus a $\K [x]$-module.
	\end{rmk}
	\begin{rmk}
		Note that subspaces which satisfy \cref{Eigen}\cref{Eigen(i)} are automatically $T$-invariant.  In particular, eigenspaces are invariant subspaces.
	\end{rmk}
	\begin{rmk}
		Strictly speaking, we didn't really \emph{need} the $\K [x]$-module defined in \cref{exm1.1.34} to state this definition, and it is admittedly easier to understand characterization in the following proposition.  Nonetheless, I decided to present it this was (i) to give you practice thinking about nonartificial examples of $R$-modules that are not vector spaces, (ii) I personally find it more elegant, and (iii) it is more systematic in the sense that the term ``invariant subspace'' is used in more general contexts where it can analogously be defined as a particular submodule.
	\end{rmk}
\end{dfn}
\begin{prp}{}{}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be a $\K$-linear operator, and let $W\subseteq V$ be a subspace.  Then, $W$ is $T$-invariant iff $T(w)\in W$ for all $w\in W$.
	\begin{rmk}
		This makes it clear one reason why invariant subspaces might be useful:  if $W$ is invariant, then $\restr{T}{W}\colon W\rightarrow W$, that is, $T$ can be considered as a linear operator on $W$.
	\end{rmk}
	\begin{rmk}
		Of course, said another way, this is the same as the statement $T(W)\subseteq W$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove this yourself.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exm}{}{}
	Define $D\colon \R [x]\rightarrow \R [x]$ by $D(p)\ceqq p'$.  Then, $\R [x]_m\subseteq \R [x]$ is a $D$-invariant subspace for all $m\in \N$.\footnote{Recall (\cref{Polynomials}) that $\R [x]_m$ is the subspace of all polynomials of degree at most $m$.}
\end{exm}

One fact we will find useful is that projections onto invariant subspaces are ``compatible'' with the linear operator.
\begin{prp}{}{prp4.5.9}
	Let $V$ be a $\K$-module, let $V=U\oplus W$ be a direct-sum decomposition, and let $T\colon V\rightarrow V$ be a linear operator.  Then, if $U$ and $W$ are $T$-invariant, then
	\begin{equation}
		\proj _U\circ T=T\circ \proj _U\text{ and }\proj _W\circ T=T\circ \proj _W.
	\end{equation}
	\begin{proof}
		Suppose that $U$ and $W$ are $T$-invariant.  Let $v\in V$ and write $v=u+w$ for unique $u\in U$ and $w\in W$.  Then, $T(v)=T(u)+T(w)$.  By invariance, $T(u)\in U$ and $T(w)\in W$, so by definition of projections, $\proj _U(T(v))=T(u)\ceqq T(\proj _U(v))$.  Hence, $\proj _U\circ T=T\circ \proj _U$.  Similarly for $W$.
	\end{proof}
\end{prp}

We know now that we can't always find a basis in which the matrix of a linear operator is diagonal.  Our knowledge of direct-sums from the previous subsection suggests that perhaps we can get what is in a sense the `next best thing':  a \emph{block-diagonal} matrix.  Before, we saw that the matrix our our linear-transformation will be diagonal iff the basis consisted of eigenvectors.  Our goal now then is to determine when the matrix of our linear-transformation with respect to a \emph{direct-sum decomposition} is block-diagonal.  As eigenspaces were special types of invariant subspaces, we might guess that the more general invariant subspaces will work.  Indeed, this is the case.
\begin{dfn}{Block-diagonalizable}{BlockDiagonalizble}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, $T$ is \term{block-diagonalizable}\index{Block-diagonalizable} iff there is a direct-sum decomposition $V=V^1\oplus \cdots \oplus V^m$ such that $\coordinates{T}{\collection{V}\leftarrow \collection{V}}$ is a block-diagonal matrix, where $\collection{V}\ceqq \{ V^1,\ldots ,V^m\}$.
	\begin{rmk}
		In this case, $\coordinates{T}{\collection{V}\leftarrow \collection{V}}$ is the \term{block-diagonalization}\index{Block-diagonalization} of $T$.
	\end{rmk}
\end{dfn}
\begin{thm}{Fundamental Theorem of Block-diagonaliz\-ability}{thm4.5.2}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, the following are equivalent.
	\begin{enumerate}
		\item $T$ is block-diagonalizable.
		\item There is a direct-sum decomposition of $V$ consisting of $T$-invariant subspaces.
		\item There are $T$-invariant subspaces $V^1,\ldots ,V^m$ such that
		\begin{equation}
		\dim (V)=\dim (V^1)+\cdots +\dim (V^m).
		\end{equation}
	\end{enumerate}
	In this case, $\coordinates{T}{\collection{V}\leftarrow \collection{V}}$ is a block-diagonal matrix, where $\collection{V}\ceqq \{ V^1,\ldots ,V^m\}$.
	\begin{rmk}
		Let $V=V^1\oplus \cdots \oplus V^m$ be a direct-sum decomposition of $V$ consisting of $T$-invariant subspaces.  As each $V^k$ is invariant, $T$ restricts to a linear-transformation $T^k\ceqq \restr{T}{V^k}\colon V^k\rightarrow V^k$.  Using this notation, we have
		\begin{equation}
		\coordinates{T}{\collection{V}\leftarrow \collection{V}}=\begin{bmatrix}T^1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & T^m\end{bmatrix}\eqqc T^1\oplus \cdots \oplus T^m.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		Warning:  This name is nonstandard---there is no standard name for this result.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

\subsubsection{Indecomposability}

Of course, $V=V$ is itself a direct-sum decomposition, and one that generally won't be very useful.  In order to simplify the block-diagonal form as much as possible, we wish to break up $V$ into the `smallest' invariant subspaces we can.  The precise sense in which we want these subspaces to the the ``smallest'' possible is called \emph{indecomposable}.
\begin{dfn}{Indecomposable}{Indecomposable}
	Let $V$ be a nonzero $R$-module.  Then, $V$ is \term{indecomposable}\index{Indecomposable module} iff whenever $V=U\oplus W$, it follows that either $U=0$ or $W=0$.
	\begin{rmk}
		If $V$ is a $\K$-module and $T\colon V\rightarrow V$ is a linear operator, we say that $V$ is \term{$T$-indecomposable} iff $V$ is indecomposable when regarded as a $\K [x]$-module.
	\end{rmk}
	\begin{rmk}
		If we say that a subspace $W\subseteq V$ is $T$-indecomposable, it is implicit that it is also $T$-invariant (otherwise it wouldn't actually possess the structure of a $\K [x]$-module, and so the definition given in the previous remark wouldn't make sense).
	\end{rmk}
	\begin{rmk}
		You can view the requirement that $V$ be nonzero as roughly analogous to how we disallow $1\in \Z$ from being considered prime.
	\end{rmk}
	\begin{rmk}
		Warning:  You will likely hear the term ``irreducible'' at some point.  This is \emph{not} the same as ``indecomposable'', though they are certainly related---irreducible implies indecomposable.
	\end{rmk}
\end{dfn}
We now consider whether we can, and if so, how can we, decompose $V$ into indecomposable submodules.  The basic idea is as follows.  If $V$ is indecomposable, we're done; otherwise, there are proper nonzero submodules $U,W\subseteq V$ such that $V=U\oplus W$.  If $U$ and $W$ are indecomposable, again, we're done; otherwise, we can break up $U$ and/or $W$ as a direct-sum of proper nonzero submodules, and so on.  In general, there is no need for this process to terminate.  However, the next result says that it does in the primary case of interest.
\begin{prp}{}{prp4.5.17}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, there are finitely-many $T$-indecomposable subspaces $V^1,\ldots ,V^m\subseteq V$ such that
	\begin{equation}
		V=V^1\oplus \cdots \oplus V^m.
	\end{equation}
	\begin{rmk}
		For what it's worth, modules that can be written as direct-sums of indecomposable modules are known as \term{completely-decomposable}\index{Completely-decomposable module} modules.  Here, we would be regarding $V$ as a $\F [x]$-module, $\F$ the ground field,\footnote{All vector spaces are `trivially' completely-decomposable---every basis gives a direct-sum decomposition into $\F$-invariant subspaces.} in which case we would say that $V$ is \term{$T$-completely-decomposable} as short-hand for the statement ``$V$ is a completely-decomposable $\F [x]$-module.''.
	\end{rmk}
	\begin{rmk}
		Warning:  This will not be true in general---see \cref{exr4.5.17}.
	\end{rmk}
	\begin{proof}
		If $V$ is indecomposable, we are done.  Otherwise, there are proper nonzero invariant subspaces $U_1,U_2\subseteq V$ such that $V=U_1\oplus U_2$.  Note that as $V$ is finite-dimensional and these are proper subspaces, we have that $\dim (U_1),\dim (U_2)<\dim (V)$.  Again, if each $U_1$ and $U_2$ are indecomposable, we are done; otherwise, we can write these as the direct-sum of invariant subspaces whose dimensions are strictly less than $\dim (U_1)$ and $\dim (U_2)$ respectively.
		
		Repeating this process inductively, the process either stops because everything is indecomposable, or we have reached that point where there are no proper nonzero subspaces, in which case the subspace has to have dimension $1$, and so must be indecomposable.  Thus, we eventually find that $V=V_1\oplus \cdots \oplus V_M$ for $V_k\subseteq V$ indecomposable subspaces, as desired.
	\end{proof}
\end{prp}
\begin{exr}{}{exr4.5.17}
	What is an example of an that is not completely-decomposable?  Can you find one that is an $\F [x]$-module?
\end{exr}

Now seems an appropriate time to return to a counter-example we mentioned previously, namely, a subspace of a $\K$-module with no complement.
\begin{exm}{A subspace of a $\K$-module with no complement}{exm4.5.15}
	Define $\K \ceqq \C$, $V\ceqq \C ^2$, $U\ceqq \Span (\coord{1,0})$, and let $T_A\colon V\rightarrow V$ be the $\C$-linear-transformation defined by the matrix
	\begin{equation}
		A\ceqq \begin{bmatrix}0 & 1 \\ 0 & 1\end{bmatrix}.
	\end{equation}
	Note that $U\subseteq V$ is a $\C [x]$-submodule, that is, it is $T_A$-invariant.  We claim that it has no complement.
	
	We proceed by contradiction:  let $W\subseteq V$ be a $\K [x]$-submodule such that $V=U\oplus W$ (as $\C [x]$-modules).  $W$ is then in particular a $\C$-subspace, and so, by a dimension count, we must have $W=\Span (\coord{a,b})$ for $a,b\in \C$, $b\neq 0$.  However, $N(\coord{a,b})=\coord{b,0}\notin W$, and so $W$ is not $N$-invariant, no matter what $a$ and $b$ are.
	\begin{rmk}
		Essentially this same argument shows that $V$ is an indecomposable $\C [x]$-module.  In particular, this serves as an example of an indecomposable module with proper nonzero submodules.\footnote{One might naively think that if $U\subseteq V$ is proper and nonzero, we can write $V=U\oplus W$ for $W\subseteq V$ also proper nonzero, as we can with vector spaces, and so the existence of such a $U$ implies decomposability.  This example shows that this naive thinking is wrong.}
	\end{rmk}
\end{exm}

\subsection{Generalized ``Eigenstuff''}

After realizing that we won't always be able to find a basis in which the matrix of a given linear-transformation was diagonal, we set out to find the `next best thing'.  In the previous sections, we decided that instead we should only try to find a \emph{block}-diagonal matrix, and in order to do that, we should decompose $V$ into invariant subspaces.  Furthermore, in order to simplify the description as much as possible, we would like these subspaces not to just be invariant but to be \emph{indecomposable}.

\subsubsection{A glimpse of generalized-eigenspaces}

Our first hint at how we might do all this is given by the following result \cref{prp4.6.1}.
\begin{prp}{}{lma4.6.2}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then,
	\begin{equation}
	0\subseteq \Ker (T)\subseteq \cdots \subseteq \Ker (T^{\dim (V)})=\Ker (T^{\dim (V)+1})=\cdots .
	\end{equation}
	\begin{proof}
		Let $v\in \Ker (T^k)$.  Then, $T^k(v)=0$, and so $T^{k+1}(v)\ceqq T(T^k(v))=0$, and so $v\in \Ker (T^{k+1})$.  Hence, $\Ker (T^k)\subseteq \Ker (T^{k+1})$.
		
		If $\Ker (T^m)=\Ker (T^{m+1})$, then in fact $\Ker (T^m)=\Ker (T^n)$ for all $n\geq m$.  To see this, write $n=m+k$ for $k\in \N$ and let $v\in \Ker (T^{m+k})$, so that $T^{m+1}(T^{k-1}(v))=T^{m+k}(v)=0$.  Then, $T^{k-1}(v)\in \Ker (T^{m+1})=\Ker (T^m)$, and so in fact $T^m(T^{k-1}(v))=T^{m+k-1}(v)=0$.  Proceeding inductively, we eventually find that $T^m(v)=0$.
		
		Thus, if there is some $m\leq \dim (V)$ such that $\Ker (T^m)=\Ker (T^{m+1})$, we are done.  Otherwise, $0$ is properly contained in $\Ker (T)$, which is properly contained in $\Ker (T^2)$, etc..  This means that $\dim (T^k)\geq \dim (T^{k-1})+1$ for $1\leq k\leq \dim (V)$, and hence $\dim (T^{\dim (V)})\geq \dim (V)$, in which case we must have $\Ker (T^{\dim (V)})=\Ker (T^m)$ for all $m\geq \Ker (T)$, as desired.
	\end{proof}
\end{prp}
\begin{prp}{}{prp4.6.1}
	Let $V$ be a vector space over a division ring $\F$, let $T\colon V\rightarrow V$ be linear, let $\lambda \in \F$ be central, and let $m\in \N$.  Then, if $\Ker ([T-\lambda ]^m)=\Ker ([T-\lambda ]^{m+1})$, then,
	\begin{equation}
		V=\Ker ([T-\lambda ]^m)\oplus \Ima ([T-\lambda ]^m)
	\end{equation}
	is a direct-sum decomposition of $V$ into $T$-invariant subspaces.
	\begin{rmk}
		Note by the previous result that, if $V$ is finite-dimensional, we always have
		\begin{equation}
			V=\Ker ([T-\lambda ]^{\dim (V)})\oplus \Ima ([T-\lambda ]^{\dim (V)}).
		\end{equation}
	\end{rmk}
	\begin{rmk}
		In fact, they are invariant subspaces for all $m\in \N$---we needn't assume anything about $m$ for this part of the result to hold.
	\end{rmk}
	\begin{rmk}
		Note that $\Eig _{\lambda}=\Ker (T-\lambda )\subseteq \Ker ([T-\lambda ]^{\dim (V)})$, that is, the first invariant subspace here contains the $\lambda$-eigenspace of $T$.\footnote{Strictly speaking, we shouldn't be using this notation or language if $\lambda$ is not an eigenvalue.  In any case, the inclusion of the kernels stated remains valid.}
	\end{rmk}
	\begin{rmk}
		We will see later that, if $\lambda \in \F$ is an eigenvalue of $T$, then $\Ker ([T-\lambda ]^{\dim (V)})$ is the \emph{$\lambda$-generalized-eigenspace} of $T$.
	\end{rmk}
	\begin{proof}
		We first check that these are invariant subspaces.  Let $v\in \Ker ([T-\lambda ]^{m})$.  As $T$ computes with $T-\lambda$, we have that
		\begin{equation}
			[T-\lambda ]^{m}(T(v))=T\left( [T-\lambda ]^{m}(v)\right) =T(0)=0,
		\end{equation}
		and so $T(v)\in \Ker ([T-\lambda ]^{m})$.
		
		Let $v=[T-\lambda ]^{m}(w)\in \Ima ([T-\lambda ]^{m})$.  Then,
		\begin{equation}
			T(v)=T\left( [T-\lambda ]^{m}(w)\right) =[T-\lambda ]^{m}\left( T(w)\right) ,
		\end{equation}
		and so $T(v)\in \Ima ([T-\lambda ]^{m})$.
		
		It suffices to prove the rest of the result for $\lambda =0$ (this is the same as proving the result for arbitrary $T$ linear).  By \cref{crl4.4.11}, it suffices to show that (i) $V=\Ker (T^{m})+\Ima (T^{m})$ and that (ii) $\Ker (T^{m})\cap \Ima (T^{m})=0$.
		
		We first check that $\Ker (T^{m})\cap \Ima (T^{m})=0$.  So, let $v=T^{m}(w)\in \Ker (T^{m})\cap \Ima (T^{m})$.  We thus have that
		\begin{equation}
		0=T^{m}(v)=T^{2m}(w),
		\end{equation}
		and hence $w\in \Ker (T^{2m})$.  By \cref{lma4.6.2}, $\Ker (T^{m})=\Ker (T^{2m})$, and so in fact $w\in \Ker (T^{m})$, and hence $v=T^{m}(w)=0$.
		
		It now follows that
		\begin{equation}
			\begin{split}
				\MoveEqLeft
				\dim (\Ker (T^{m})+\Ima (T^{m})) \\
				& =\dim (\Ker (T^{m}))+\dim (\Ima (T^{m})) \\
				& =\footnote{By the \namerefpcref{RankNullityTheorem}.}m,
			\end{split}
		\end{equation}
		and hence $V=\Ker (T^{m})+\Ima (T^{m})$.	
	\end{proof}
\end{prp}

The next idea is to apply this decomposition inductively:  List the distinct eigenvalues of $T$, $\lambda _1,\ldots ,\lambda _m$, write $V=\Ker ([T-\lambda _1]^{\dim (V)})\oplus W_1$, where we have defined $W_1\ceqq \Ima ([T-\lambda _1]^{\dim (V)})$, write $W_1=\Ker ([\restr{T-\lambda _2}{W_1}]^{\dim (W_1)})\oplus W_2$, where we have defined $W_2\ceqq \Ima (\restr{[T-\lambda _2]}{W_1}^{\dim (V)}$, and so on.  With any luck, we will find $W_m=0$, so that $V$ can be written as a finite direct-sum of subspaces of the form $\Ker ([T-\lambda ]^{\dim (V)})$.

That said, the issue or not of whether we can make such a decomposition is going to be irrelevant if we don't understand the behavior of $T$ on subspaces of the form $\Ker ([T-\lambda ])^{\dim (V)})$.  In brief, $T-\lambda$ is \emph{nilpotent} on this subspace, which brings us to the next subsubsection.

\subsubsection{Nilpotent linear operators}

Before we begin a study if nilpotency proper, let us briefly consider an alternative perspective one can take on finding the ``next best thing'' to diagonalization.

Let $T\colon V\rightarrow V$ be a linear operator and let $\basis{B}$ be a basis for $V$.  As we can't always find a $\basis{B}$ such that $\coordinates{T}{\basis{B}}$ is diagonal, we instead try to see if we can find a $\basis{B}$ such that
\begin{equation}
	\coordinates{T}{\basis{B}}=D+N,
\end{equation}
where $D$ is diagonal and $N$ is `small' (i.e.~`close' to $0$) in some sense.

To see what notion of ``small'' we might want, recall that we are going to try to decompose $V$ into subspaces of the form $\Ker ([T-\lambda ]^{\dim (V)})$, and so are also interested in understanding the behavior of the restriction of $T$ to these subspaces.  If this were simply $\Ker (T-\lambda )$, that is, an eigenspace,\footnote{At least for $\lambda$ an eigenvalue.} we would have $T-\lambda =0$ on this subspace---in the notation above, we have $D=\lambda$ and $N=0$.  However, we don't quite have that for the subspace $\Ker ([T-\lambda ]^{\dim (V)})$.  Rather, we only have that the restriction of $T-\lambda$ to this subspace is \emph{nilpotent}.  It is in this sense that $N$ will be ``small''.
\begin{dfn}{Nilpotent}{Nilpotent}
	Let $X$ be a rg and let $x\in X$.  Then, $x$ is \term{nilpotent}\index{Nilpotent} iff there is some $m\in \N$ such that $x^m=0$.
	\begin{rmk}
		For us, the rg we're going to be interested in is in fact a ring, $\End _{\Mod{\K}}(V)$, $V$ a $\K$-module, and which case for $T\colon V\rightarrow V$ to be nilpotent means that there is some $m\in \N$ such that $T^m=0$, where
		\begin{equation}
			T^m\ceqq \underbrace{T\circ \cdots \circ T}_m,
		\end{equation}
		that is, $T$ composed with itself $m$ times.
	\end{rmk}
\end{dfn}
\begin{exm}{}{}
	Let $N$ be a strictly upper-triangular matrix.  Then, $N$ is nilpotent.
	\begin{exr}[breakable=false]{}{}
		Check this.
	\end{exr}
\end{exm}

One of the first things we note about nilpotent operators is that they have but one possible eigenvalue:  zero.
\begin{prp}{}{}
	Let $V$ be a vector space, let $N\colon V\rightarrow V$ be linear, and let $\lambda \in \F$ be an eigenvalue.  Then, if $N$ is nilpotent, $\lambda =0$.
	\begin{rmk}
		In fact, except for the silly case $V=0$ (in which case no operator can have any eigenvalue), $0$ will be an eigenvalue of $N$.
	\end{rmk}
	\begin{proof}
		Suppose that $N$ is nilpotent.  By definition, there is then some $m\in \N$ such that $N^m=0$.  By \cref{crl4.2.36}, it follows that $\lambda ^m=0$, whence $\lambda =0$ because the ground ring is a division ring.
	\end{proof}
\end{prp}
This has an important, albeit unfortunate, consequence.
\begin{crl}{}{}
	Let $V$ be a finite-dimensional vector space and let $N\colon V\rightarrow V$ be nilpotent linear.  Then, $N$ is diagonalizable iff $N=0$.
	\begin{rmk}
		Moreover, we will see later that, in a sense, having a nonzero `nilpotent part' is the only way in which a linear operator can fail to be diagonalizable.
	\end{rmk}
	\begin{proof}
		$(\Rightarrow )$ Suppose that $N$ is diagonalizable.  Then, there is a basis of $V$ consisting of eigenvectors of $N$.  By the previous result, the corresponding eigenvalues are all $0$, and so $N$ vanishes on each of these basis vectors.  Hence, $N=0$.
		
		\blni
		$(\Leftarrow )$ Suppose that $N=0$.  Then, $\coordinates{N}{\basis{B}}$ will be a diagonal\footnote{To accommodate the silly case in which $V=0$, note that the empty matrix is vacuously diagonal.} matrix for any basis $\basis{B}$ of $V$.
	\end{proof}
\end{crl}

We thus know that nilpotent linear-transformations are essentially never diagonalizable, so if we are to come up with a `next best thing' to diagonalization, we had at the very least know how to do so for nilpotent linear-transformations.  The following result is our solution to this problem.
\begin{thm}{Jordan Canonical Form of Nilpotent Operators}{thm4.6.17}
	Let $V$ be a finite-dimensional vector space and let $N\colon V\rightarrow V$ be nilpotent linear.  Then, if $V$ is $N$-indecomposable, then there is a basis $\basis{B}$ of $V$ such that
	\begin{equation}\label{eqn4.6.17}
		\coordinates{N}{\basis{B}\leftarrow \basis{B}}=\begin{bmatrix}0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 1 & \cdots & 0 \\ \vdots &\ddots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 1 \\ 0 & 0 & 0 & \cdots & 0\end{bmatrix}.
	\end{equation}
	\begin{rmk}
		Note that (\cref{prp4.5.17}) we can always write $V$ as a direct-sum of finitely many $N$-indecomposable submodules, and so in the general case (when $V$ is not necessarily indecomposable), $\coordinates{N}{\basis{B}\leftarrow \basis{B}}$ will be a direct-sum of matrices of the above form.
	\end{rmk}
	\begin{proof}
		\Step{Define the basis}
		Suppose that $V$ is $N$-indecomposable.  As $N$ is nilpotent, there is a smallest positive integer $m\in \Z ^+$ such that $N^m=0$.  It must then be the case that $N^{m-1}\neq 0$, and so there is some $v\in V$ such that $N^{m-1}(v_0)\neq 0$.  Define
		\begin{equation}
			\basis{B}\ceqq \{ N^{m-1}(v_0),\ldots ,N(v_0),v_0\} .
		\end{equation}
		As $N(N^{m-1}(v_0))=0$ and $N(N^k(v_0))=N^{k+1}(v_0)$ for $0\leq k<m-1$, \eqref{eqn4.6.17} will follow if we can show that $\basis{B}$ is a basis.
		
		\Step{Check linear-independence}
		Suppose that
		\begin{equation}
			0=\alpha _0\cdot v_0+\cdots +\alpha _{m-1}\cdot N^{m-1}(v_0).
		\end{equation}
		Applying $N^{m-1}$ to this equation, we obtain $0=\alpha _0\cdot N^{m-1}(v_0)=0$, and hence $\alpha _0$.  Applying $N^{m-2}$ in the same way, we find $\alpha _1$.  Proceeding inductively, we eventually find that every $\alpha _k=0$, establishing linear-independence.
		
		\Step{Start the proof of spanning with induction}
		As for spanning, we proceed by induction on $m$.  That is, we prove the statement
		\begin{displayquote}
			If $N$ is a nilpotent operator on an $N$-indecomposable finite-dimensional vector space $V$ where $m\in \Z ^+$ is the smallest positive integer such that $N^m=0$ and $v\in V$ is such that $N^{m-1}(v_0)\neq 0$, then $\{ N^{m-1}(v_0),\ldots ,v_0\}$ spans $V$.
		\end{displayquote}
		First take $m=1$, so that $N=0$.  In this case, every subspace is $N$-invariant, and so indecomposability forces $\dim (V)=1$, so that indeed $V=\Span (v_0)$.
		
		Now suppose the result is true for $m-1$.  If $m$ is the smallest positive integer such that $N^m=0$ on $V$, then $m-1$ is the smallest positive integer such that $N^{m-1}=0$ on $\Ima (N)$.  Furthermore, $N(v_0)\in \Ima (N)$ is such that $N^{(m-1)-1}(N(v_0))=N^{m-1}(v_0)\neq 0$.  Thus, if we can show that $\Ima (N)$ is indecomposable still, then the induction hypothesis will gives us that $\Ima (N)=\Span (N^{m-1}(v_0),\ldots ,N(v_0))$.
		
		\Step{Show that $\Ima (N)$ is indecomposable}[stp4.6.17.4]
		So, let $U,W\subseteq \Ima (N)$ be subspaces and suppose that $\Ima (N)=U\oplus W$ with $U$ and $W$ $N$-invariant.  Then write $V=U\oplus W\oplus V'$ for some subspace $V'\subseteq V$ (by \cref{prp4.4.24}).  For every $v\in V'$, we may write
		\begin{equation}\label{eqn4.4.132}
			\begin{split}
				N(v) & =\proj _U(N(v))+\proj _W(N(v)) \\
				& =\footnote{$N$ commutes with the projections because the subspaces are invariant---see \cref{prp4.5.9}.}N(\proj _U(v))+N(\proj _W(v))\eqqc N(v_U)+N(v_W),
			\end{split}
		\end{equation}
		where we have written $v_U\ceqq \proj _U(v)$ and $v_W\ceqq \proj _W(v)$.  Let $\basis{B}'$ be a basis for $V'$, and define
		\begin{equation}
			\breve{\basis{B}}\ceqq \{ b-b_U:b\in \basis{B}'\} 
		\end{equation}
		and $\breve{V}\ceqq \Span (\breve{\basis{B}})$.  From \eqref{eqn4.4.132}, we see that $N(b-b_U)=N(b_W)\in W$, and so $W+\breve{V}$ is an invariant subspace.  Thus, if we can show $V=U\oplus W\oplus \breve{V}$, indecomposability will imply that either $U=0$ or $W\oplus \breve{V}=0$, so that either $U=0$ or $W=0$, as desired.
		
		So, suppose that $0=u+w+\breve{v}$ for $u\in U$, $w\in W$, and $\breve{v}\in \breve{V}$.  Write $\breve{v}=\alpha _1(b_1-[b_1]_U)+\cdots +\alpha _m(b_m-[b_m]_U)$, so that
		\begin{equation}
			0=\left( u-\sum _{k=1}^m\alpha _k[b_k]_U\right) +w+\sum _{k=1}^m\alpha _kb_k.
		\end{equation}
		As $V=U\oplus W\oplus W'$, it follows that $\sum _{k=1}^m\alpha _kb_k=0$, whence each $\alpha _k=0$ by linear-independence, that $w=0$, and that $u=\sum _{k=1}^m\alpha _k[b_k]_U=0$.  By \cref{prp4.4.7}, it just remains to check that $V=U+W+\breve{V}$.
		
		So, let $v\in V$.  We may then write $v=u+w+v'$ for $u\in U$, $w\in W$, and $v'\in V'$.  Write $v'=\alpha _1b_1+\cdots +\alpha _mb_m$.  Then,
		\begin{equation}
			v'=\left( u+\sum _{k=1}^m\alpha _k[b_k]_U\right) +w+\sum _{k=1}^m\alpha _k(b_k-[b_k]_U)\in U+W+\breve{V},
		\end{equation}
		as desired.  Thus, $\Ima (T)$ is indecomposable, and hence $\Ima (N)=\Span (N^{m-1}(v_0),\ldots ,N(v_0))$.
		
		\Step{Finish the proof of spanning}
		Extend the linear-independent set $\basis{B}\ceqq \{ N^{m-1}(v_0),\ldots ,v_0\}$ to a basis $\basis{C}$ of $V$ such that $\basis{C}\supseteq \basis{B}$.  We wish to show that $\basis{C}=\basis{B}$.  So, let $c\in \basis{C}\setminus \basis{B}$.  As $N(c)\in \Ima (N)$, we can write
		\begin{equation}\label{eqn4.6.21}
			\begin{split}
				N(c)& =\alpha _1N(v_0)+\cdots +\alpha _{m-1}N^{m-1}(v_0) \\
				& =N\left( \alpha _1v_0+\cdots +\alpha _{m-1}N^{m-2}(v_0)\right) .
			\end{split}
		\end{equation}
		Now define
		\begin{equation}\label{eqn4.6.22}
			\breve{c}\ceqq c-\sum _{k=1}^{m-1}\alpha _kN^{k-1}(v_0)
		\end{equation}
		as well as $\breve{\basis{A}}\ceqq \{ \breve{c}:c\in \basis{C}\setminus \basis{B}\}$.  \eqref{eqn4.6.21} implies that $N(\breve{c})=0$.  In particular, $\breve{V}\ceqq \Span (\breve{\basis{C}})$ is invariant.  Thus, if we can show that $V=\Span (\basis{B})\oplus \Span (\breve{\basis{A}})$, indecomposability will force $\Span (\breve{\basis{A}})=0$, hence $\breve{\basis{A}}=\emptyset$, hence $\basis{C}\setminus \basis{B}=\emptyset$, hence $\basis{C}=\basis{B}$, completing the proof.
		
		Thus, it remains only to check that $V=\Span (\basis{B})\oplus \Span (\breve{\basis{A}})$.  So, let $v\in \Span (\basis{B})\cap \Span (\breve{\basis{A}})$.  We could then write this vector as a linear-combination of both elements of $\basis{B}$ and elements of $\breve{\basis{A}}$.  Using the definition of the $\breve{c}$s in \eqref{eqn4.6.22}, we see that this would yield a nontrivial linear-dependence relation about elements of $\basis{B}$ and elements of $\basis{C}\setminus \basis{B}$ unless $v=0$.  By linear-dependence of $\basis{C}$ then, we have that $\Span (\basis{B})\cap \Span (\breve{\basis{A}})=0$.
		\begin{exr}[breakable=false]{}{}
			Finish the proof by showing that $V=\Span (\basis{B})+\Span (\basis{C})$.
			\begin{rmk}
				Hint:  You might try an argument similar to the one we used to show that $V=U+W+\breve{V}$ in \cref{stp4.6.17.4}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{thm}

The use of nilpotent operators is far more common, and in our primary case of interest, it will work just fine.  However, for the purposes of dealing with generalized-eigenspaces in general, a related concept is more natural, that of \emph{local-nilpotency}.
\begin{dfn}{Locally-nilpotent}{LocallyNilpotent}
	Let $V$ be an $R$-module and let $T\in R$.  Then, $T$ is \term{locally-nilpotent}\index{Locally-nilpotent} iff for every $v\in V$ there is some $m_v\in \N$ such that $T^{m_v}\cdot v=0$.
	\begin{rmk}
		Of course, we're interested in the case $V$ is a $\K [x]$-module with module structure defined as usual by a given linear-transformation $T\colon V\rightarrow V$, in which case we say that $T$ is \term{locally-nilpotent} iff $x\in \K[x]$ is.  Explicitly, $T$ is locally-nilpotent iff for every $v\in V$ there is some $m_v\in \N$ such that $T^{m_v}(v)=0$.
	\end{rmk}
	\begin{rmk}
		Intuitively, the difference between nilpotency and local-nilpotency if the depends of $m_v$ on $V$.  For local-nilpotency, you are allowed pick a different $m_v$ for each $v\in V$; for nilpotency on the other hand, you must be able to pick a single $m\in \N$ that `works' ``uniformly'' for all $v\in V$.
		
		Thus, it is clear from the definitions that a nilpotent linear-transformation is locally-nilpotent.
	\end{rmk}
\end{dfn}
\begin{exm}{A locally-nilpotent operator that is not nilpotent}{exm4.4.129}
	Consider the left-shift operator $L\colon \C ^{\infty}\rightarrow \C ^{\infty}$ (\cref{ShiftOperator}).  Let $a\in \C ^{\infty}$.  By definition of $\C ^{\infty}$, $m\mapsto a_m$ is eventually $0$, so that there is some $M$ such that whenever $m\geq M$ it follows that $a_m=0$.  Then $L^M(a_m)=0$, and hence $L$ is locally-nilpotent.
	
	To see that $L$ is not nilpotent, note that for every $m\in \N$, the sequence $a\in C^{\infty}$ that is identically $0$ except for a $1$ at index $m$ is not `killed' by $L^m$:  $L^m(a)\neq 0$.  Hence, $L^m\neq 0$ for any $m\in \N$, and hence $L$ is not nilpotent.
	
	\horizontalrule
	
	For $m\in \N$, let $E_m\subseteq \C ^{\infty}$ be the subspace of sequences which vanish for indices larger than $m$.  Note that $T$ is nilpotent on $E_m$, but not nilpotent on all of $\C ^{\infty}$.  Thus, there is no maximal subspace on which $T$ is nilpotent.
	\begin{rmk}
		This fact is mentioned later when we discuss generalized-eigenspaces, and in particular, why locally-nilpotent, and not nilpotent, is the notion that should be used in the general definition.
	\end{rmk}
\end{exm}
On the other hand, in finite-dimensions, they are equivalent.
\begin{prp}{}{prp4.4.130}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, $T$ is nilpotent iff it is locally-nilpotent.
	\begin{proof}
		$(\Rightarrow )$ This is always true.
		
		\blni
		$(\Leftarrow )$ Suppose that $T$ is locally-nilpotent.  Let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis for $V$.  Let $m_k\in \N$ be such that $T^{m_k}(b_k)=0$.  Define $m\ceqq \max \{ m_1,\ldots ,m_d\}$.  Then, $T^m(b_k)=0$ for all $1\leq k\leq m$.  Let $v\in V$ and write $v=v^1\cdot b_1+\cdots +v^d\cdot b_d$.  Then,
		\begin{equation}
			T^m(v)=v^1\cdot T^m(b_1)+\cdots +v^d\cdot T^m(b_d)=0.
		\end{equation}
		Hence, $T^m=0$, and so $T$ is nilpotent.
	\end{proof}
\end{prp}

\subsubsection{More than a glimpse of generalized-eigenspaces}

Having understood the behavior of $T$ on subspaces of the form $\Ker ([T-\lambda ]^{\dim (V)})$, it is time we actually decompose $V$ into such subspaces.  Before we do so, we give such subspaces a name.
\begin{dfn}{Generalized-eigenspaces}{GeneralizedEigenspaces}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $W\subseteq V$ be a nonzero subspace, and let $\lambda \in \K$.  Then, $W$ is a \term{$\lambda$-generalized-eigenspace}\index{Generalized-eigenspace} iff
	\begin{enumerate}
		\item $\restr{T}{W}=\lambda \id _W+N$ with $N\colon W\rightarrow W$ locally-nilpotent linear; and
		\item $W$ is maximal with this property.
	\end{enumerate}
	\begin{rmk}
		Such a $\lambda$ is referred to a \term{generalized-eigenvalue}\index{Generalized-eigenvalue} of $T$.
	\end{rmk}
	\begin{rmk}
		Nonzero elements of $W$ are \term{generalized-eigenvectors} of $T$ with eigenvalue $\lambda$.
	\end{rmk}
	\begin{rmk}
		Note that it follows immediately from this that scaling by $\lambda$ on $W$ is linear.  Furthermore, we shall see shortly (\cref{prp4.6.25}) that generalized-eigenvalues are just eigenvalues,\footnote{Which is why you probably won't see the term ``generalized-eigenvalue'' in most places---it turns out they're just the same as eigenvalues, and so people just stick with using the single term ``eigenvalue'' for everything.} and so will in particular be central, at least for vector spaces (\cref{thm4.2.17}).
	\end{rmk}
	\begin{rmk}
		See the definition of eigenspaces (\cref{Eigen}) for some other potentially useful remarks that apply equally well here.
	\end{rmk}
\end{dfn}
As before, we start by establishing uniqueness of generalized-eigenspaces.
\begin{prp}{Generalized-eigenspaces are unique}{}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $\lambda \in \K$, and let $U,W\subseteq V$ be $\lambda$-generalized-eigenspaces of $T$.  Then, $U=W$.
	\begin{rmk}
		We denote the unique $\lambda$-generalized-eigenspace of $V$ by $\Eig _{\lambda ,T}^{\infty}$\index[notation]{$\Eig _{\lambda ,T}^{\infty}$} (for reasons that we will understand shortly).\footnote{See \cref{RankOfGeneralizedEigenvectors} to see why the use of ``$\infty$'' is appropriate here.}  If $T$ is clear from context, we may simply write $\Eig _{\lambda}^{\infty}\ceqq \Eig _{\lambda ,T}^{\infty}$\index[notation]{$\Eig _{\lambda}^{\infty}$}.
	\end{rmk}
	\begin{rmk}
		If ever we write ``$\Eig _{\lambda ,T}$'' without having explicitly said so, it should be assumed that $\lambda$ is an eigenvalue of $T$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				See the proof of \cref{prp4.2.3}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{Generalized-eigenspaces are maximum with their defining property}{}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $W\subseteq V$ be a subspace, and let $\lambda \in \K$ be a generalized-eigenvalue of $V$.  Then, if $\restr{T}{W}=\lambda \id _W+N$ with $N\colon W\rightarrow W$ locally-nilpotent linear, then $W\subseteq \Eig _{\lambda}$.
	\begin{rmk}
		A corollary of this is that, to show that $\lambda$ is a generalized-eigenvalue, it suffices to exhibit a single nonzero subspace $W$ on which $T-\lambda$ is locally-nilpotent linear.  For this result implies that $\Eig _{\lambda}^{\infty}\supseteq W$, so that if $W$ is nonzero, so is $\Eig _{\lambda}^{\infty}$, in which case $\lambda$ would be a generalized-eigenvalue.
	\end{rmk}
	\begin{rmk}
		Note that this is one reason why we prefer locally-nilpotent over nilpotent in the definition of generalized-eigenspaces.  Were we to use ``nilpotent'' in place of ``locally-nilpotent'' in the definition, this result would fail.  A counter-example is given by the left-shift operator on $\C ^{\infty}$---see \cref{exm4.4.129}.
	\end{rmk}
	\begin{proof}
		Suppose that $\restr{T}{W}=\lambda \id _W+N$ with $N\colon W\rightarrow N$ locally-nilpotent linear.  Define
		\begin{equation}
			\begin{split}
				\collection{U} & \ceqq \left\{ U\subseteq V\text{ a subspace}:\restr{T}{U}-\lambda \id _U\text{ is}\right. \\ & \qquad \left. \text{locally-nilpotent linear.}\right\} 
			\end{split}
		\end{equation}
		and
		\begin{equation}
		E\ceqq \sum _{U\in \collection{U}}U.
		\end{equation}
		
		Let $e\in E$ and and write $e=u_1+\cdots +u_m$ for some $u_1\in U_1,\ldots ,u_m\in U_m$ with $U_1,\ldots ,U_m\in \collection{U}$.  As $T-\lambda$ is locally-nilpotent on $U_k$, there is some $n_k\in \N$ such that $[T-\lambda ]^{m_k}(u_k)=0$.  Define $n\ceqq \max \{ n_1,\ldots ,n_m\}$.  Then,
		\begin{equation}
			[T-\lambda ]^n(e)=[T-\lambda ]^n(u_1)+\cdots +[T-\lambda ]^n(u_m)=0.
		\end{equation}
		That is, $\restr{T}{E}=\lambda \id _E+N$ with $N\colon E\rightarrow E$ locally-nilpotent linear.  $E$ is maximal with this property, and hence $E=\Eig _{\lambda}^{\infty}$.  Finally, note that $W\in \collection{U}$, and hence $W\subseteq E=\Eig _{\lambda}^{\infty}$. 
	\end{proof}
\end{prp}
\begin{prp}{}{prp4.6.25}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \K$.  Then, $\lambda$ is a generalized-eigenvalue of $T$ iff it is an eigenvalue of $T$.
	\begin{rmk}
		Thus, hereafter, we will likely stick with just ``eigenvalue'' and stop using the term ``generalized-eigenvalue''.
	\end{rmk}
	\begin{proof}
		$(\Rightarrow )$ Suppose that $\lambda$ is a generalized-eigenvalue of $T$.  Then, $\Eig _{\lambda}^{\infty}\subseteq V$ is a subspace of $V$ on which $T-\lambda$ is locally-nilpotent and is maximal with this property.
		
		For succinctness of notation, let us temporarily write $N\ceqq \restr{T-\lambda}{\Eig _{\lambda}^{\infty}}$, so that $N\colon \Eig _{\lambda}^{\infty}\rightarrow \Eig _{\lambda}^{\infty}$ is locally-nilpotent linear.  If $\Ker (N)=0$, then $N$ would be injective, and so $N^m$ would be injective for all $m\in \N$, and hence $N$ couldn't possibly be locally-nilpotent.  Thus, $\Ker (N)\subseteq V$ is a nonzero subspace such that $\restr{T}{\Ker (N)}=\lambda \id _{\Ker (N)}$, and therefore $\lambda$ is an eigenvalue of $T$.
		
		\blni
		$(\Leftarrow )$ Suppose that $\lambda$ is an eigenvalue of $T$.  Then, $\Eig _{\lambda}$ is a nonzero subspace on which $T-\lambda$ is locally-nilpotent, and so $\lambda$ is a generalized-eigenvalue of $T$.
	\end{proof}
\end{prp}
\begin{prp}{Generalized-eigenvalues are unique (in vector spaces)}{}
	Let $V$ be a vector space and let $T\colon V\rightarrow V$ be linear.  Then, if $\Eig _{\lambda ,T}^{\infty}=\Eig _{\mu ,T}^{\infty}$, it follows that $\lambda =\mu$.
	\begin{proof}
		Suppose that $\Eig _{\lambda ,T}^{\infty}=\Eig _{\mu ,T}^{\infty}$.  For convenience, let us write $\Eig _{\lambda ,T}^{\infty}\eqqc E\ceqq \Eig _{\mu ,T}^{\infty}$.  There is then a locally-nilpotent operator $N\colon E\rightarrow E$ such that
		\begin{equation}
			\lambda \id _E+N=\restr{T}{E}=\mu \id _E+N
		\end{equation}
		Let $v\in E$ be nonzero.  Then,
		\begin{equation}
			\lambda v+N(v)=\mu v+N(v),
		\end{equation}
		and hence $(\lambda -\mu )v=0$.  As $v\neq 0$ and the ground ring is a division ring, we must have that $\lambda -\mu =0$, that is, $\lambda =\mu$.
	\end{proof}
\end{prp}
We also have a result analogous to \cref{thm4.2.17} that is useful for actually `calculating' generalized-eigenspaces.
\begin{thm}{}{thm4.4.143}
	Let $V$ be a vector space over a division ring $\F$, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \F$.  Then, if $\lambda$ is an eigenvalue of $T$, then
	\begin{equation}
		\begin{split}
			\Eig _{\lambda ,T}^{\infty} & =\left\{ v\in V:[T-\lambda ]^m(v)=0\text{for some }m\in \N \text{.}\right\} \\
			& =\bigcup _{m\in \N}\Ker ([T-\lambda ]^m).
		\end{split}
	\end{equation}
	In fact, if $V$ is finite-dimensional, then
	\begin{equation}
		\Eig _{\lambda ,T}^{\infty}=\Ker ([T-\lambda ]^{\dim (V)}).
	\end{equation}
	The analogous result for eigenvalues \cref{thm4.2.17} had two parts whereas this result only has one.  This is because the first part of \cref{thm4.2.17} gives an explicit characterization of eigenvalues, but as eigenvalue and generalized-eigenvalues are the same, there is no additional characterization here.
	\begin{proof}
		Suppose that $\lambda$ is an eigenvalue of $T$.  Define
		\begin{equation}
			W\ceqq \left\{ v\in V:[T-\lambda ]^m(v)=0\text{for some }m\in \N \text{.}\right\} =\bigcup _{m\in \N}\Ker ([T-\lambda ]^m).
		\end{equation}
		\begin{exr}[breakable=false]{}{}
			Check that $W$ is a subspace.
		\end{exr}
		$W$ contains $\Ker (T-\lambda )$, and so is nonzero as $\lambda$ is an eigenvalue.  By definition, $T-\lambda$ is locally-nilpotent on $W$.
		
		To show maximality, let $U\supseteq W$ be a subspace such that $\restr{T}{U}=\lambda \id _U+N$ for $N\colon U\rightarrow U$ locally-nilpotent.  Then, for every $u\in U$, there is some $m\in \N$ such that $N^m(u)=0$, in which case $[T-\lambda ]^m(u)=N^m(u)=0$.  Thus, $u\in W$, so that $U\subseteq W$, showing maximality.  Hence,
		\begin{equation}
		W\ceqq \left\{ v\in V:[T-\lambda ]^m(v)=0\text{for some }m\in \N \text{.}\right\} =\Eig _{\lambda ,T}^{\infty}.
		\end{equation}
		
		Now suppose that $V$ is finite-dimensional.  It follows from \cref{lma4.6.2} that
		\begin{equation}
			\Eig _{T,\lambda}^{\infty}=\bigcup _{m\in \N}\Ker ([T-\lambda ]^m)=\Ker ([T-\lambda ]^{\dim (V)}).
		\end{equation}
	\end{proof}
\end{thm}
Related to the fact that we can write the generalized-eigenspace as a union in this way is that we now have a notion of \emph{rank}.\footnote{Well, I suppose we had a notion of rank before with just eigenvalues, but it's a bit silly because all eigenvalues have rank $1$, as you'll see in the following definition.}
\begin{dfn}{Rank (of generalized-eigenvectors)}{RankOfGeneralizedEigenvectors}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, let $\lambda \in \K$ be an eigenvalue of $T$, and let $v\in \Eig _{\lambda}^{\infty}$.  Then, the \term{rank}\index{Rank (of a generalized-eigenvector)} of $v$ is the smallest natural number $\Rank (v)$\index[notation]{$\Rank (v)$} such that $[T-\lambda ]^{\Rank (v)}(v)=0$.
	\begin{rmk}
		For $m\in \N$, the \term{rank $m$ $\lambda$-eigenspace}\index{Rank $m$ eigenspace}, $\Eig _{T,\lambda}^m$, is defined by
		\begin{equation}
			\Eig _{T,\lambda}^m\ceqq \left\{ v\in \Eig _{T,\lambda}^{\infty}:\Rank (v)\leq m\right\} .
		\end{equation}\index[notation]{$\Eig _{T,\lambda}^m$}
		As before, we will often write $\Eig _{\lambda}^m\ceqq \Eig _{\lambda ,T}^m$\index[notation]{$\Eig _{\lambda}^m$}.
	\end{rmk}
	\begin{rmk}
		We may say ``rank $m$ eigenvector'' instead of the more verbose ``rank $m$ generalized-eigenvector''---the fact that we are mentioning the rank at all implies that we do not mean eigenvectors is the `usual' sense (unless of course $m$ happens to be $1$).
	\end{rmk}
	\begin{rmk}
		Warning:  Note that $\Eig _{\lambda ,T}^m$ is \emph{not} the space of rank $m$ eigenvectors, but rather, the space of generalized-eigenvectors with rank \emph{at most} $m$.  The set of generalized-eigenvectors with rank exactly equal to $m$ is not a subspace in general.
	\end{rmk}
	\begin{rmk}
		As $\restr{T-\lambda}{\Eig _{\lambda}^{\infty}}$ is locally-nilpotent, there is by definition some such natural number.
	\end{rmk}
	\begin{rmk}
		Note that the rank $1$ generalized-eigenvectors are precisely the (`ordinary') eigenvectors.  Also note that $0\in \Eig _{\lambda ,T}^{\infty}$ has rank $0$, though it is technically not a generalized-eigenvector.
	\end{rmk}
	\begin{rmk}
		Hence,
		\begin{equation}
			\Eig _{\lambda ,T}^1=\Eig _{\lambda ,T}
		\end{equation}
		and
		\begin{equation}
			\Eig _{\lambda ,T}^0=0.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		Note that $\Rank (v)\leq \dim (V)$ if $V$ is finite-dimensional by the previous result \cref{thm4.4.143}.
	\end{rmk}
\end{dfn}
\begin{prp}{Properties of higher rank eigenspaces}{prp4.4.154}
	Let $V$ be a $\K$-module, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \K$ be an eigenvalue of $T$.  Then,
	\begin{enumerate}
		\item \label{prp4.4.154(i)}
		\begin{equation}
			\Eig _{\lambda}^m=\Ker ([T-\lambda ]^m);
		\end{equation}
		\item \label{prp4.4.154(ii)}$\Eig _{\lambda}^m\subseteq V$ is a $T$-invariant subspace; and
		\item
		\begin{equation}
			\Eig _{\lambda}^0\subseteq \Eig _{\lambda}^1\subseteq \Eig _{\lambda}^2\subseteq \cdots ;
		\end{equation}
		and
		\item \label{prp4.4.154(iii)}if $v\in \Eig _{\lambda}$ has rank $m\in Z^+$, then $[T-\lambda ](v)$ has rank $m-1$.
	\end{enumerate}
	\begin{rmk}
		In particular, $\Eig _{\lambda}^0=0$ and $\Eig _{\lambda}^1=\Eig _{\lambda}$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

Recall (\cref{prp4.2.22}) that eigenvectors with distinct eigenvalues are linearly-independent.  The same is true for generalized-eigenvectors (and in fact this is a strict generalization of the previous result).
\begin{prp}{Generalized-eigenvectors with distinct ei\-genvalues are linearly-independent}{prp4.4.157}
	Let $V$ be a vector space, let $T\colon V\rightarrow V$ be linear, let $\lambda _1,\ldots ,\lambda _m$ be distinct eigenvalues of $T$, and let $v_k\in \Eig _{\lambda _k}^{\infty}$ be nonzero $1\leq k\leq m$.  Then, $\{ v_1,\ldots ,v_m\}$ is linearly-independent.
	\begin{proof}
		Denote the ground division ring by $\F$.  Suppose that
		\begin{equation}
			0=\alpha _1\cdot v_1+\cdots +\alpha _m\cdot v_m.
		\end{equation}
		Let $n\in \Z$ be the smallest positive integer such that $w_1\ceqq [T-\lambda _1]^n(v_1)\neq 0$, so that $[T-\lambda _1](w_1)=0$, that is, $T(w_1)=\lambda _1w_1$.  It follows that $[T-\lambda ](w_1)=(\lambda _1-\lambda )w$ for any $\lambda \in \F$.  Apply $[T-\lambda _1]^n$ to this equation to obtain
		\begin{equation}
			0=\alpha _1\cdot w_1+\cdots +\alpha _m\cdot w_m,
		\end{equation}
		where we have defined $w_k\ceqq [T-\lambda _1]^n(v_k)$.  Note that, as generalized-eigenspaces are $T$-invariant subspaces and $\lambda _1\in \F$ is central, $w_k\ceqq [T-\lambda _1]^n(v_k)\in \Eig _{\lambda _k}^{\infty}$ is still in the $\lambda _k$-generalized-eigenspace.  Hence, for each $2\leq k\leq m$, there is some $n_k\in \Z ^+$ such that $[T-\lambda _k]^{n_k}(w_k)=0$.  Applying $[T-\lambda _2]^{n_2}\cdots [T-\lambda _m]^{n_m}$ to this equation, we thus find
		\begin{equation}
			0=\alpha _1(\lambda _1-\lambda _2)^{n_2}\cdots (\lambda _1-\lambda _m)^{n_m}.
		\end{equation}
		As the eigenvalues are distinct and we're working over a division ring, this implies that $\alpha _1=0$.
		
		Proceeding inductively and doing the same thing for $\alpha _2$, $\alpha _3,$ and so on, we eventually obtain $0=\alpha _1=\cdots =\alpha _m$, and so $\{ v_1,\ldots ,v_m\}$ is linearly-independent.
	\end{proof}
\end{prp}

We are now essentially ready to put all the pieces together.

\subsection{The Jordan Canonical Form Theorem}

Okay, so that was a lie.  There is still one minor detail we need to address:  how do we know we have any eigenvalues at all!?  Indeed, we don't know that.  It's just not true.  We saw in \cref{exm4.3.7} an example of a linear-transformation that didn't have any eigenvalues.  Fortunately, there is a natural hypothesis would can impose on the ground division ring in order to guarantee we have eigenvalues, in which case (\cref{GeneralizedEigenspaceDecomposition}), not only do we have a single eigenvalue, but we have `enough' generalized-eigenvectors to generate the entire space.  To state this hypothesis, however, we're going to need to study polynomials in a bit more depth.

\subsubsection{Polynomials and algebraic-closure}

We mentioned after \cref{exm4.3.7} that, for a given linear operator, there is a polynomial, called the characteristic polynomial, of which all the eigenvalues are a root, and that essentially the reason that the matrix given in \cref{exm4.3.7} was diagonalizable over $\C$ but not over $\R$ was because the characteristic polynomial in this case had complex but not real roots.  This suggests that the property we want to impose on the ground division ring has something to do with roots of polynomials.  So, before anything else, let's be absolutely clear what we mean by the term ``root''.
\begin{dfn}{Root}{Root}
	Let $\K$ be a ring, let $p\in \K [x]$, and let $x_0\in \K$.  Then, $x_0$ is a \term{root}\index{Root} of $p$ iff $p(x_0)=0$.
	\begin{rmk}
		Write $p(x)=a_0+a_1x+\cdots +a_mx^m$.  Then,
		\begin{equation}
			p(x_0)\ceqq a_0+a_1x_0+\cdots +a_mx_0^m.
		\end{equation}
		Note that in general this is \emph{not} the same as $a_0+x_0a_1+\cdots +x_0^ma_m$.  For this reason, this is sometimes called a \term{right root} of $p$ for emphasis, though strictly speaking this is not necessary as one is only interested in right roots of elements of $\K [x]$.  Roots of elements of $[x]\K$\index[notation]{$[x]\K$},\footnote{I can't say I've ever actually seen this notation used, but what else could it possibly mean?} polynomials with coefficients written on the right, would then be called \term{left roots}.  
	\end{rmk}
	\begin{rmk}
		Thus, the definition was probably exactly what you thought it would be, but we gave it just to be absolutely sure that noncommutativity didn't change the definition in any serious way.  Be warned, however:  though it didn't change the definition, it \emph{will} change the behavior of roots.  For example, we no longer have $[pq](x_0)=p(x_0)q(x_0)$, and so we can no longer conclude that $x_0$ is a root of $pq$ if it is a root of $p$.  (It will be true that $x_0$ is a root of $pq$ if it's a root of $q$, though hopefully this makes it clear we have to be more careful with our proof than you might have previously thought.)
	\end{rmk}
\end{dfn}
\begin{exm}{A degree $2$ polynomial with infinitely many roots}{}
	The polynomial $x^2+1$ has infinitely many roots in $\H$.\footnote{Of course, it has none in $\R$ and $2$ in $\C$.}
	\begin{exr}[breakable=false]{}{}
		Check this.
	\end{exr}
	\begin{exr}[breakable=false]{}{}
		On the other hand, show that $x^2+1$ doesn't have \emph{any} central roots.
	\end{exr}
	\begin{rmk}
		For fields, it is well-known (perhaps even to you) that a polynomial of degree $m$ can have at most $m$ roots.  Thus, this phenomenon is perhaps quite startling:  in what is arguably the simplest of all noncommutative division rings, we have a polynomial of degree $2$ with infinitely many roots!.\footnote{Of course, we can't do this with degree less than $2$.}  Instead, this classical result generalizes to the statement that the set of all roots of $p$ belong to at most $m$ \emph{conjugacy classes} in $D$.\footnote{You don't need to know what ``conjugacy class'' means.  This is just a comment for those that already know and those that are curious enough to look up the details.  By the way, this theorem has a name:  it's called the \emph{Gordon-Motzkin Theorem}\index{Gordon-Motzkin Theorem}.}  Furthermore, it \emph{is} true that either (i) there are infinitely many roots or (ii) the number of roots is less than $m$.  For example, even over the most noncommutative division rings, you can't find a degree $3$ polynomial with $6$ roots:  the only possibilities are $0$, $1$, $2$, $3$, and infinitely many roots.
	\end{rmk}
\end{exm}
\begin{dfn}{Algebraically-closed}{AlgebraicallyClosed}
	Let $\K$ be a ring.
	\begin{enumerate}
		\item $\K$ is \term{right algebraically-closed}\index{Right algebraically-closed} iff every nonconstant $p\in \K [x]$ has a root.
		\item $\K$ is \term{left algebraically-closed}\index{Left algebraically-closed} iff every nonconstant $p\in [x]\K$ has a root.
		\item $\K$ is \term{algebraically-closed}\index{Algebraically-closed} iff it is both right and left algebraically-closed.
	\end{enumerate}
	\begin{rmk}
		The reason why this is \emph{right} algebraically-closed (and not left) is that the statement that $\alpha \in \K$ is a root of $p$ is equivalent to the statement that $p(x)=q(x)(x-\alpha )$ for some $q\in \K [x]$\footnote{This is \emph{not} in general equivalent to the statement that $p(x)=(x-\alpha )r(x)$ for some $r\in \K [x]$ if $\K$ is noncommutative.}---see the following result \cref{prp4.4.174}.  Similarly for left algebraically-closed.
	\end{rmk}
	\begin{rmk}
		In the commutative case, the left and right versions are equivalent to one another.  Thus, in the commutative case, there is essentially just one condition, simply ``algebraically-closed''.
	\end{rmk}
\end{dfn}
\begin{exm}{}{}
	Note that $\C$ is algebraically-closed but $\R$ is not.  $\H$ is also (left and right) algebraically-closed.
\end{exm}
\begin{exr}{}{}
	Can you find an example of a division ring that is left algebraically-closed but not right algebraically-closed (or vice versa)?
\end{exr}
\begin{prp}{}{prp4.4.174}
	Let $\F$ be a division ring and let $x_0\in \F$.
	\begin{enumerate}
		\item $x_0$ is a root of $p\in \F[x]$ iff there is some $q\in \F [x]$ such that $p(x)=q(x)(x-x_0)$.
		\item $x_0$ is a root of $p\in [x]\F$ iff there is some $q\in [x]\F$ such that $p(x)=(x-x_0)q(x)$.
	\end{enumerate}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  See \cite[Proposition 16.2]{Lam}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{prp}

\subsubsection{The theorem.  Finally.}

We're finally ready to state the Jordan Canonical Form Theorem, which, among other things, details the near-diagonal form our matrices will take in a suitable basis.  To state it, however, it will be convenient to have a name for a special type of matrix appearing in the description.
\begin{dfn}{Jordan block}{JordanBlock}
	Let $\K$ be a ring, let $\lambda \in \K$, and let $m\in \Z ^+$.  Then, the \term{Jordan block}\index{Jordan block}, $\Jord _{\lambda ,m}$\index[notation]{$\Jord _{\lambda ,m}$}, of size $m$ with eigenvalue $\lambda$ is the $m\times m$ matrix
	\begin{equation}
	\Jord _{\lambda ,m}\ceqq \begin{bmatrix}\lambda & 1 & 0 & \cdots & 0& 0 \\ 0 & \lambda & 1 & \cdots & 0 & 0 \\ \vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & \lambda & 1 \\ 0 & 0 & 0 & \cdots & 0 & \lambda\end{bmatrix}.
	\end{equation}
	\begin{rmk}
		Note that this can be written as
		\begin{equation}
		\Jord _{\lambda ,m}=\lambda \id +N_m,
		\end{equation}
	\end{rmk}
	where we have used the notation
	\begin{equation}
	N_m\ceqq \begin{bmatrix}0 & 1 & 0 & \cdots & 0 & 0 \\ 0 & 0 & 1 & \cdots & 0 & 0 \\ \vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 0 & 1 \\ 0 & 0 & 0 & \cdots & 0 & 0\end{bmatrix}.
	\end{equation}
\end{dfn}
And now, what is arguably the most important theorem in the entirety of the notes.
\begin{thm}{Jordan Canonical Form Theorem}{GeneralizedEigenspaceDecomposition}
	Let $V$ be a finite-dimensional vector space over an algebraically-closed field $\F$, let $T\colon V\rightarrow V$ be linear, and denote the eigenvalues of $T$ by $\lambda _1,\ldots ,\lambda _m$.  Then,
	\begin{enumerate}
		\item \label{GeneralizedEigenspaceDecomposition(i)}
		\begin{equation}\label{eqn4.4.173}
			V=\Eig _{\lambda _1}^{\infty}\oplus \cdots \oplus \Eig _{\lambda _m}^{\infty};
		\end{equation}
		\item \label{GeneralizedEigenspaceDecomposition(ii)}the coordinates of $T$ with respect to this direct-sum decomposition is
		\begin{equation}
			\begin{bmatrix}\lambda _1\id _{\Eig _{\lambda _1}^{\infty}}+N_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \lambda _m\id _{\Eig _{\lambda _m}^{\infty}}+N_m\end{bmatrix},
		\end{equation}
		where $N_k\colon \Eig _{\lambda _k}^{\infty}\rightarrow \Eig _{\lambda _k}^{\infty}$ is nilpotent linear;
		\item \label{GeneralizedEigenspaceDecomposition(iii)}for every $1\leq k\leq m$, there is a basis $\basis{B}_k$ for $\Eig _{\lambda _k}^{\infty}$ such that the coordinates
		\begin{equation}\label{eqn4.4.175}
			\coordinates{\lambda _k\id _{\Eig _{\lambda _k}^{\infty}}+N_k}{\basis{B}_k\leftarrow \basis{B}_k}
		\end{equation}
		is a direct-sum of Jordan blocks with eigenvalue $\lambda _k$;
		\item \label{GeneralizedEigenspaceDecomposition(iv)}$\basis{B}\ceqq \basis{B}_1\cup \cdots \basis{B}_m$ is a basis for $V$ such that the coordinates
		\begin{equation}\label{eqn4.4.176}
			\coordinates{T}{\basis{B}\leftarrow \basis{B}}
		\end{equation}
		is the direct-sum of the matrices in \eqref{eqn4.4.175}; and
		\item \label{GeneralizedEigenspaceDecomposition(v)}if $\basis{C}$ is another basis of $V$ that has the property that $\coordinates{T}{\basis{C}\leftarrow \basis{C}}$ is a direct-sum of Jordan blocks, then the set of Jordan blocks appearing in this decomposition is the same as the set of Jordan blocks appearing in the decomposition of $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$.
	\end{enumerate}
	\begin{rmk}
		\eqref{eqn4.4.173} is the \term{generalized-eigenspace decomposition}\index{Generalized-eigenspace decomposition} of $V$.
	\end{rmk}
	\begin{rmk}
		The matrix in \eqref{eqn4.4.176} is the \term{Jordan Canonical Form}\index{Jordan Canonical Form} of $T$.  It is also called the \term{Jordan Normal Form}\index{Jordan Normal Form} of $T$, but I prefer ``Jordan Canonical Form'' for the simple reason that the word ``normal'' tends to be overused in mathematics.
	\end{rmk}
	\begin{rmk}
		In short:
		
		\cref{GeneralizedEigenspaceDecomposition(i)} says that $V$ is a direct-sum of the generalized-eigenspaces.
		
		\cref{GeneralizedEigenspaceDecomposition(ii)} says that the matrix of $T$ with respect to this decomposition is block-diagonal with $\lambda _k\id _{\Eig _{\lambda _k}^{\infty}}+N_k$ on the diagonal.
		 
		\cref{GeneralizedEigenspaceDecomposition(iii)} says that we can choose bases for each of the generalized-eigenspaces in which the matrix associated to the linear operators $\lambda _k\id _{\Eig _{\lambda _k}^{\infty}}+N_k$ is a direct-sum of Jordan blocks.
		 
		\cref{GeneralizedEigenspaceDecomposition(iv)} says that these bases assemble together to form a basis for all of $V$ for which the matrix of $T$ is a direct-sum of the matrices of the previous part, and so in particular a direct-sum of Jordan blocks.
		
		\cref{GeneralizedEigenspaceDecomposition(v)} says that the Jordan Canonical Form of $T$ is essentially unique in the sense that, if you write $T$ as a direct-sum of Jordan blocks, the Jordan blocks appearing in that decomposition must be exactly the same as the ones you found before.
	\end{rmk}
	\begin{rmk}
		The reason for the asymmetry of the handedness in this statement is because $V$ is a \emph{left} $\F$-module.  Had we written scaling on the right so that $V$ were a right $\F$-module, then the hypothesis would instead be that $\F$ is \emph{left} algebraically-closed.
	\end{rmk}
	\begin{rmk}
		In particular, $T$ has at least one eigenvalue.
	\end{rmk}
	\begin{rmk}
		In particular, generalized-eigenvectors with distinct eigenvalues are linearly-independent.
	\end{rmk}
	\begin{rmk}
		This is one of the few times, and most likely the most significant time, where I was not able to remove the assumption of commutativity.  The reason for this is that eigenvalues are central, and so you would need to assume that all nonconstant polynomials have \emph{central} roots.  But if you do that, then you automatically have a field!\footnote{For every $\alpha \in \F$, $x-\alpha$ would have a central root, implying that $\alpha$ is central.}
	\end{rmk}
	\begin{proof}
		\cref{GeneralizedEigenspaceDecomposition(i)} We proceed by (strong induction) on $\dim (V)$.  If $\dim (V)=0$, the statement is vacuously true.
		
		So, let $d\ceqq \dim (V)\in \Z ^+$ and suppose that the result is true whenever the dimension of the vector space is less than $d$.
		\begin{lma}[break at=20cm/0pt]{}{}
			Let $V$ be a finite-dimensional vector space over an algebraically-closed field and let $T\colon V\rightarrow V$ be linear.  Then, $T$ has an eigenvalue.
			\begin{proof}
				Let $v\in V$ be nonzero.
				\begin{equation}
				\{ v,T(v),T^2(v),\ldots ,T^d(v)\}
				\end{equation}
				is a set of $d+1$ vectors in a vector space of dimension $d$, and so it must be linearly-depend.  Thus, there are $\alpha _0,\ldots ,\alpha _d\in \F$, not all zero, such that
				\begin{equation}\label{eqn4.4.180}
				0=\alpha _0\cdot v+\cdots +\alpha _d\cdot T^d(v).
				\end{equation}
				Note that it cannot be the case that $0=\alpha _1=\cdots =\alpha _d$, for then the above equation would in turn imply $\alpha _0=0$.  Hence,
				\begin{equation}
					p(x)\ceqq \alpha _0+\alpha _1x+\cdots +\alpha _dx^d\in \F [x]
				\end{equation}
				is a nonconstant polynomial.  As $\F$ is right algebraically-closed, there is a root $\lambda _1\in \F$ of $p$.  Hence, by \cref{prp4.4.174}, there is a polynomial $p_2\in \F [x]$ such that
				\begin{equation}
					p(x)=p_2(x)(x-\lambda _1).
				\end{equation}
				If $p_2$ is constant, stop.  Otherwise, $p_2$ has another root $\lambda _2\in \F$, in which case we can write
				\begin{equation}
					p(x)=p_3(x)(x-\lambda _2)(x-\lambda _1)
				\end{equation}
				for some $p_3\in \F [x]$.  Continuing inductively, we eventually find
				\begin{equation}
					p(x)=\alpha (x-\lambda _d)\cdots (x-\lambda _1)
				\end{equation}
				for some nonzero $\alpha \in \F$.
				
				By \eqref{eqn4.4.180}, we have that $[p(T)](v)=0$, and hence
				\begin{equation}
					0=\alpha [T-\lambda _d]\left( [T-\lambda _{d-1}]\left( \cdots ([T-\lambda _1](v))\cdots \right) \right) .
				\end{equation}
				If $[T-\lambda _{d-1}](\cdots )$ is nonzero, then $T-\lambda _d$ has nonzero kernel.  Otherwise, in fact we have $[T-\lambda _{d-1}](\cdots )=0$.  Now, if $[T-\lambda _{d-2}](\cdots )$ is nonzero, $T-\lambda _{d-1}$ has nonzero kernel.  Continuing inductively, we eventually find some $k$ such that $\Ker (T-\lambda _k)\neq 0$.  Thus, $\lambda _k$ is an eigenvalue of $T$.
			\end{proof}
		\end{lma}
	
		So, let $\lambda _1$ be an eigenvalue of $T$.  By \cref{thm4.4.143}, we have that $\Eig _{\lambda}^{\infty ,T}=\Ker ([T-\lambda ]^{\dim (V)})$.  Then, by \cref{prp4.6.1}, we have
		\begin{equation}\label{eqn4.4.178}
			V=\Eig _{\lambda ,T}^{\infty}\oplus U,
		\end{equation}
		where $U\ceqq \Ima ([T-\lambda ]^{\dim (V)})$ is $T$-invariant.
	
		Enumerate all the eigenvalues of $T$ by $\lambda _1,\ldots ,\lambda _m$.
		\begin{exr}[breakable=false]{}{}
			Show that the eigenvalues of $\restr{T}{U}\colon U\rightarrow U$ are $\lambda _2,\ldots ,\lambda _m$.
		\end{exr}
		By the induction hypothesis, we now have
		\begin{equation}\label{eqn4.4.180x}
			U=\Eig _{\lambda _2,\restr{T}{U}}^{\infty}\oplus \cdots \oplus \Eig _{\lambda _m,\restr{T}{U}}^{\infty}.
		\end{equation}
		Combining this with \eqref{eqn4.4.178} will complete the proof if we can show that $\Eig _{\lambda _k,\restr{T}{U}}^{\infty}=\Eig _{\lambda _k,T}^{\infty}$.  As the proof is the same for all $k$, it suffices to prove this for $k=2$.
	
		So, let $u\in \Eig _{\lambda _2,\restr{T}{U}}^{\infty}$.  There is then some $m\in \N$ such that $[T-\lambda _2]^m(u)=0$, and so $u\in \Eig _{\lambda _2,T}^{\infty}$.  In the other direction, let $v\in \Eig _{\lambda _2,T}^{\infty}$.  As before, we know that there is some $m\in \N$ such that $[T-\lambda _2]^m(v)=0$, but now we need to show additionally that $v\in U$.
	
		By \eqref{eqn4.4.178}, we can write
		\begin{equation}\label{eqn4.4.181}
			v=v_1+u
		\end{equation}
		for unique $v_1\in \Eig _{\lambda _1,T}^{\infty}$ and $u\in U$.  We wish to show that $v_1=0$.
	
		By \eqref{eqn4.4.180x}, we may write
		\begin{equation}
			u=u_2+\cdots +u_m
		\end{equation} 
		for unique $u_k\in \Eig _{\lambda _k,\restr{T}{U}}^{\infty}$.  Hence,
		\begin{equation}
			v=v_1+u_2+\cdots +u_m.
		\end{equation}
		As generalized-eigenvectors with distinct eigenvalues are linearly-independent (\cref{prp4.4.157}), this implies that $v_1=0$, $u_2=v$, and $u_3=\cdots =u_m=0$.  Thus, $v\in U$, as desired.
		
		\blni
		\cref{GeneralizedEigenspaceDecomposition(ii)} As each generalized-eigenspace is invariant (\cref{prp4.4.154}\cref{prp4.4.154(ii)}, the \namerefpcref{thm4.5.2} says that the matrix of $T$ with respect to this direct-sum decomposition is
		\begin{equation}
			\begin{bmatrix}\restr{T}{\Eig _{\lambda _1}^{\infty}} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \restr{T}{\Eig _{\lambda _m}^{\infty}}\end{bmatrix}
		\end{equation}
		By definition of generalized-eigenspaces, $N_k\ceqq \restr{T}{\Eig _{\lambda _k}^{\infty}}-\lambda _k\id _{\Eig _{\lambda _k}}^{\infty}$ is locally-nilpotent, hence nilpotent by finite-dimensionality (\cref{prp4.4.130}).  We thus have that the matrix of $T$ with respect to this direct-sum decomposition is indeed of the form
		\begin{equation}
			\begin{bmatrix}\lambda _1\id _{\Eig _{\lambda _1}^{\infty}}+N_1 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & \lambda _m\id _{\Eig _{\lambda _m}^{\infty}}+N_m\end{bmatrix},
		\end{equation}
		where $N_k\colon \Eig _{\lambda _k}^{\infty}\rightarrow \Eig _{\lambda _k}^{\infty}$ is nilpotent linear.
		
		\blni
		\cref{GeneralizedEigenspaceDecomposition(iii)} By \cref{thm4.6.17}, there is a basis $\basis{B}_k$ of $\Eig _{\lambda _k}^{\infty}$ such that
		\begin{equation}
			\coordinates{\restr{T}{\Eig _{\lambda _k}^{\infty}}-\lambda _k\id _{\Eig _{\lambda _k}^{\infty}}}{\basis{B}_k\leftarrow \basis{B}_k}
		\end{equation}
		is a direct-sum of Jordan blocks with eigenvalue $0$, and hence
		\begin{equation}
			\coordinates{\restr{T}{\Eig _{\lambda _k}^{\infty}}}{\basis{B}_k\leftarrow \basis{B}_k}
		\end{equation}
		is a direct-sum of Jordan blocks with eigenvalue $\lambda _k$.
		
		\blni
		\cref{GeneralizedEigenspaceDecomposition(iv)} As $\basis{B}_k$ is a basis for $\Eig _{\lambda _k}^{\infty}$ and $V=\Eig _{\lambda _1}^{\infty}\oplus \cdots \oplus \Eig _{\lambda _m}^{\infty}$, $\basis{B}\ceqq \basis{B}_1\cup \cdots \cup \basis{B}_m$ is a basis for $V$.  Furthermore, $\coordinates{T}{\basis{B}\leftarrow \basis{B}}$ is a block-diagonal matrix whose $k^{\text{th}}$ block is $\coordinates{\lambda _k\id _{\Eig _{\lambda _k}^{\infty}}+N_k}{\basis{B}_k\leftarrow \basis{B}_k}$.
		\begin{exr}[breakable=false]{}{}
			Check this.
		\end{exr}
		
		\blni
		\cref{GeneralizedEigenspaceDecomposition(v)} Let $\basis{C}$ is another basis of $V$ that has the property that $\coordinates{T}{\basis{C}\leftarrow \basis{C}}$ is a direct-sum of Jordan blocks.  The eigenvalues of such a direct-sum are the same as the eigenvalues of $T$, and so the eigenvalues appearing among the Jordan blocks are the same for both the bases $\basis{B}$ and $\basis{C}$.  So, fix $\lambda \in \F$ an eigenvalue of $T$, and for the moment consider only the restriction $S\ceqq \restr{T}{\Eig _{\lambda}^{\infty}}\colon \Eig _{\lambda}^{\infty}$.  As $$\coordinates{T}{\basis{C}\leftarrow \basis{C}}$$ is a direct-sum of Jordan blocks, the elements of $\basis{C}$ are all generalized-eigenvectors, and so $\basis{C}_{\lambda}\ceqq \basis{C}\cap \Eig _{\lambda}^{\infty}$ is a basis for $\Eig _{\lambda}^{\infty}$.  Similarly for $\basis{B}_{\lambda}\ceqq \basis{B}\cap \Eig _{\lambda}^{\infty}$.  Thus, $\coordinates{S}{\basis{B}}$ and $\coordinates{S}{\basis{C}}$ are both direct-sums of Jordan blocks and we would like to prove that the size of the Jordan blocks appearing in this decomposition are the same (the eigenvalues are of course all equal to $\lambda$).  Let $m\in \Z ^+$ be the smallest positive integer such that $[S-\lambda ]^m=0$ (there is some such $m$ as we know $S-\lambda$ is nilpotent).  It follows that $m$ is the smallest positive integer such that $\coordinates{[S-\lambda ]}{\basis{B}}^m=0$ and also the smallest positive integer such that $\coordinates{[S-\lambda ]}{\basis{C}}^m=0$.  It follows that the largest Jordan block appearing in both of these matrices is size $m$.  Now, replace $S$ with $S-\Jord _{\lambda ,m}$ and apply the same logic again.  We find again that the largest Jordan blocks appearing in these decompositions have the same size.  Proceeding inductively, we see that they have the same Jordan blocks.
	\end{proof}
\end{thm}