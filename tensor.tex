\chapter{Multilinear algebra and tensors}

Multilinear algebra is, well, it's the study of multilinear maps.  I realize that's not terribly enlightening, and for that to actually be a meaningful description at all, I had better first tell you what ``multilinear map'' actually means.  Before I do so, however, I think it's best to start with some motivation.

\section{Motivation}

\subsection{The derivative}

You'll recall from multivariable calculus\footnote{You have taken multivariable calculus, right?  In case you haven't, the TL;DR version is:  it's calculus, but when the functions have multiple variables.} the notion of the \emph{gradient}, which really should be thought of as the derivative in higher dimensions.  Given a smooth\footnote{Recall that ``smooth'' means infinitely-differentiable with continuous derivatives.  I assume this so that $\nabla f$ actually exists.} function $f\colon \R ^d\rightarrow \R$, you were probably taught something like ``The gradient of a $f$ is the vector (field) whose coordinates are given by the partial derivatives.''.  LIES!

The gradient is \emph{not} a vector field---it is a \emph{co}vector field.  The derivative $\nabla _af(x)$ is a function which takes in a tangent vector\footnote{You don't need to know the precise definition of tangent vector or tangent space, only that $v^a$ is supposed to indicate (at an intuitive level) a ``direction''.} $v^a\in \tangent{\R ^d}[x]$ and spits out a number, the \emph{directional derivative} of $f$ at the point $x$ in the direction $v$:  $v^a\nabla _af(x)$.\footnote{Don't worry about the indices fo now.  They will be explained later---see \cref{sct5.4}.}  Thus, the derivative is not itself a vector, but rather, it's something that \emph{takes in} vectors and \emph{spits out} numbers.  Such things are called \emph{covectors} (or linear-functionals), and this will motivate us to introduce the \emph{dual-space}.

So what about the second derivative then?  Well, the second derivative is something that takes in \emph{two} vectors and spits out a number, this number itself being the directional derivative in the direction of the first vector of the directional derivative in the direction of the second vector.  The third derivative is a thing that takes in \emph{three} vectors and spits out a number, and so on.  The dual-space will give us things that takes in single vectors and spits out numbers, but to obtain objects that take in multiple vectors and spit out numbers, we'll need to discuss (higher rank) \emph{tensors} and hence the \emph{tensor product}.

\subsection{Unification}

If that's not satisfying to you, another motivation for the introduction of tensors is that they can be viewed as a unifying concept for all of linear algebra in the sense that nearly every concept one encounters can be thought of as a tensor.  For example, scalars, vectors, covectors, linear-transformations, inner-products, pairings (in a dual-pair), etc.\textellipsis all of these are tensors.

\subsection{Multilinear-transformations}

Okay, cool, so tensors are a thing we should care about.  But what does that have to do with multilinear-transformations?  I suppose the answer is that we ultimately need them to define tensors, and in particular, the tensor product---see \cref{TensorProduct}.  For example, this was already hinted at when we noted that the second derivative (which is supposed to be (and in fact will be) a tensor) takes in two vectors and spits out a number---as it does so in such a way that it is linear in each of the vectors, this is one example of how a tensor is really just a type of multilinear map.

While we won't begin study of multilinear maps proper for a couple of sections, we introduce the definition here, as we shall occasionally make use of the terminology.
\begin{dfn}{Multilinear-transformation\hfill}{MultilinearTransformation}
	Let $V_1,\ldots ,V_m$ be respectively $\K _k$-$\K _{k+1}$-bimodules, let $V$ be a $\K _1$-$\K _{m+1}$-bimodule, and let $T\colon V_1\times \cdots \times V_m\rightarrow V$ be a function.  Then, $T$ is \term{multilinear}\index{Multilinear-transformation} iff
	\begin{enumerate}
		\item
		\begin{equation}
			V_k\in v\mapsto T(v_1,\ldots ,v_{k-1},v,v_{k+1},\ldots ,v_m)\in V
		\end{equation}
		is a group homomorphism for all $1\leq k\leq m$;
		\item
		\begin{equation}
			T(\alpha \cdot v_1,v_2,\ldots ,v_{m-1},v_m\cdot \beta )=\alpha \cdot T(v_1,v_2\ldots ,v_{m-1},v_m)\cdot \beta
		\end{equation}
		for $\alpha \in \K _1$ and $\beta \in \K _{m+1}$; and
		\item
		\begin{equation}
			\begin{multlined}
				T(v_1,\ldots ,v_k\cdot \alpha ,v_{k+1},\ldots ,v_m) \\ =T(v_1,\ldots ,v_k,\alpha \cdot v_{k+1},\ldots ,v_m)
			\end{multlined}
		\end{equation}
		for $\alpha \in \K _k$.
	\end{enumerate}
	\begin{rmk}
		If $m=2$, the term \term{bilinear}\index{Bilinear-transformation} is more commonly used instead of ``multilinear-transformation''.  While I can't say I've heard the term before, it would stand to reason that \term{trilinear}\index{Trilinear-transformation} would be used for the $m=3$ case, etc..
	\end{rmk}
	\begin{rmk}
		In essence, this means that (i) each argument preserves addition and (ii) you can move scalars around as you please just so long as you don't move scalars past vectors.
		
		If you find this confusing, I wouldn't worry.  Our interest is primarily in the commutative case, in which case these conditions simplify to something more understandable---see \cref{prp5.1.1}.
	\end{rmk}
	\begin{rmk}
		At this level of generality, you might hear this concept being referred to as \term{balanced}\index{Balanced linear-transformation}, in which case ``multilinear-linear transformation'' would only be used in the commutative case where the condition simplifies---see \cref{prp5.1.1}.
	\end{rmk}
\end{dfn}
Recall that (\cref{exm1.1.55}) if $\K$ is commutative, then a left $\K$-module obtains a canonical right module structure and vice-versa, and does so in such a way so as to `commute' with the vectors:  $\alpha \cdot v=v\cdot \alpha$.  Thus, in this case, the above condition simplifies to the following.
\begin{prp}{}{prp5.1.1}
	Let $V_1,\ldots ,V_m,V$ be $\K$-modules, $\K$ a cring, and let $T\colon V_1\times \cdots \times V_m\rightarrow V$ be a function.  Then, $T$ is multilinear iff
	\begin{equation}
		V_k\ni v\mapsto T(v_1,\ldots ,v_{k-1},v,v_{k+1},\ldots ,v_m)\in V
	\end{equation}
	is linear for all $1\leq k\leq m$.
	\begin{rmk}
		In other words, over commutative rings, multilinear is equivalent to being linear in every argument.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

\section{Dual-pairs and dual-spaces}

We've already stated why we should care about covectors and the dual-space.  After having defined the dual-space $V^{\T}$ of a vector space $V$ (over a ground ring $\K$), we will obtain a \emph{bilinear} map $V^{\T}\times V\rightarrow \K$, yielding our first example of a \emph{dual-pair}.  In fact, it is essentially the only example of a dual-pair that we'll be interested in, but using this language will make the theory less `clunky'.  For example, working with dual-spaces themselves, our definitions would have to be written in such a way that, for example, $[T^{\T}]^{\T}$ is map from $[V^{\T}]^{\T}$ to $[W^{\T}]^{\T}$, whereas using the language of dual-pairs, it is instead a map from $V$ to $W$.

\subsection{Dual-spaces}

\begin{dfn}{Dual-space}{DualSpace}
	Let $V$ be a $\K$-$\K$-bimodule.  Then, the \term{dual-space}\index{Dual-space} of $V$ is the $\K$-$\K$-bimodule
	\begin{equation}
		V^{\T}\ceqq \Mor _{\Mod{\K}}(V,\K ).
	\end{equation}\index[notation]{$V^{\T}$}
	\begin{rmk}
		We also say simply that $V^{\T}$ is the \term{dual}\index{Dual (of a bimodule)} of $V$.
	\end{rmk}
	\begin{rmk}
		Elements of $V^{\T}$ are \term{covectors}\index{Covectors} or \term{linear-functionals}\index{Linear-functional}.  The terms are synonymous, though ``covector'' is more commonly used in the context of tensors while ``linear-functional'' tends to be used elsewhere.
	\end{rmk}
	\begin{rmk}
		Warning:  This definition is \emph{tentative}.  It will later be replaced by the more general definition given in \cref{TopologicalDualSpace}.  What we really should have said is that the dual-space is the space of all \emph{continuous} linear-transformations from $V$ to $\K$.\footnote{In fact, as $V$ secretly has the discrete topology, all linear-transformations $V\rightarrow \K$ are continuous, and so in this case the two definitions agree.  In general, however, they disagree, and in the general context, you want to consider only the \emph{continuous} linear-functionals.}
	\end{rmk}
	\begin{rmk}
		In other words, the elements of $V^{\T}$ take in elements of $V$ and spit out scalars.  Recall that the motivating example of this was the derivative:  this takes in a vector (the direction in which to differentiate) and spits out a number (the directional derivative in that direction).
	\end{rmk}
	\begin{rmk}
		Recall that (\cref{sss1.1.2}) $\Mor _{\Mod{\K}}(V,W)$ does not have the structure of a module if $V$ and $W$ are just modules.  If we want morphism sets to have some sort of nontrivial module structure, we need to work with \emph{bimodules} from the get-go.  Requiring that $V$ be a $\K$-$\K$-bimodule ensures that $\Mor _{\Mod{\K}}(V,\K )$ is a $\K$-$\K$-bimodule as well, so that $V$ and $V^{\dagger}$ are the same type of object.\footnote{Note that $\K$ is a $\K$-$\K$-bimodule (\cref{RingBimodule}), and hence $V^{\T}\ceqq \Mor _{\Mod{\K}}(V,\K )$ is a $\K$-$\K$-bimodule by \cref{exm1.1.72}.}
		
		That said, recall that (\cref{exm1.1.55}) modules over commutative rings have a canonical bimodule structure, and so if we're working over commutative rings, we can get away with just saying ``$\K$-module'' everywhere.
	\end{rmk}
	\begin{rmk}
		The ``$\T$'' is for ``transpose''---we'll see why later.  This is uncommon notation.  More common notation includes $V^*$ and $V'$.  The former I choose not to use as I reserve this notation for the \emph{conjugate}-dual, and the latter, well, $V'$ just looks weird to me.  There's also the fact that ``$\T$'' kind of just looks like a ``t''.\footnote{In English, ``$\T$'' is actually read as ``dagger'' (the \TeX \ command for this is ``\texttt{\textbackslash dagger}, though I imagine the English usage came first).}
	\end{rmk}
	\begin{rmk}
		If $V$ comes with a topology, you're only going to want to look at the \emph{continuous} linear functionals.  Of course, you can look at all of them (including the discontinuous ones), but this is probably not going to be as useful.
	\end{rmk}
	\begin{rmk}
		You might say that this is the ``left dual-space'' and that $\Mor _{\Mod*{\K}}(V,\K )$ would be the ``right dual-space''.  Just as we only worked with left modules by default (even though there was a corresponding notion of right module), we won't every worry about the right dual-space.  Besides, if $\K$ is commutative, there isn't going to be any difference anyways.
	\end{rmk}
\end{dfn}
You should note that we need to require $\K$ to be commutative in order to guarantee that $V^{\T}$ itself has the structure of a $\K$-module.\footnote{Recall from \cref{sss1.1.2} that morphism sets are in general not themselves $\K$-modules unless $\K$ is commutative.}  As we're going to be wanting to be considering $V$ and $V^{\T}$ on more or less equal footings, it is awkward if $V$ is a $\K$-module but $V^{\T}$ is just a commutative group.  Therefore, you should keep in mind that we will be requiring $\K$ to be commutative more often than usual, and that this is the reason why.

\begin{exm}{Row-vectors}{RowVectors}
	Let $\K$ be a ring, let $d\in \N$, and define $V\ceqq \K ^d$.  Recall that we can think of elements of $V$ has $d\times 1$ matrices.  Therefore, for every $1\times d$ matrix $\phi \in \Matrix _{1\times d}(\K )$, we have a linear map $V\rightarrow \K$ defined by
	\begin{equation}
	V\ni v\mapsto \phi v,
	\end{equation}
	where $\phi v$ is just matrix multiplication (and we are implicitly identifying $1\times 1$ matrices with $\K$ itself).
	
	This defines a map from $\Matrix _{1\times d}(\K )$ to $V^{\T}$, which is an isomorphism.
	\begin{exr}[breakable=false]{}{}
		Check that this is indeed an isomorphism.
	\end{exr}
	
	We thus hereafter identity $V^{\T}\cong \Matrix _{1,d}(\K )$, in which case elements of $V^{\T}\ceqq [\K ^d]^{\T}$ are referred to as \term{row-vectors}\index{Row-vector}.
	
	For example, $\begin{bmatrix}1 & 2 & 3\end{bmatrix}\in [\R ^3]^{\T}$ defines a linear-functional on $\R ^3$ via
	\begin{equation}
	\R ^3\ni \begin{bmatrix}x \\ y \\ z\end{bmatrix}\mapsto \begin{bmatrix}1 & 2 & 3\end{bmatrix}\begin{bmatrix}x \\ y \\ z\end{bmatrix}\ceqq x+2y+3z.
	\end{equation}
	\begin{rmk}
		Thus, row-vectors should be thought not of as vectors but as \emph{co}vectors.
	\end{rmk}
\end{exm}
\begin{exm}{$[\K ^{\infty}]^{\T}=\K ^{\N}$}{exm5.2.16}
	Let $\K$ be a ring.  Given $a\in \K ^{\N}$, we obtain a linear-functional $\phi _a\colon \K ^{\infty}\rightarrow \K$ defined by
	\begin{equation}
		\phi _a(b)\ceqq \sum _{k\in \N}b_ka_k.
	\end{equation}
	Note that this sum is finite as $b\in \K ^{\infty}$.
	\begin{exr}[breakable=false]{}{}
		Show that
		\begin{equation}
		\K ^{\N}\ni a\mapsto \phi _a\in [\K ^{\infty}]^{\T}
		\end{equation}
		is an isomorphism of $\K$-$\K$-bimodules.
	\end{exr}
\end{exm}

One reason the term ``dual'' is used is because, while on one hand, we can obviously view elements of $V^{\T}$ as linear-functionals on $V$, we can ``dually'' view elements of $V$ as linear-functionals on $V^{\T}$.
\begin{thm}{Duality of the dual}{DualityOfTheDual}
	Let $V$ be a $\K$-$\K$-bimodule.  Then, the map $V\rightarrow [V^{\T}]^{\T}$ defined by
	\begin{equation}
		v\mapsto (\phi \mapsto \phi (v))
	\end{equation}
	is linear and natural.
	
	Furthermore,
	\begin{enumerate}
		\item \label{DualityOfTheDual(i)}if $V$ is a vector space, this map is injective; and
		\item \label{DualityOfTheDual(ii)}if $V$ is a finite-dimensional vector space, this map is an isomorphism.
	\end{enumerate}
	\begin{rmk}
		See \cref{sbsB.2.2} for a discussion of what is meant here by the term ``natural''.  You should note, however, that it's not particularly important and could require a relatively large effort to fully understand, especially if you've never seen anything like this before.  Thus, you might consider just pretending the word ``natural'' didn't appear anywhere in the statement above---you won't be missing out on all \emph{that} much.
	\end{rmk}
	\begin{rmk}
		For $v\in V$ and $\phi \in V^{\T}$, we write
		\begin{equation}
		\pinnerprod{\phi |v}\ceqq \phi (v).
		\end{equation}\index[notation]{$\pinnerprod{\phi |v}$}
		Using this notation, the map $V\rightarrow [V^{\T}]^{\T}$ can be written as
		\begin{equation}
		v\mapsto \pinnerprod{\blankdot |v},
		\end{equation}
		where $\pinnerprod{\blankdot |v}$ is the linear-functional on $V^{\T}$ that sends $\phi \in V^{\T}$ to $\pinnerprod{\phi |v}\ceqq \phi (v)\in \K$.
		
		This notation is used suggestively when we want to think of $V$ and $V^{\T}$ as `on the same footing'---on one hand, we can view elements of $V^{\T}$ as linear-functionals on $V$ (e.g.~for $\phi \in V^{\T}$, $\pinnerprod{\phi |\blankdot}$ is a linear-functional on $V$), but on the other hand we can also view elements of $V$ as linear-functional on $V^{\T}$ (e.g.~for $v\in V$, $\pinnerprod{\blankdot |v}$ is a linear-functional on $V^{\T}$).  Thinking of things in terms of this ``duality'' is particularly appropriate when $V$ is a finite-dimensional vector space, so that, up to natural isomorphism, $V$ is `the same as' $[V^{\T}]^{\T}$.
	\end{rmk}
	\begin{rmk}
		Warning:  \cref{DualityOfTheDual(i)} need not hold if $V$ is not a vector space and \cref{DualityOfTheDual(ii)} need not hold if $V$ is a vector space but not finite-dimensional---see \cref{exm5.2.16x,exm5.2.17} respectively.
	\end{rmk}
	\begin{proof}
		We first check that it is linear.  Let $v,w\in V$ and let $\alpha ,\beta \in \K$.  We wish to show that
		\begin{equation}
			\pinnerprod{|\alpha v+\beta w}=\alpha \pinnerprod{|v}+\beta \pinnerprod{|w}.
		\end{equation}
		However, this will be the case iff for all $\phi \in V^{\dagger}$ we have
		\begin{equation}
			\pinnerprod{\phi|\alpha v+\beta w}=\alpha \pinnerprod{\phi |v}+\beta \pinnerprod{\phi |w}.
		\end{equation}
		However, by definition of the notation $\pinnerprod$, this equation is the same as
		\begin{equation}
			\phi (\alpha v+\beta w)=\alpha \phi (v)+\beta \phi (w),
		\end{equation}
		which is of course true as $\phi$ is linear.

		We now check that it is natural.\footnote{This part of the proof makes uses of things we have not yet encountered.  You can verify it is not circular as these new things don't make use of this result.  (It doesn't make sense to move this result so drastically just to avoid this small worry about potential circularity).  It makes use of the \namerefpcref{Transpose}, as well as the concept of a \emph{commutative diagram}, which is explained in a remark of \cref{TensorProduct}.}  Let $T\colon V\rightarrow W$ be a linear-transformation between $\K$-modules.  By definition (\cref{NaturalTransformation}), $V\mapsto [V^{\T}]^{\T}$ is natural iff the following diagram commutes.
		\begin{equation}
			\begin{tikzcd}
				V \ar[r] \ar[d,"T"'] & \left[ V^{\T}\right] ^{\T} \ar[d,"{[T^{\T}]^{\T}}"] \\
				W \ar[r] & \left[ W^{\T}\right] ^{\T}
			\end{tikzcd}
		\end{equation}
		By definition, this means that we want to show that
		\begin{equation}
			\pinnerprod{\psi |[[T^{\T}]^{\T}](\pinnerprod{|v})}=\pinnerprod{\psi|T(v)}
		\end{equation}
		for al $v\in V$ and $\psi \in W^{\T}$.  However, by definition of the transpose and $\pinnerprod{|v}$,
		\begin{equation}
			\begin{split}
				\pinnerprod{\psi |[[T^{\T}]^{\T}](\pinnerprod{|v})} & \ceqq \pinnerprod{[T^{\T}](\psi )|\pinnerprod{|v}}\ceqq \pinnerprod{[T^{\T}](\psi )|v} \\
				& \ceqq \pinnerprod{\psi |T(v)},
			\end{split}
		\end{equation}
		as desired.
		
		\blni
		\cref{DualityOfTheDual(i)} Suppose that $V$ is a vector space.  To show that it is injective, we check that the kernel is $0$.  So, let $v\in V$ suppose that $\pinnerprod{\phi |v}=0$ for all $\phi \in V^{\T}$.  If $\K =0$, then $V=0$, and so we are immediately done.  Otherwise, if $v\neq 0$, then there is a linear-functional $\phi \colon V\rightarrow \K$ that sends $v$ to $1$, in which case $\pinnerprod{\phi |v}=1\neq 0$, a contradiction.  Thus, it must be the case that $v=0$.
		
		\blni
		\cref{DualityOfTheDual(ii)} Suppose that $V$ is a finite-dimensional vector space.  We know from the defining result of the dual basis (\cref{TheDualBasis}) that $\dim (V)=\dim (V^{\T})=\dim ([V^{\T}]^{\T})$.  Thus, we have a injective linear map $V\rightarrow [V^{\T}]^{\T}$ between two finite-dimensional vector spaces of the same dimension, and hence it must in fact be an isomorphism (\cref{crl2.2.74}).
	\end{proof}
\end{thm}
Thus, in this sense, if $V$ is a finite-dimensional vector space, then $V$ is ``the same'' as $[V^{\T}]^{\T}$, in which case $V$ and $V^{\T}$ are ``on the same footing'' in the sense that the dual of $V$ is $V^{\T}$ and the dual of $V^{\T}$ `is' $V$.

Be careful however---this doesn't hold in infinite-dimensions.
\begin{exm}{A module $V$ for which $V\rightarrow [V^{\T}]^{\T}$ is not injective}{exm5.2.16x}
	Define $\K \ceqq \Z$ and $V\ceqq \Z /2\Z$.
	\begin{exr}[breakable=false]{}{}
		Show that there is no $\Z$-linear map $\Z /2\Z \rightarrow \Z$.
		\begin{rmk}
			A ``$\Z$-linear map'' is the same as a group homomorphism---see \cref{exm1.1.22}.
		\end{rmk}
	\end{exr}
	Thus, $V^{\T}=0$, and so certainly $[V^{\T}]^{\T}=0$, and hence the map $V\rightarrow [V^{\T}]^{\T}$ cannot be injective.
\end{exm}
\begin{exm}{A vector space $V$ for which $V\rightarrow [V^{\T}]^{\T}$ is not an isomorphism}{exm5.2.17}
	Define $\K \ceqq \C$ and $V\ceqq \C ^{\infty}$.  By \cref{exm5.2.16}, we know that $[\C ^{\infty}]^{\T}\cong \C ^{\N}$ via the map $\C ^{\N}\in a\mapsto \pinnerprod{a|}\in [\C ^{\infty}]^{\T}$.  Hence, $[[\C ^{\infty}]^{\T}]^{\T}=[\C ^{\N}]^{\T}$ and the map $\C ^{\infty}\rightarrow [\C ^{\N}]^{\T}$ is given by $\C ^{\infty}\ni b\mapsto \pinnerprod{|b}\in [\C ^{\N}]^{\T}$.  We thus wish to find $\phi \in [\C ^{\N}]^{\T}$ such that $\phi \neq \pinnerprod{|b}$ for any $b\in \C ^{\infty}$.
	
	Let $e_k\in \C ^{\infty}$ denote the sequence that is identically $0$ except for a $1$ at index $k$.  Then, $\pinnerprod{e_k|b}=b_k$, and so in particular, for every $b\in C^{\infty}$, there is some $e_{k_b}$ such that $\pinnerprod{e_{k_b}|b}=0$ (recall that \cref{exm1.1.18} the elements of $\C ^{\infty}$ are those sequences which are eventually $0$).
	
	Using \cref{prp1.2.68}, let $\phi \colon \C ^{\N}\rightarrow \C$ be a linear-functional such that $\phi (e_k)=1$ for all $k\in \N$.  We then cannot have that $\phi =\pinnerprod{|b}$ for any $b\in \C ^{\infty}$ as $\phi (e_k)\neq 0$ for all $k$, and so $V\rightarrow [V^{\T}]^{\T}$ is not surjective.
\end{exm}

On the off chance this talk of dual-spaces has you thinking ``Okay, but if $\Mor _{\Mod{\K}}(V,\K )$ is so damn interesting, why don't we care about $\Mor _{\Mod{\K}}(\K ,V)$.''.  The answer is that ``We do care.'', but there's no need to have a separate discussion---this `is' just $V$ itself.
\begin{prp}{$\Mor _{\Mod{\K}}(\K ,V)\cong V$}{prp5.2.24}
	Let $V$ be a $\K$-module.  Then, the map
	\begin{equation}
		\Mor _{\Mod{\K}}(\K ,V)\ni \phi \mapsto \phi (1)\in V
	\end{equation}
	is a natural isomorphism.
	\begin{proof}
		It follows from the definition that it is linear.  To check naturality, let $T\colon U\rightarrow V$ be linear.  Then, this map is natural (\cref{NaturalTransformation}) iff the following diagram commutes.
		\begin{equation}
			\begin{tikzcd}
				\Mor (\K ,U) & U \\
				\Mor (\K ,V) & V
			\end{tikzcd}
		\end{equation}
		Writing out the definitions of these maps, we see that this commutes for tautological reasons.
		
		To show that it is an isomorphism, we construct an inverse.  For every $v\in V$, there is a unique linear map $\phi _v\colon \K \rightarrow V$ such that $\phi _v(1)=v$.  By construction, $V\ni V\mapsto \phi _v\in \Mor (\K ,V)$ is an inverse to $\phi \mapsto \phi (1)$.
	\end{proof}
\end{prp}

Before moving on, we end this subsection with some more examples of dual-spaces and linear-functions.
\begin{exm}{Evaluation maps}{}
	Let $x_0\in \R$.  Then, $\ev _{x_0}\colon \Mor _{\Top}(\R ,\C )\rightarrow \C$\index[notation]{$\ev _{x_0}$}, the \term{evaluation map}\index{Evaluation map} at $x_0$ defined by
	\begin{equation}
	\ev _{x_0}(f)\ceqq f(x_0),
	\end{equation}
	is a linear-functional.
	
	Of course, it restricts to $\C [x]\subseteq C^{\infty}(\R )\subseteq \Mor _{\Top}(\R ,\C )$ to give linear-functionals on $\C [x]$ and $C^{\infty}(\R )$ as well.
	\begin{rmk}
		$\Mor _{\Top}(\R ,\C )$ is the category-theoretic notation for the set of all \emph{continuous} functions from $\R$ to $\C$.  (``$\Top$'' is the category of topological spaces,\footnote{Without giving the precise definition, the ``point'' of topological spaces is that they are one of the most general contexts in which the notion of continuity makes sense.} and the morphisms in this category are by definition continuous functions.)
	\end{rmk}
\end{exm}
\begin{exm}{Integration}{}
	Let $X$ be a measure space.  Then, the map $\Leb ^1 (X)\rightarrow \C$, $f\mapsto \int _X\dif x\, f(x)$, is a linear-functional on $\Leb ^1(X)$.
	\begin{rmk}
		$\Leb ^1(X)$ is the set of all (complex-valued) \emph{integrable} on $X$.  If you've never studied measure theory before, you don't need to know the precise definition to understand what's going on here---integrable functions are essentially just those functions for which the integral is defined and the integral is a linear-functional simply because $\int _X\dif x\, [f_1(x)+f_2(x)]=\int _X\dif x\, f_1(x)+\int _X\dif x\, f_2(x)$ and $\int _X\dif x\, [\alpha f(x)]=\alpha \int _X\dif x\, f(x)$.
	\end{rmk}
\end{exm}
\begin{exm}{The trace}{TheTrace}
	Let $V$ be a finite-dimensional vector space over an algebraically-closed field $\F$.  Then,
	\begin{equation}
	\End _{\Vect _{\F}}(V)\ni T\mapsto \sum _{\lambda \in \Eig (T)}\dim (\Eig _{\lambda}^{\infty})\lambda \in \F
	\end{equation}
	is a linear-functional on $\End _{\Vect _{\F}}(V)$.
	\begin{rmk}
		The dimension here is to account for the ``multiplicity'' of the eigenvalue.
	\end{rmk}
\end{exm}
\begin{exr}{}{}
	Let $\K$ be a ring.  By \cref{exm5.2.16}, $[\K ^{\infty}]^{\T}=\K ^{\N}$.  By the previous result, $\K ^{\infty}$ embeds into $[\K ^{\N}]^{\T}$.  Show that $\K ^{\infty}\rightarrow [\K ^{\N}]^{\T}$ is not surjective.
\end{exr}

\subsection{Dual-pairs}

\subsubsection{The definition and basic facts}

While it won't be true in general, in our case of interest, $V$ and $V^{\T}$ will be `on the same footing'.  A priori, that just doesn't look like the case at all---you start with $V$ and then $V^{\T}$ is a collection of functions with domain $V$.  However, if we shift our perspective, it becomes clearer how this might work.

Consider instead the map $V^{\T}\times V\ni \coord{\phi ,v}\mapsto \phi (v)\in \K$.  In fact, we will suggestively write $\pinnerprod{\phi |v}=\phi (v)$ to stress that we want to think of $V^{\T}$ ``on the same footing'' as $V$.  Now the situation is more symmetric.  This motivates us to define the notion of a \emph{dual-pair}.\footnote{You should also take note that (\cref{DualityOfTheDual}), at least for finite-dimensional vector spaces over fields, $[V^{\T}]^{\T}$ `is' $V$.  Thus, if you take the dual of $V$ you get $V^{\T}$ (duh), but furthermore taking the dual of $V^{\T}$ gives you $[V^{\T}]^{\T}=V$ back as well.  In brief, $V$ and $V^{\T}$ are the duals of each other, which is perhaps a stronger argument that we should be thinking of $V$ and $V^{\T}$ as ``on the same footing''.  Be warned, however, this is very much special to the finite-dimensional case.}
\begin{dfn}{Dual-pair}{DualPair}	
	A \term{dual-pair}\index{Dual-pair} is
	\begin{data}
		\item two $\K$-$\K$-bimodules $V$ and $W$; together with
		\item a bilinear map $\pinnerprod{\blankdot |\blankdot} \colon W\times V\rightarrow \K$\index[notation]{$\pinnerprod{\blankdot |\blankdot}$}.
	\end{data}
	\begin{rmk}
		$\pinnerprod{\blankdot |\blankdot}$ is the \term{pairing}\index{Pairing (of a dual-pair)} of $V$ and $W$.  As this in principle contains all the data of a dual-pair itself ($V$ and $W$ are `encoded' in the domain of $\pinnerprod{\blankdot |\blankdot}$'', we may refer to ``$\pinnerprod{\blankdot |\blankdot}\colon W\times V\rightarrow \K$'' itself as the ``dual-pair''.
	\end{rmk}
	\begin{rmk}
		This notation is useful because it allows us to treat $V$ and $W$ ``on the same footing'' when they otherwise might not be.  For example, if you look at the definition of orthogonal complement (\cref{OrthogonalComplement}), if we had tried to define this using just $V$ and $V^{\T}$ (so without using the language of ``dual-pair''), then the orthogonal complement of $S\subseteq V^{\T}$ would be a subset $S^{\perp}\subseteq [V^{\T}]^{\T}$ of the double dual, whereas we want $S^{\perp}\subseteq V$.
	\end{rmk}
	\begin{rmk}
		Warning:  Some authors require $\pinnerprod{\blankdot |\blankdot}$ to be \emph{nondegenerate} (\cref{NonsingularAndNondegenerate}), whereas we do not.  If we want to consider a nondegenerate (or nonsingular) pairing, we will say so explicitly.
	\end{rmk}
\end{dfn}
Note that, given a dual pair $\pinnerprod{\blankdot |\blankdot}\colon W\times V\rightarrow \K$, we have maps $V\rightarrow W^{\T}$ and $W\rightarrow V^{\T}$, defined by $v\mapsto \pinnerprod{\blankdot |v}$ and $w\mapsto \pinnerprod{w|\blankdot}$ respectively.
\begin{dfn}{Nonsingular and nondegenerate\hfill}{NonsingularAndNondegenerate}
	Let $\pinnerprod{\blankdot |\blankdot}\colon W\times V\rightarrow \K$ be a dual-pair.
	\begin{enumerate}
		\item $\pinnerprod{\blankdot |\blankdot}$ is \term{nonsingular}\index{Nonsingular (dual-pair)} iff the maps $V\rightarrow W^{\T}$ and $W\rightarrow V^{\T}$ are both isomorphisms.
		\item $\pinnerprod{\blankdot |\blankdot}$ is \term{nondegenerate}\index{Nondegenerate (dual-pair)} iff the maps $V\rightarrow W^{\T}$ and $W\rightarrow V^{\T}$ are both injective.
	\end{enumerate}
	\begin{rmk}
		Given $v\in V$, the linear-functional $\pinnerprod{\blankdot |v}\in W^{\T}$ is the \term{dual-vector} of $v$ with respect to $\pinnerprod{\blankdot |\blankdot}$.  Similarly, given $w\in W$, the linear-functional $\pinnerprod{w|\blankdot}\in V^{\T}$ is the \term{dual-vector} of $w$ with respect to $\pinnerprod{\blankdot |\blankdot}$.\index{Dual-vector}
		
		Using this language, nondegeneracy says that the dual-vector $\pinnerprod{\blankdot |v}$ is uniquely determined by $v$ (and dually for $w\in W$).  Similarly, nonsingularity says that every element of $W^{\T}$ is of this form (and dually for $V^{\T}$).
	\end{rmk}
\end{dfn}
The condition of nondegeneracy is usually stated in the following equivalent form.
\begin{prp}{}{}
	Let $\pinnerprod{\blankdot |\blankdot}\colon W\times V\rightarrow \K$ be a dual-pair.  Then, $\pinnerprod{\blankdot |\blankdot}$ is nondegenerate iff $\pinnerprod{w|v}=0$ for all $w\in W$ implies $v=0$ and $\pinnerprod{w|v}=0$ for all $v\in V$ implies $w=0$.
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{}{prp5.2.16}
	Let $\pinnerprod{\blankdot |\blankdot}\colon W\times V\rightarrow \K$ be a dual pair.
	\begin{enumerate}
		\item \label{prp5.2.16(i)}If $\pinnerprod{\blankdot |\blankdot}$ is nonsingular, then it is nondegenerate.
		\item \label{prp5.2.16(ii)}Suppose that $V$ and $W$ are finite-dimensional vectors spaces.  Then, $\pinnerprod{\blankdot |\blankdot}$ is nonsingular iff it is nondegenerate.
	\end{enumerate}
	\begin{proof}
		\cref{prp5.2.16(i)} Immediate from the definitions.
		
		\blni
		\cref{prp5.2.16(ii)} $(\Rightarrow )$ This is \cref{prp5.2.16(i)}.
		
		$(\Leftarrow )$ Suppose that $\pinnerprod{\blankdot |\blankdot}$ is nondegenerate.  Then, it is nonsingular as injective is equivalent to bijective for finite-dimensional vector spaces (\cref{crl2.2.74}).
	\end{proof}
\end{prp}
\begin{prp}{}{prp5.2.38}
	Let $V$ be a $\K$-$\K$-bimodule.  Then, $\pinnerprod{\blankdot |\blankdot}\colon V^{\T}\times V\rightarrow \K$ is a dual-pair.
	
	Furthermore, it is nondegenerate if $V$ is a vector space and nonsingular if $V$ is a finite-dimensional vector space.
	\begin{rmk}
		A corollary of this is that $\dim (V)=\dim (V^{\T})$ if $V$ is a finite-dimensional vector space:  The pairing is nonsingular, and so $\basis{B}^{\T}$ is a basis of $V^{\T}$ by \cref{TheDualBasis}.  As $\abs{\basis{B}}=\abs{\basis{B}^{\T}}$, it follows that $\dim (V)=\dim (V^{\T})$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

\subsubsection{The transpose}

We now have a notion of ``dual'' for vector spaces, and so, as you might expect, we also get a notion of ``dual'' for linear-transformations.
\begin{dfn}{Transpose}{Transpose}
	Let $V_1$ and $V_2$ be $\K$-$\K$-bimodules and let $T\colon V_1\rightarrow V_2$.  Then, the \term{transpose}\index{Transpose} of $T$, $T^{\T}\colon V_2^{\T}\rightarrow V_1^{\T}$\index[notation]{$T^{\T}$}, is defined by
	\begin{equation}
		\pinnerprod{T^{\T}(w_2)|v_1}\ceqq \pinnerprod{w_2|T(v_1)}
	\end{equation}
	for $w_2\in V_2^{\T}$ and $v_1\in V_1$.
	\begin{rmk}
		$T^{\T}$ is also called the \term{dual-map}\index{Dual-map} or just \term{dual}\index{Dual (of a linear-transformation)} of $T$.
	\end{rmk}
	\begin{rmk}
		Note that this definition doesn't actually require the existence of any dual-pairs.  That said, we are going to be primarily interested in the case when there are pairings involved.\footnote{This is why we used ``$w_2$'' above instead of something like ``$\phi _2$''.}
		
		Let $\pinnerprod{\blankdot |\blankdot}\colon W_1\times V_1\rightarrow \K$ and $\pinnerprod{\blankdot |\blankdot}\colon W_2\times V_2\rightarrow \K$ be \emph{nonsingular} dual-pairs.  Nonsingularity implies in particular that $W_2\cong V_2^{\T}$ and $W_1\cong V_1^{\T}$,\footnote{Note that this does not quite use the full strength of singularity---it is also the case that $W_2^{\T}\cong V_2$ and $W_1^{\T}\cong V_1$.  However, if we want to take the transpose twice to get back a map $V_1\rightarrow V_2$ (which it turns out agrees with the original map---see \cref{prp5.2.36}), we will need the full strength of nonsingularity.} in which case we will readily make these identifications.  We can then view $T^{\T}$ as a map $T^{\T}\colon W_2\rightarrow W_1$.  If we do have nonsingular dual-pairs like this, we will always make this identification.
	\end{rmk}
\end{dfn}
In case you were wondering, yes, this is the same ``transpose'' you have probably heard of before in the context of matrices, though we'll have to wait until we've discussed dual-bases to see exactly why---see \cref{prpTranspose}.  What follows is an example of the transpose for something besides matrix linear-transformations.
\begin{exm}{}{exm5.2.36}
	Recall that (\cref{exm5.2.16}) $[\K ^{\infty}]^{\T}\cong \K ^{\N}$.  Thus, regarding the left and right shift operators (\cref{ShiftOperator}) $L,R\colon \K ^{\infty}\rightarrow \K ^{\infty}$ as operators on $\K ^{\infty}$, their transposes $L^{\T},R^{\T}\colon \K ^{\N}\rightarrow \K ^{\N}$ are operators on $\K ^{\N}$.
	\begin{exr}[breakable=false]{}{}
		Show that $L^{\T}=R$ and $R^{\T}=L$ as operators on $\K ^{\N}$.
	\end{exr}
\end{exm}
\begin{prp}{Properties of the transpose}{prp5.2.36}
	Let $\pinnerprod{\blankdot |\blankdot}\colon W_0\times V_0\rightarrow$, $\pinnerprod{\blankdot |\blankdot}\colon W_1\times V_1\rightarrow \K$, and $\pinnerprod{\blankdot |\blankdot}\colon W_2\times V_2\rightarrow \K$ be nonsingular dual-pairs.
	\begin{enumerate}
		\item \label{prp5.2.36(i)}$\Mor _{\Mod{\K}[\K]}(V_1,V_2)\ni T\mapsto T^{\T}\in \Mor _{\Mod{\K}[\K]}(W_2,W_1)$ is linear.
		\item \label{prp5.2.36(ii)}$[T\circ S]^{\T}=S^{\T}\circ T^{\T}$ for $S\colon V_0\rightarrow V_1$ and $T\colon V_1\rightarrow V_2$ linear.
		\item \label{prp5.2.36(iii)}$[\id _{V_0}]^{\T}=\id _{V_0^{\T}}$.
		\item \label{prp5.2.36(iv)}$[T^{\T}]^{\T}=T$.
	\end{enumerate}
	\begin{rmk}
		For what it's worth, \cref{prp5.2.36(ii),prp5.2.36(iii)} are the statement that $\blank ^{\T}$ is a \emph{cofunctor} (\cref{Cofunctor}), which explains why we use the same notation for $T^{\T}$ as we do for $V^{\T}$ (and in turn why $T^{\T}$ is sometimes called the ``dual'' of $T$).
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{}{prp5.2.44}
	Let $V$ be a finite-dimensional vector space, let $T\colon V\rightarrow V$ be linear, let $v\in V$.  Then,
	\begin{equation}
		\Eig (T)=\Eig (T^{\T}).
	\end{equation}
	\begin{rmk}
		Warning:  This fails in infinite-dimensions---see \cref{exm5.2.46}.
	\end{rmk}
	\begin{proof}
		$\lambda \in \Eig (T)$ iff $\Ker (T-\lambda )\neq 0$ iff\footnote{This requires finite-dimensionality.} $T-\lambda$ is invertible iff there is a linear map $S\colon V\rightarrow V$ such that $S(T-\lambda )=\id =(T-\lambda S)$ iff\footnote{This uses finite-dimensionality again.  Certainly, we have $\Rightarrow$ all the time, but to go backwards, we need to use the fact that $[A^{\T}]^{\T}=A$, which requires finite-dimensionality---see \cref{prp5.2.38}.}\footnote{Of course, we will have that $R=S^{\T}$.} there is a linear map $R\colon V^{\T}\rightarrow V^{\T}$ such that $R(T^{\T}-\lambda )=\id _{V^{\T}}=(T^{\T}-\lambda )R$ iff \textellipsis $\lambda \in \Eig (T^{\T})$.
	\end{proof}
\end{prp}
\begin{exm}{$\Eig (T)\neq \Eig (T^{\T})$}{exm5.2.46}
	Consider the right shift operator $R\colon \K ^{\infty}\rightarrow \K ^{\infty}$.  By \cref{exm4.2.26}, $\Eig (R)=\emptyset$.  On the other hand, from \cref{exm5.2.36}, we know that $R^{\T}=L$ as an operator on $\K ^{\N}\cong [\K ^{\infty}]^{\T}$, and again from \cref{exm4.2.26} that $\Eig (R^{\T})=\Eig (L)=\K$.
\end{exm}

\subsubsection{The orthogonal complement}

The intuition behind the following concept will likely be easier to understand when we begin our study of inner-product spaces, though one can see it's usefulness easily enough:  it allows us to related the kernel and image of $T^{\T}$ to that of $T$---see \cref{prp5.2.21}.
\begin{dfn}{Orthogonal complement}{OrthogonalComplement}\index{Orthogonal complement}\index[notation]{$S^{\perp}$}
	Let $\pinnerprod{\blankdot |\blankdot}\colon W\times V\rightarrow \K$ be a dual-pair.
	\begin{enumerate}
		\item For $S\subseteq V$, the \term{orthogonal complement} of $S$, $S^{\perp}$, is defined by
		\begin{equation}
		S^{\perp}\ceqq \left\{ w\in W:\pinnerprod{w|v}=0\text{ for all }v\in S\text{.}\right\} .
		\end{equation}
		\item For $S\subseteq V^{\T}$, the \term{orthogonal complement} of $S$, $S^{\perp}$, is defined by
		\begin{equation}
		S^{\perp}\ceqq \left\{ v\in V:\pinnerprod{w|v}=0\text{ for all }w\in S\text{.}\right\} .
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		Let $v\in \K ^d$ and $\phi \in [\K ^d]^{\T}$.  Then,
		\begin{equation}
		\pinnerprod{\phi |v}=\phi v\ceqq \sum _{k=1}^d\phi _kv_k,
		\end{equation}
		where ``$\phi v$'' here denotes matrix multiplication (recall that (\cref{RowVectors}) $\phi$ is a row-vector, so a $1\times d$ matrix, just as $v$ is a $d\times 1$ matrix).  This expression should remind you of the dot product.  Of course, this won't literally be the case in general, but nevertheless, much of the intuition about the ``pairing'' $\pinnerprod{w|v}$ in general comes from your intuition about the dot product.
		
		For example, in this case, you'll recall that, by definition, two vectors are \emph{orthogonal} iff their dot product vanishes, whence the term ``orthogonal complement'':  $S^{\perp}$ is the set of vectors orthogonal to the set of `covectors'\footnote{In quotes because unless $W=V^{\T}$, it need not literally be the case that the elements of $S$ are linear-functionals on $V$.} $S$ (or dually if $S\subseteq V$).
	\end{rmk}
	\begin{rmk}
		The symbol ``$\perp$'' in English is read (at least by me) as ``perp'' (as in ``\emph{perp}endicular'').  We'll see later when we study inner-product spaces that, in a sense, ``orthogonal'' is essentially equivalent to ``perpendicular'', hence ``perp''.
	\end{rmk}
	\begin{rmk}
		I have also seen the notation $S^0$ used.  I've also seen this referred to as the \emph{annihilator} of $S$, 
	\end{rmk}
\end{dfn}
\begin{prp}{Properties of the orthogonal \\ complement}{prp5.2.18}
	Let $\pinnerprod{\blankdot |\blankdot}\colon W\times V\rightarrow \K$ be a dual pair and let $S$ and $T$ either both be subsets of $V$ or both be subsets of $W$.
	\begin{enumerate}
		\item \label{prp5.2.18(i)}$S^{\perp}$ is a subspace.
		\item \label{prp5.2.18(iix)}If $S\subseteq T$, then $T^{\perp}\subseteq S^{\perp}$.
		\item \label{prp5.2.18(ii)}if $\pinnerprod{\blankdot |\blankdot}$ is a nonsingular pairing of vector spaces, then $[S^{\perp}]^{\perp}=\Span (S)$.
		\item \label{prp5.2.18(iiix)}$0^{\perp}=V$ and $0^{\perp}=W$.
		\item \label{prp5.2.18(iiixx)}If the pairing is nondegenerate, then $V^{\perp}=0$ and $W^{\perp}=0$.
		\item \label{prp5.2.18(iii)}If $V$ and $W$ are finite-dimensional vector spaces, then
		\begin{equation}
			\dim (V)=\dim (U)+\dim (U^{\perp})
		\end{equation}
		if $U\subseteq V$ is a subspace; and
		\begin{equation}
			\dim (W)=\dim (U)+\dim (U^{\perp})
		\end{equation}
		if $U\subseteq W$ is a subspace.
	\end{enumerate}
	\begin{rmk}
		Note that this doesn't require $S$ or $T$ to be a subspaces themselves.
	\end{rmk}
	\begin{rmk}
		As in \namerefpcref{RankNullityTheorem}, \cref{prp5.2.18(iii)} generalizes to the statements that $U^{\perp}\rightarrow (V/U)^{\T}$ and $U^{\perp}\rightarrow (W/U)^{\T}$ are isomorphisms (at least for nonsingular pairings).\footnote{Every $w\in U^{\perp}$ can be regarded as a linear map $V\rightarrow \K$ that vanishes on $U$, and hence descends to a well-defined linear map $V/U\rightarrow \K$.  Dually for the case $U\subseteq W$.}
	\end{rmk}
	\begin{rmk}
		If you care, I suspect that \cref{prp5.2.18(ii)} holds if $W$ and $V$ are semisimple (\cref{SemisimpleModule}) (this is automatic if $\K$ is a division ring by \cref{prp4.4.24}).
	\end{rmk}
	\begin{proof}
		\cref{prp5.2.18(i)} We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove this part.
		\end{exr}
		
		\blni
		\cref{prp5.2.18(iix)} We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove this part.
		\end{exr}
		
		\blni
		\cref{prp5.2.18(ii)} Without loss of generality, take $S\subseteq V$.  Let $v\in S$.  Then, by definition, $\pinnerprod{w|v}=0$ for all $w\in S^{\perp}$, and hence $v\in [S^{\perp}]^{\perp}$.  Thus, $S\subseteq [S^{\perp}]^{\perp}$, and hence, as $[S^{\perp}]^{\perp}$ is a subspace, $\Span (S)\subseteq [S^{\perp}]^{\perp}$.
		
		For the other inclusion, we first prove it in the case $S\subseteq V$ itself is a subspace, so that $\Span (S)=S$.  Write $V=S\oplus T$ for some subspace $T\subseteq V$.  We first check that $W=S^{\perp}\oplus T^{\perp}$.  If $w\in S^{\perp}\cap T^{\perp}$, then $\pinnerprod{w|s+t}=0$ for all $s\in S$ and $t\in T$, and hence $w=0$ by nondegeneracy.  To show spanning, let $w\in W$.  As $V=S\oplus T$, there is a unique linear-functional $w_s\colon V\rightarrow \K$ such $\restr{w_s}{S}=\restr{\pinnerprod{w|\blankdot}}{S}$ and $\restr{w_s}{T}=0$.  Similarly there is a unique linear-functional $w_t\colon V\rightarrow \K$ such that $\restr{w_t}{S}=0$ and $\restr{w_t}{T}=\restr{\pinnerprod{w|\blankdot}}{T}$.  By nonsingularity, every element of $V^{\T}$ is of the form $\pinnerprod{u|\blankdot}$ for a unique $u\in W$.  By abuse of notation, let $w_s,w_t\in W$ be the unique elements such that $\pinnerprod{w_s|s}=\pinnerprod{w|s}$ for $s\in S$ and $\pinnerprod{w_s|t}=0$ for $t\in T$, and $\pinnerprod{w_t|s}=0$ for $s\in S$ and $\pinnerprod{w_t|t}=\pinnerprod{w|t}$ for $t\in T$.  It follows immediately from this that $w_s\in T^{\perp}$ and $w_t\in S^{\perp}$.  We claim that $w=w_s+w_t$.  So, let $v\in V$ and write $v=s+t$ for unique $s\in S$ and $t\in T$.  Then,
		\begin{equation}
			\begin{split}
				\pinnerprod{w|v} & =\pinnerprod{w|s+t}=\pinnerprod{w|s}+\pinnerprod{w|t} \\
				& =\pinnerprod{w_s|s}+\pinnerprod{w_t|t} \\
				& =\pinnerprod{w_s|s+t}+\pinnerprod{w_t|s+t} \\
				& =\pinnerprod{w_s|v}+\pinnerprod{w_t|v}=\pinnerprod{w_s+w_t|v}.
			\end{split}
		\end{equation}
		By nondegeneracy, we have that $w=w_s+w_t$, as desired.  Hence, $W=S^{\perp}\oplus T^{\perp}$.
		
		We return to checking that $[S^{\perp}]^{\perp}\subseteq S$ if $S\subseteq V$ is a subspace.  So, let $v\in [S^{\perp}]^{\perp}$ and write $v=s+t$ for unique $s\in S$ and $t\in T$.  We wish to show that $t=0$.  To show this, by nondegeneracy, it suffices to show that $\pinnerprod{w|t}=0$ for all $w\in W$.  So, let $w\in W$.  We now know that we can write $w=w_s+w_t$ for unique $w_s\in T^{\perp}$ and $w_t\in S^{\perp}$.  We then have
		\begin{equation}
		\pinnerprod{w|t}=\pinnerprod{w_t|t}=\pinnerprod{w_t|s+t}=\pinnerprod{w_t|v}=0,
		\end{equation}
		and so $t=0$, as desired.
		
		Finally, we prove that $[S^{\perp}]^{\perp}\subseteq \Span (S)$ for an arbitrary subset $S\subseteq V$.  We have that $S\subseteq \Span (S)$, and so $\Span (S)^{\perp}\subseteq S^{\perp}$, and so
		\begin{equation}
		[S^{\perp}]^{\perp}\subseteq [\Span (S)^{\perp}]^{\perp}=\Span (S),
		\end{equation}
		as desired.
		
		\blni
		\cref{prp5.2.18(iiix)} We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove this part.
		\end{exr}
		
		\blni
		\cref{prp5.2.18(iiixx)} We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove this part.
		\end{exr}
		
		\blni
		\cref{prp5.2.18(iii)} We prove the case $U\subseteq V$.  The other case is essentially identical.  Let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis for $V$ with $\{ b_1,\ldots ,b_e\}$ a basis of $U$, so that $\basis{B}^{\T}\ceqq \{ [b^1]^{\T},\ldots ,[b^d]^{\T}\}$ is a basis for $V^{\T}$.\footnote{This is the dual-basis which we will meet shortly---see \cref{TheDualBasis}.}  As $\pinnerprod{[b^k]^{\T}|b_l}=\delta \indices{^k_l}$,\footnote{} $[b^k]^{\T}\in U^{\perp}$ iff $e+1\leq m$.  Furthermore, for $\phi \in U^{\perp}$, if we write $\phi =\phi _1\cdot [b^1]^{\T}+\cdots +\phi _d\cdot [b^d]^{\T}$, plugging in $b_k$ for $1\leq k\leq e$ shows that we must have $\phi _k=0$.  Thus, $\phi \in \Span ([b^{e+1}]^{\T},\ldots ,[b^d]^{\T})$, and hence $\{ [b^{e+1}]^{\T},\ldots ,[b^d]^{\T})$ forms a basis for $U^{\perp}$.  Hence,
		\begin{equation}
			\dim (U)+\dim (U)^{\perp}=e+(d-e)=d=\dim (V).
		\end{equation}
	\end{proof}
\end{prp}
\begin{prp}{}{prp5.2.21}
	Let $\pinnerprod{\blankdot |\blankdot}\colon W_1\times V_1\rightarrow \K$ and $\pinnerprod{\blankdot |\blankdot}\colon W_2\times V_2\rightarrow \K$ be nonsingular dual-pairs, and let $T\colon V_1\rightarrow V_2$ be linear.  Then,
	\begin{align}
	\Ker (T^{\T}) & =\Ima (T)^{\perp}\label{eqn5.2.22} \\
	\Ima (T^{\T}) & =\Ker (T)^{\perp}\label{eqn5.2.23}
	\end{align}
	\begin{rmk}
		I think this should be relatively easy to remember.  $^{\T}$ comes outside of $\Ker $ and $\Ima$, `flips upside-down', and $\Ker$ gets replaced with $\Ima$ and vice versa.
	\end{rmk}
	\begin{rmk}
		In particular, this (together with \cref{prp5.2.18}) implies (for vector spaces anyways) that $T$ is injective iff $T^{\T}$ is surjective and that $T$ is surjective iff $T^{\T}$ is injective.
	\end{rmk}
	\begin{rmk}
		Do take note of this.  It is quite important.
		
		Also, don't overlook the fact that \emph{nonsingularity} is a hypothesis.
	\end{rmk}
	\begin{proof}
		\eqref{eqn5.2.22} $w_2 \in \Ker (T^{\T})$ iff $T^{\T}(w_2)=0$ iff $0=\pinnerprod{T^{\T}(w_2)|v_1}=\pinnerprod{w_2|T(v_1)}$ for all $v_1\in V_1$ iff $w_2\in \Ima (T)^{\perp}$.
		
		\blni
		\eqref{eqn5.2.23} Replacing $T$ with $T^{\T}$ in \eqref{eqn5.2.22}, we obtain
		\begin{equation}
		\Ker (T)=\Ker ([T^{\T}]^{\T})=\Ima (T^{\T})^{\perp}.
		\end{equation}
		Taking the perp of this equation, we obtain
		\begin{equation}
		\Ker (T)^{\perp}=\Span (\Ima (T^{\T}))=\Ima (T^{\T}),
		\end{equation}
		as desired.
	\end{proof}
\end{prp}
Among other things, this can be used to figure out the ``row-space'' of matrices as is often asked in elementary linear algebra courses.
\begin{exm}{Row-space}{}
	Let $A$ be an $m\times n$ matrix.  The \term{row-space}\index{Row-space} of $A$, $\Row (A)$\index[notation]{$\Row (A)$}, is the span of the rows of $A$.  The first thing is to note that $\Row (A)=\Col (A^{\T})$.  Hence,
	\begin{equation}
	\Row (A)=\Col (A^{\T})=\Null (A)^{\perp}.
	\end{equation}
	As you already know how to compute null spaces, this gives you an alternative description of $\Row (A)$.  In particular, from \cref{prp5.2.18}\cref{prp5.2.18(iii)},
	\begin{equation}
	\begin{split}
	\dim (\Row (A)) & =\dim (\Null (A)^{\perp})=\dim (\K ^n)-\dim (\Null (A)) \\
	& =\footnote{By the \namerefpcref{RankNullityTheorem}.}\dim (\Col (A)).
	\end{split}
	\end{equation}
	
	\horizontalrule
	
	Similarly, if you had been asked to calculate $\Null (A^{\T})$ for whatever reason, you could use the fact that $\Null (A^{\T})=\Ima (A)^{\perp}$.
\end{exm}

\subsubsection{The dual basis}

Given a dual-pair $\pinnerprod{\blankdot |\blankdot}\colon W\times V\rightarrow \K$ and a basis $\basis{B}\subseteq V$, there will be a corresponding subset $\basis{B}^{\T}\subseteq W$ which is always linearly-independent and, in good cases, a basis of $W$, the \emph{dual basis}.
\begin{prp}{The dual basis}{TheDualBasis}
	Let $\pinnerprod{\blankdot |\blankdot}\colon W\times V\rightarrow \K$, let $\basis{B}$ be a basis of $V$, and for $b\in \basis{B}$ let $b^{\T}\colon V\rightarrow \K$ be the unique linear map such that
	Let $V$ be a $\K$-module, let $\basis{B}$ be a basis of $V$, for $b\in \basis{B}$ let $b^{\T}\colon V\rightarrow \K$ be the unique linear map such that
	\begin{equation}
	b^{\T}(c)=\begin{cases}1 & \text{if }c=b \\ 0 & \text{otherwise,}\end{cases}
	\end{equation}
	and define $\basis{B}^{\T}\ceqq \{ b^{\T}:b\in \basis{B}\}$.
	\begin{enumerate}
		\item If the pairing is nondegenerate, then $\basis{B}^{\T}\subseteq V$ is linearly-independent.
		\item If the pairing is nonsingular, then $\basis{B}^{\T}\subseteq V$ is a basis.
	\end{enumerate}
	\begin{rmk}
		If $\basis{B}^{\T}$ is actually a basis of $W$, then it is referred to as the \term{dual basis}\index{Dual basis} of $\basis{B}$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exr}{}{}
	Find an example of a dual-pair with basis $\basis{B}\subseteq V$ such that $\basis{B}^{\T}$ does \emph{not} span $W$.
\end{exr}

It's now time to return to the issue of how this notion of transpose corresponds with the classical one you might already be familiar with. 
\begin{prp}{Transpose of a matrix}{prpTranspose}
	Let $\pinnerprod{\blankdot |\blankdot}\colon W_1\times V_1\rightarrow \K$ and $\pinnerprod{\blankdot |\blankdot}\colon W_2\times V_2\rightarrow \K$ be nonsingular dual-pairs, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ and $\basis{C}\eqqc \{ c_1,\ldots ,c_e\}$ be bases for $V_1$ and $V_2$ respectively, and let $T\colon V_1\rightarrow V_2$ be linear.  Then,
	\begin{equation}
		\coordinates{T^{\T}}{\basis{B}^{\T}\leftarrow \basis{C}^{\T}}=\coordinates{T}{\basis{C}\leftarrow \basis{B}}^{\T},
	\end{equation}
	where we have defined
	\begin{equation}\label{eqn5.2.72}
		[A^{\T}]\indices{^i_j}\ceqq A\indices{_j^i}
	\end{equation}
	for $A$ an $e\times d$ matrix.
	\begin{rmk}
		$A^{\T}$ is the \term{transpose}\index{Transpose (of a matrix)} of $A$.  Intuitively, it is the matrix formed by `flipping' $A$ about its diagonal.  For example,
		\begin{equation}
		\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9\end{bmatrix}^{\T}=\begin{bmatrix}1 & 4 & 7 \\ 2 & 5 & 8 \\ 3 & 6 & 9\end{bmatrix}.
		\end{equation}
		
		That we have staggered the indices the way we have will be clear when we study index notation---see \cref{TransposeIndex}.  In the context of matrices, however, this staggering is just suggestive---the only thing that really matters is the ordering, and so we could have written $[A^{\T}]_{ij}\ceqq A_{ji}$, though this is probably not the best practice.
	\end{rmk}
	\begin{rmk}
		In words, for any choice of bases, the matrix of the transpose is the transpose of the matrix.  It is in this sense that the ``transpose'' of a linear-transformation and the ``transpose'' of a matrix coincide.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

\section{The tensor product}

We said before that multilinear algebra is the study of multilinear maps.  From the definition \cref{MultilinearTransformation}, it is clear that (ordinary) linear algebra will be relevant to a study of linear maps, but what would be really awesome is if we could reduce the entire study of multilinear maps to just linear maps so that we can apply everything we've learned thus far.  It is the \emph{tensor product} that allows us to do this.

\subsection{The definition}

\begin{thm}{Tensor product (of bimodules)}{TensorProduct}
	Let $V$ be an $R$-$S$-bimodule and let $W$ be an $S$-$T$-bimodule.  Then, there is a unique bilinear map $\blank \otimes \blank \colon V\times W\rightarrow V\otimes _SW$ into the $R$-$T$-bimodule $V\otimes _SW$\index[notation]{$V\otimes _SW$}, the \term{tensor product}\index{Tensor product (of bimodules)} of $V$ and $W$ over $S$, such that if $V\times W\rightarrow U$ is any other bilinear map into an $R$-$T$-bimodule $U$, then there is a unique map of bimodules $V\otimes _SW\rightarrow U$ such that the following diagram commutes.
	\begin{equation}\label{eqn5.3.2}
		\begin{tikzcd}
			V\times W \ar[r,"\blank \otimes \blank "] \ar[rd] & V\otimes _SW \ar[d,dashed] \\
			& U
		\end{tikzcd}
	\end{equation}
	\begin{rmk}
		For $v\in V$ and $w\in W$, the image under the bilinear map $V\times W\rightarrow V\otimes _SW$ is written $v\otimes w\in V\otimes _SW$\index[notation]{$v\otimes w$} and is the \term{tensor product}\index{Tensor product (of vectors)} of $v$ and $w$.
	\end{rmk}
	\begin{rmk}
		There is an analogous result for not just bilinear maps, but all types of multilinear maps.  Specifically, if $V_k$ is a $\K _k$-$\K _{k+1}$-bimodules, then we have a multilinear map $V_1\times \cdots \times V_m\rightarrow V_1\otimes _{\K _1}\cdots \otimes _{\K _{m-1}}V_m$ into a $\K _1$-$\K _m$-bimodule that is ``universal'' in a sense exactly analogous to \eqref{eqn5.3.2}.
		
		Additionally, the empty tensor product over $\K$, that is, the tensor product of no spaces, is defined to be $\K$ itself, the motivation for which can be seen from \cref{prp1.3.22}\cref{prp1.3.22(i)}\cref{prp1.3.22(ii)}.  In symbols:
		\begin{equation}
			\underbrace{V\otimes _{\K}\cdots \otimes _{\K}V}_0\ceqq \K .
		\end{equation}
	\end{rmk}
	\begin{rmk}
		To clarify, there are tensor products of \emph{bimodules}, and then there are tensor products of \emph{vectors themselves}.  The tensor product of two vectors `lives in' the tensor product of the corresponding bimodules.  And in fact, \emph{everything} in $V\otimes _SW$, while \emph{not} of the form $v\otimes w$ itself necessarily, can be written as a finite sum of elements of this form---see \cref{prp5.3.15}.  (Elements of the form $v\otimes w$ are sometimes called \term{pure}\index{Pure tensor} or \term{simple}\index{Simple tensor}, as opposed to, e.g.~, $v_1\otimes w_1+v_2\otimes w_2$).
	\end{rmk}
	\begin{rmk}
		If $S$ is clear from context, it may be omitted:  $V\otimes W\ceqq V\otimes _SW$\index[notation]{$V\otimes W$}.
	\end{rmk}
	\begin{rmk}
		As in \cref{PolynomialAlgebraMultiple} (the defining result for polynomial algebras), $V\otimes _SW$ is not \emph{literally} unique, but instead, it is ``unique up to unique isomorphism''.  In this case, this means that if $V\times W\rightarrow U$ is another bilinear map into a $R$-$T$-bimodule satisfying the same property as $V\otimes _SW$, then there is a unique isomorphism of bimodules $V\otimes _SW\rightarrow U$ such that the following diagram commutes.
		\begin{equation}
		\begin{tikzcd}
		V\times W \ar[r] \ar[rd] & V\otimes _SW \ar[d,dashed] \\
		& U
		\end{tikzcd}
		\end{equation}
		
		Note that while people say ``unique up to \emph{unique} isomorphism'', the isomorphism is itself not unique---it would be more accurate to say ``unique up to unique isomorphisms which commute with blah blah blah diagram''.  That is to say, while there might be many isomorphisms between $V\otimes _SW$ and $U$, there is only one which makes the above diagram commute.  Given that the latter option, while more accurate, is incredibly verbose, people just stick to ``unique up to unique isomorphism''.
	\end{rmk}
	\begin{rmk}
		A common question I've gotten from students is ``But what actually \emph{is} $v\otimes w$?''.  I'm afraid there's not a terribly good answer for that.  It is what it is:  the image of $\coord{v,w}\in V\times W$ under the canonical bilinear map $V\times W\rightarrow V\otimes _SW$.  As that's probably not very satisfying, I ask you to consider the following.
		
		What if I asked you ``But what actually \emph{is} $\sqrt{2}\cdot \uppi$?''.  The answer is that it is what it is:  it's the product of $\sqrt{2}$ and $\uppi$.  You can't really reduce it to something simpler without going so far out of your way so as to not be worth it.  For example, what are you going to do?  Try to argue that the number $\sqrt{2}\cdot \uppi$ is $\uppi$ added to itself $\sqrt{2}$ times?  Good luck with that.
		
		Anyways, I'm sure you probably don't feel very uncomfortable talking about ``$\sqrt{2}\cdot \uppi$'', and my claim is that if you feel comfortable working with this, then you should feel comfortable working with $v\otimes w$.
		
		That said, in special cases, one can be a bit more explicit---see the remark in \cref{Tensor}, and in particular, \eqref{eqn5.4.6}.  I personally don't find this perspective particularly useful, but I have found that some students to.
	\end{rmk}
	\begin{rmk}
		Thus you can take the tensor product of an $R$-$S$-bimodule and an $S$-$T$-bimodule, the result being an $R$-$T$-bimodule.  To remember this, you might note that this is exactly analogous to matrix multiplication:  the `inner' things have to be the same in which case the result has the structure coming from the `outer' things.
		
		This was one motivation for working with bimodules.\footnote{The other big motivation is that you will need to learn tensor products in this level of generality at some point in your mathematical life, so may as well learn it now.}  If I were working just with vector spaces, then you could take the tensor product of any two things you like, but in this context, you can only take the tensor product of an $R\times S$-bimodule and an $S$-$T$-bimodule with the result being an $R$-$T$-bimodule---this makes it clearer what roles everything is playing. 
	\end{rmk}
	\begin{rmk}
		\emph{This is important---do not ignore.}  Essentially what this result says is that, instead of working with \emph{bilinear} maps $V\times W\rightarrow U$, instead, we can work with \emph{linear} maps $V\otimes _SW\rightarrow U$.  You'll find in time that this `trade-off' is worth it.
		
		In practice, this is often used in the following way.  Suppose you want to define a function $T\colon V\otimes _SW\rightarrow U$.  The definition of the tensor product says that \emph{you only need to say where elements of the form $v\otimes w$ map to}.  In practice, you will say something like ``Let $T(v\otimes w)\ceqq \text{blah blah blah}$\textellipsis'', and while superficially it doesn't look like you're defining $T$ on all of $V\otimes _SW$ (because you're not), this is enough.  As long as your ``$\text{blah blah blah}$'' is bilinear in $\coord{v,w}\in V\times W$, the definition of the tensor product says that this corresponds to a unique linear map $V\otimes _SW\rightarrow U$.  This idea is similar to that \cref{prp1.2.68}, where you can define linear-transformations by only defining what it does to a basis.  Similarly here, you can define a linear-transformation on all of $V\otimes _SW$ by only specifying what happens to elements of the form $v\otimes w$.
		
		TL;DR:
		\begin{important}
			To define linear maps $V\otimes _SW\rightarrow U$, it suffices to say where elements of the form $v\otimes w\in V\otimes _SW$ get mapped to.  As long as what you write down is bilinear in $\coord{v,w}$, the definition of the tensor product says that this serves to define a unique linear map on all of $V\otimes _SW$.
		\end{important}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
This result says that if I every have a bilinear map $V\times W\rightarrow U$, I can `replace' it with a linear map $V\otimes _SW\rightarrow U$.  In this sense, the tensor product reduces the study of multilinear maps to linear maps.

There are a couple ways to think about $V\otimes _SW$ itself more intuitively.  First of all, note that $\blank \otimes \blank \colon V\times W\rightarrow V\otimes _SW$ satisfying the following properties.
\begin{subequations}
	\begin{align}
		v\otimes (w_1+w_2) & =v\otimes w_1+v\otimes w_2 \\
		(v_1+v_2)\otimes w & =v_1\otimes w+v_2\otimes w \\
		(\alpha \cdot v)\otimes w & =\alpha \cdot (v\otimes w) \\
		v\otimes (w\cdot \alpha ) & =(v\otimes w)\cdot \alpha \\
		(v\cdot \alpha )\otimes w & =v\otimes (\alpha \cdot w)\label{eqn5.3.5e}
	\end{align}
\end{subequations}
Furthermore, $V\otimes _SW$ is the ``freest'' $R$-$T$-bimodule that satisfies these identities, that is, $\blank \otimes \blank$ satisfies only the above identities and those they imply.  Thus, it is safe to think of $V\otimes _SW$ as the $R$-$T$-bimodule spanned by elements of the form $v\otimes w$, $v\in V$ and $w\in W$, with the only rules for `simplification' given above.  Note in particular, however, that there will most certainly be other elements in $V\otimes _SW$ besides just elements of the form $v\otimes w$.  For example, $v_1\oplus w_1+v_2\oplus w_2$ in general cannot be written as $v\otimes w$.

Another way of thinking about $V\otimes W$ that might help your intuition is in terms of bases.  Though this is only true for vector spaces,\footnote{Duh.  We don't have bases for general modules.} it essentially says the following:  if you have a bunch of $b_k$s that are a basis for $V$ and a bunch of $c_l$s that are a basis for $W$, then the collection of all $b_k\otimes c_l$s forms a basis for $V\otimes W$.  Thus, the elements of $V\otimes W$ are precisely those things that can be written (uniquely) as a linear combinations of $b_k\otimes c_l$s.
\begin{prp}{Basis for $V\otimes W$}{}
	Let $V$ and $W$ be vector spaces over a field $\F$, and let $\basis{B}$ and $\basis{C}$ be bases for $V$ and $W$ respectively.  Then,
	\begin{equation}
	\left\{ b\otimes c:b\in \basis{B},c\in \basis{C}\right\}
	\end{equation}
	is a basis for $V\otimes W$.
	\begin{rmk}
		In particular, $\dim (V\otimes W)=\dim (V)\dim (W)$.
	\end{rmk}
	\begin{rmk}
		We see immediately from working in the level of generality that we did that the ground division ring need be commutative, that is, a field.  If it weren't, then $V$ would be just an $\F$-$\Z$-bimodule and $W$ would be a $\F$-$\Z$-bimodule, in which case we could not take their tensor product!
	\end{rmk}
	\begin{rmk}
		This result can probably be generalized to the noncommutative case, but then we will need to take $V$ to be a $\K _1$-$\L$-bimodule and $W$ to be an $\L$-$\K _2$-bimodule, with $\K _1$ and $\K _2$ division rings.  Furthermore, the statement would require us to have a notion of ``basis'' for bimodules.  It's easy enough to write one down, but as we have not done so, we refrain from `officially' giving this noncommutative version.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{crl}{}{crl5.3.59}
	Let $V$ and $W$ be vector spaces over a field, and let $v\in V$ and $w\in W$.  Then, if $v\otimes w=0$, then $v=0$ or $w=0$.
	\begin{rmk}
		Warning:  This may fail for general $\K$-modules---see \cref{exm5.3.59}.
	\end{rmk}
	\begin{rmk}
		Just as the previous result should generalize to the noncommutative case, so to should this one.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{crl}
Not only can this corollary fail in general for modules, but something quite bit worse can happen.
\begin{exm}{$V\otimes W=0$ with $V,W\neq 0$}{exm5.3.59}
	Define $\K \ceqq \Z$, $V\ceqq \Q$, and $W\ceqq \Z /2\Z$.  Then, for $v\otimes w\in V\otimes _{\Z}W$, we have
	\begin{equation}
	v\otimes w=\left( v\tfrac{1}{2}\right) \otimes (2w)=\left( v\tfrac{1}{2}\right) \otimes 0=0,
	\end{equation}
	and hence $V\otimes _{\Z}W=0$.
\end{exm}

Students tend to find the tensor product (and tensors in general) quite confusing.  I get it.  This definition, the first time you see it, is difficult to understand.  My advice is to go on, even if you don't quite understand everything, and to not stress about it for now.  What is more important the first time around is whether or not you can actually \emph{work with} tensors.  For example, I'm sure virtually everyone reading this learned how to compute integrals quite awhile before they were actually able to define the integral.  In a similar way, to do more `computational' things with tensors, you don't really need to have a complete understanding of the definition of the tensor product.\footnote{Though, to be sure, if you want to \emph{prove} things, you're likely going to need to understand the definition.}  And pedagogically anyways, it does make sense to teach things in that order:  you're going to have much more intuition for things (which is essential for coming up with proofs) if you've previously had hands-on concrete experience with it.

So, don't worry for the time being.  Move on, and work out some ``hands-on concrete'' problems with tensors.\footnote{For example, see \cref{EinsteinsEquation,CliffordAlgebra}.}  After you're more comfortable working with tensors, you can come back again to this definition to deepen your understanding.\footnote{And maybe then, if things still don't make sense, you can begin to worry.}

\subsection{Tensor products of linear-transformations}\label{sbsTensorProductsOfLinearTransformations}

When studying dual-spaces, we first defined $V^{\T}$, and subsequently defined $T^{\T}$, finding later that (\cref{prp5.2.36}) the common notation was justified as this defined a cofunctor (\cref{Cofunctor}).  We have now defined what it means to take the tensor product of spaces, and so it is natural to wonder whether there is a notion of tensor product of linear-transformations.  Of course, there is.
\begin{thm}{Tensor product (of linear-transform\-ations)}{TensorProductLinearTransformationx}
	Let $V_1$, $W_1$, $V_2$, and $W_2$ be $\K$-$\K$-bimodules, and let $S\colon V_1\rightarrow W_1$ and $T\colon V_2\rightarrow W_2$ be linear.  Then, there is a unique linear-transformation $S\otimes T\colon V_1\otimes V_2\rightarrow W_1\otimes W_2$\index[notation]{$S\otimes T$}, the \term{tensor product (of linear-transformations)}, such that
	\begin{equation}
	[S\otimes T](v_1\otimes v_2)=S(v_1)\otimes T(v_2).
	\end{equation}
	\begin{rmk}
		In particular, given $T\colon V\rightarrow V$ linear, we obtain maps
		\begin{equation}
			\tensoralg ^kT\colon \tensoralg ^kV\rightarrow \tensoralg ^kV
		\end{equation}
		for all $k\in \N$.
	\end{rmk}
	\begin{proof}
		$V_1\times V_2\ni \coord{v_1,v_2}\mapsto S(v_1)\otimes T(v_2)$ is bilinear, and so by the definition of the tensor product (\cref{TensorProduct}), there is a unique linear map $S\otimes T\colon V_1\otimes V_2\rightarrow W_1\otimes W_2$ such that $[S\otimes T](v_1\otimes v_2)=S(v_1)\otimes T(v_2)$.
\end{proof}
\end{thm}
\begin{thm}{$\blank \otimes \blank$ is a functor}{}
Let $S_1\colon V_1\rightarrow V_2$, $S_2\colon V_2\rightarrow V_3$, $T_1\colon W_1\rightarrow W_2$, and $T_2\colon W_2\rightarrow W_3$ be linear-transformations of $\K$-$\K$-bimodules.  Then,
\begin{equation}
[S_2\circ S_1]\otimes [T_2\circ T_1]=[S_2\otimes T_2]\circ [S_1\otimes T_1]
\end{equation}
as linear-transformations $V_1\otimes _{\K}W_1\rightarrow V_3\otimes _{\K}W_3$.
\begin{rmk}
	Of course, it is also true that $\id _V\otimes \id _W=\id _{V\otimes W}$.
\end{rmk}
\begin{proof}
	We leave this as an exercise.
	\begin{exr}[breakable=false]{}{}
		We leave this as an exercise.
	\end{exr}
\end{proof}
\end{thm}
There is actually a slight notational ambiguity here---this notation conflicts with
\begin{equation}
	S\otimes T\in \Mor (V_1,W_1)\otimes \Mor (V_2\otimes W_2).
\end{equation}
We will see how to resolve this in \cref{TensorProductLinearTransformation}.

\subsection{Basic properties}

We continue with an enumeration of some basic properties of the tensor product.
\begin{prp}{}{prp5.3.15}
	Let $R$, $S$, and $T$ be rings, let $V$ be an $R$-$S$-bimodule and let $W$ be an $S$-$T$-bimodule.  Then,
	\begin{equation}
	V\otimes _SW=\Span \left\{ v\otimes w:v\in V,w\in W\right\} .
	\end{equation}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{}{prp5.3.16}
	\begin{enumerate}
		\item \label{prp5.3.16(i)}Let $Q$, $R$, $S$, and $T$ be rings, let $U$ be a $Q$-$R$-bimodule, let $V$ be an $R$-$S$-bimodule, and let $W$ be an $S$-$T$-bimodule.  Then,
		\begin{equation}
		(U\otimes _RV)\otimes _SW\cong U\otimes _RV\otimes _SW\cong U\otimes _R(V\otimes _SW)
		\end{equation}
		via the unique maps that has the property $(u\otimes v)\otimes w\mapsto u\otimes v\otimes w$ and $u\otimes v\otimes w\mapsfrom u\otimes (v\otimes w)$ respectively.
		\item \label{prp5.3.16(ii)}Let $\K$ is a cring, and let $V$ and $W$ be $\K$-modules.  Then,
		\begin{equation}
		V\otimes _{\K}W\cong W\otimes _{\K}V
		\end{equation}
		via the unique map that has the property $v\otimes w\mapsto w\otimes v$.
		\item \label{prp5.3.16(iii)}Let $R$, $S$, and $T$ be rings, and let $V$ be an $R$-$S$-bimodule and let $W$ be an $S$-$T$-module.  Then,
		\begin{enumerate}
			\item if $W=\bigoplus _{U\in \collection{U}}U$, then
			\begin{equation}
			V\otimes _S\bigg( \bigoplus _{U\in \collection{U}}U\bigg) \cong \bigoplus _{U\in \collection{U}}(V\otimes _SU)
			\end{equation}
			via the unique map that has the property $v\otimes w\mapsto \sum _{U\in \collection{U}}v\otimes \proj _U(w)$; and
			\item if $V=\bigoplus _{U\in \collection{U}}U$, then
			\begin{equation}
			\bigg( \bigoplus _{U\in \collection{U}}U\bigg) \otimes _SW\cong \bigoplus _{U\in \collection{U}}(U\otimes _SW)
			\end{equation}
			via the unique map that has the property $v\otimes w\mapsto \sum _{U\in \collection{U}}\proj _U(v)\otimes w$.
		\end{enumerate}
	\end{enumerate}
	\begin{rmk}
		That is, \cref{prp5.3.16(i)} the tensor product is associative, \cref{prp5.3.16(ii)} commutative over commutative rings, and \cref{prp5.3.16(iii)} distributes over a direct sums.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{}{prp1.3.22}
	Let $R$, $S$, and $T$ be rings, let $V$ be an $R$-$S$-bimodule and let $W$ be an $S$-$T$-bimodule.
	\begin{enumerate}
		\item \label{prp1.3.22(i)}$V\cong V\otimes _SS$ naturally via the map $v\mapsto v\otimes 1$.
		\item \label{prp1.3.22(ii)}$W\cong S\otimes _SW$ naturally via the map $w\mapsto 1\otimes w$.
		\item \label{prp1.3.22(iii)}$V\otimes _S0\cong 0\cong 0\otimes _SW$.
		\item \label{prp1.3.22(iv)}$0\otimes w=0=v\otimes 0\in V\otimes _SW$ for all $v\in V$ and $w\in W$.
	\end{enumerate}
	\begin{rmk}
		Recall that (see the remark in the definition of the tensor product \cref{TensorProduct}) that
		\begin{equation}
			\underbrace{V\otimes _{\K}\cdots \otimes _{\K}V}_0\ceqq \K .
		\end{equation}
		This definition is motivated by \cref{prp1.3.22(i)} and \cref{prp1.3.22(ii)}, which state that the tensor product by $\K$ acts as the `identity', and so we define the empty tensor product to be $\K$ for the same reason we define the empty product in, say, a ring, to be the multiplicative identity (\cref{dfnA.1.33}).
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

\begin{exr}{}{}
	Let $R$, $S$, and $T$ be rings, let $U$ be a $R$-$S$-bimodule, let $V$ be an $S$-$T$-bimodule, let $W$ be an $R$-$T$-bimodule, and let $f\colon U\otimes _SV\rightarrow W$ be linear.
	
	Suppose that $f(u\otimes v)=0$ implies that $u\otimes v=0$ for all $u\in U$ and $v\in V$.  Need $f$ be injective?  Does the answer change if the rings are commutative or division rings?  What if everything is a finite-dimensional vector space?
\end{exr}

\subsection{Natural-isomorphisms}

What follows is one of the most important properties of the tensor product.
\begin{thm}{Tensor-Hom Adjunction}{TensorHomAdjunction}
	Let $R$, $S$, and $T$ be rings, and let $U$ be an $R$-$S$ bimodule, $V$ and $S$-$T$ bimodule, and $W$ an $R$-$T$ bimodule.
	\begin{enumerate}
		\item \label{TensorHomAdjunction(i)}The map
		\begin{equation}\label{eqn5.3.7}
			\begin{multlined}
				\Mor _{\Mod{R}[T]}(U\otimes _SV,W)\leftarrow \\ \Mor _{\Mod{S}[T]}(V,\Mor _{\Mod{R}}(U,W))
			\end{multlined}
		\end{equation}
		defined by
		\begin{equation}\label{eqn5.3.8}
			(u\otimes v\mapsto [\phi (v)](u))\mapsfrom \phi
		\end{equation}
		is an isomorphism of commutative groups.
		\item \label{TensorHomAdjunction(ii)}The map
		\begin{equation}\label{eqn5.3.9}
			\begin{multlined}
				\Mor _{\Mod{R}[T]}(U\otimes _SV,W)\leftarrow \\ \Mor _{\Mod{R}[S]}(U,\Mor _{\Mod*{T}}(V,W))
			\end{multlined}
		\end{equation}
		defined by
		\begin{equation}
			(u\otimes v\mapsto [\phi (u)](v))\mapsfrom \phi
		\end{equation}
		is an isomorphism of commutative groups.
	\end{enumerate}
	\begin{rmk}
		The $R$, $S$, and $T$s everywhere clutter things up.  Dropping all of the notational baggage, these become the more readable
		\begin{align}
			\Mor (U\otimes V,W) & \cong \Mor (V,\Mor (U,W)) \\
			\Mor (U\otimes V,W) & \cong \Mor (U,\Mor (V,W)).
		\end{align}
	\end{rmk}
	\begin{rmk}
		To understand this, it might first help to understand an analogous result in a different category:  the map defined analogously as above yields an isomorphism
		\begin{equation}
			\Mor _{\Set}(X\times Y,Z)\rightarrow \Mor _{\Set}(X,\Mor _{\Set}(Y,Z)).
		\end{equation}
		In other words, functions from $X\times Y$ into $Z$ are `the same as' functions from $X$ into $\Mor _{\Set}(Y,Z)$; given a function of two variables, we can instead think of it as a function-valued function $f\mapsto (x\mapsto (y\mapsto f(x,y)))$.  In computer science, this concept is called \emph{currying}.  Thus, you could say that this result is just the linear algebraic analogue of currying.
	\end{rmk}
	\begin{rmk}
		The ``Hom'' in ``Tensor-Hom Adjunction'' comes from the fact that ``$\Mor$'' is often written as ``$\Hom$''.
	\end{rmk}
	\begin{rmk}
		Though you (probably) don't know what the term means yet, it turns out that this (by which I mean \cref{TensorHomAdjunction(i)}) actually yields what is called an adjunction\footnote{This means that not only is \eqref{eqn5.3.8} an isomorphism, but it defines an isomorphism that is natural (\cref{NaturalTransformation}) in both $V$ and $W$.} between the functors $U\otimes _S\blank \colon \Mod{S}[T]\rightarrow \Mod{R}[T]$ and $\Mor _{\Mod{R}}(U,\blank )\colon \Mod{R}[T]\rightarrow \Mod{S}[T]$, hence ``Tensor-Hom Adjunction''.  In this case, we say that $U\otimes _S\blank$ is \emph{left adjoint} to $\Mor _{\Mod{R}}(U,\blank )$, and the other way around, that $\Mor _{\Mod{R}}(U,\blank )$ is \emph{right adjoint} to $U\otimes _S\blank$.  Thus, as the tensor product is the \emph{left} adjoint and the ``Hom'' is the \emph{right} adjoint, I recommend you say ``tensor-hom adjunction'' and \emph{not} ``hom-tensor adjunction''.
		
		Dually, \cref{TensorHomAdjunction(ii)} yields an adjunction between the functors $\blank \otimes _SV$ and $\Mor _{\Mod*{T}}(V,\blank )$.
	\end{rmk}
	\begin{proof}
		We prove \cref{TensorHomAdjunction(i)}.  The proof of \cref{TensorHomAdjunction(ii)} is essentially identical.
		
		Given $f\colon U\otimes _SV\rightarrow W$ a map of $R$-$T$-bimodules, define $g_f\colon V\rightarrow \Mor _{\Mod{R}}(U,W)$ by
		\begin{equation}
			[g_f(v)](u)\ceqq f(u\otimes v).
		\end{equation}
		First of all, note that $g_f(v)\in \Mor _{\Mod{R}}(U,W)$ as $f$ is linear and the tensor product is bilinear.
		
		To show that $g_f\colon V\rightarrow \Mor _{\Mod{R}}(U,W)$ is a map of $S$-$T$-bimodules, we must show that
		\begin{equation}
			[g_f(s\cdot v\cdot t)](u)=[s\cdot [g_f(v)]\cdot t](u)
		\end{equation}
		for all $u\in U$.  However, recall from \cref{exm1.1.72} that the $S$-$T$-bimodule action on $\Mor _{\Mod{R}}(U,W)$ is given by
		\begin{equation}
			[s\cdot T\cdot t](u)\ceqq T(u\cdot s)\cdot t,
		\end{equation}
		and hence what we would actually like to show is that
		\begin{equation}
			[g_f(s\cdot v\cdot t)](u)=[g_f(v)](u\cdot s)\cdot t.
		\end{equation}
		From the definition of $g_f$, this means we would like to show that
		\begin{equation}
			f\left( u\otimes (s\cdot v\cdot t)\right) =f\left( (u\cdot s)\otimes v\right) \cdot t.
		\end{equation}
		This is of course true because $f$ is linear and because of properties of the tensor product (\eqref{eqn5.3.5e}).
		
		Finally, to check that $f\mapsto g_f$ is a group homomorphism
		\begin{equation}
			\Mor _{\Mod{R}[T]}(U\otimes _SV,W)\rightarrow \Mor _{\Mod{S}[T]}(V,\Mor _{\Mod{R}}(U,W)),
		\end{equation}
		we must show that $g_{f_1+f_2}=g_{f_1}+g_{f_2}$.  In other words, we must show that
		\begin{equation}
			\begin{split}
				[f_1+f_2](u\otimes v) & =[g_{f_1+f_2}(v)](u)=[[g_{f_1}+g_{f_2}](v)](u)\\
				& =f_1(u\otimes v)+f_2(u\otimes v)
			\end{split}
		\end{equation}
		for all $u\in U$ and $v\in U$.  This is of course true because of the definition of addition of functions.
		
		To show that $f\mapsto g_f$ is an isomorphism, we construct an inverse $g\mapsto f_g$ from $\Mor _{\Mod{S}[T]}(V,\Mor _{\Mod{R}}(U,W))$ to $\Mor _{\Mod{R}[T]}(U\otimes _SV,W)$.  So, let $g\colon V\rightarrow \Mor _{\Mod{R}}(U,W)$ be a map of $S$-$T$-bimodules and define $f_g\colon \Mor _{\Mod{R}[T]}(U\otimes _SV,W)$ by
		\begin{equation}
			f_g(u\otimes v)\ceqq [g(v)](u).
		\end{equation}
		As this is bilinear in $u$ and $v$, this serves to define a map of $S$-$T$-bimodules $U\otimes _SV\rightarrow W$---see the last remark in the definition of the tensor product (\cref{TensorProduct}).  As the check that $g\mapsto f_g$ is a group homomorphism is similar to before, we omit it (it comes down to the definition of addition of functions).
		
		It remains to check that $f\mapsto g_f$ and $g\mapsto f_g$ are inverse to each other.  To do that, we must show that $g_{f_g}=g$ and $f_{g_f}=f$.  For the first one, note that
		\begin{equation}
			[[g_{f_g}](v)](u)\ceqq f_g(u\otimes v)\ceqq [g(v)](u).
		\end{equation}
		As this holds for all $u\in U$ and $v\in V$, we have $g_{f_g}=g$.  For the other one, note that
		\begin{equation}
			[f_{g_f}](u\otimes v)\ceqq [g_f(v)](u)\ceqq f(u\otimes v),
		\end{equation}
		and again we have that $f_{g_f}=f$, as desired.
	\end{proof}
\end{thm}
What follows are a couple of results similar in flavor to the tensor-hom adjunction.  While the tensor-hom adjunction is probably more important in mathematics in general, for us, the following three results will be more important, and you should take note of them, especially the case of finite-dimensional vector spaces.
\begin{thm}{$\Mor (V_1,W_1)\otimes \Mor (V_2,W_2)\cong \Mor (V_1\otimes V_2,W_1\otimes W_2)$}{TensorProductLinearTransformation}
	Let $V_1$, $W_1$, $V_2$, and $W_2$ be $\K$-$\K$-bimodules.  Then, the map
	\begin{equation*}
		\Mor (V_1,W_1)\otimes \Mor (V_2,W_2)\rightarrow \Mor (V_1\otimes V_2,W_1\otimes W_2)
	\end{equation*}
	defined by
	\begin{equation}
		S\otimes T\mapsto (v_1\otimes v_2\mapsto S(v_1)\otimes T(v_2))
	\end{equation}
	is linear and natural.
	
	Furthermore,
	\begin{enumerate}
		\item \label{TensorProductLinearTransformation(i)}if $V_1$, $W_1$, $V_2$, and $W_2$ are vector spaces, then this map is injective; and
		\item \label{TensorProductLinearTransformation(ii)}if $V_1$, $W_1$, $V_2$, and $W_2$ are finite-dimensional vector spaces, then this map is an isomorphism.
	\end{enumerate}
	\begin{rmk}
		We will abuse notation write write $S\otimes T$\index[notation]{$S\otimes T$} for both the element in the tensor product $\Mor (V_1,W_1)\otimes \Mor (V_2,W_2)$ and the linear-transformation it defines $V_1\otimes V_2\rightarrow W_1\otimes W_2$.  Of course, this result says that this isn't even really an abuse of notation if everything involved is a vector space, but even if they weren't, this abuse should not cause any confusion.
	\end{rmk}
	\begin{rmk}
		Warning:  This need not be an isomorphism even for vector spaces if they are not finite-dimensional---see \cref{exm5.3.27}.
	\end{rmk}
	\begin{proof}
		As this is bilinear in $S$ and $T$, it defines a linear-transformation on the tensor product.
		
		To show naturality, let $f\colon U_1\rightarrow V_1$ be a linear-transformation.  By definition (\cref{NaturalTransformation}), this will be natural in the first space iff the following diagram commutes.
		\begin{equation}
			\begin{tikzcd}[column sep=scriptsize]
				\Mor (V_1,W_1)\otimes \Mor (V_2,W_2) \ar[r] \ar[d] & \Mor  (V_1\otimes V_2,W_1\otimes W_2) \ar[d] \\
				\Mor (U_1,W_1)\otimes \Mor (V_2,W_2) \ar[r] & \Mor (U_1\otimes V_2,W_1\otimes W_2)
			\end{tikzcd}
		\end{equation}
		By definition, this commutes iff
		\begin{equation}
			S(f(u_1))\otimes T(v_2)=S(f(u_1))\otimes T(v_2),
		\end{equation}
		which if tautologically true.\footnote{Going right then down essentially gives $S(v_1)\otimes T(v_2)$ and then replaces $v_1$ with $f(u_1)$.  Going down then right replaces $v_1$ with $f(u_1)$ and then takes $S(f(u_1))\otimes T(u_1)$.}  By $V_1\leftrightarrow V_2$ symmetry, it is natural in $V_2$ as well.  A similar check shows that it is natural in $W_1$, and hence $W_2$ as well.
		
		\blni
		\cref{TensorProductLinearTransformation(i)} Suppose that $V_1$, $W_1$, $V_2$, and $W_2$ are vector spaces.  Let $\sum _{k=1}^m\sum _{l=1}^nS_k\otimes T_l$ be an arbitrary nonzero element of $\Mor (V_1,W_1)\otimes \Mor (V_2,W_2)$.  Without loss of generality, let $m,n\in \Z ^+$ be the smallest such positive integers.  \footnote{So, for example, if you can simplify $S_1\otimes T_1+S_2\otimes T_2$ to something of the form $S\otimes T$, do that, and take $m=1=n$ instead of $m=2=n2$.}
		
		Now suppose that this element of $\Mor (V_1,W_1)\otimes \Mor (V_2,W_2)$ is sent to $0$.  In other words,
		\begin{equation}
			\sum _{k=1}^m\sum _{l=1}^nS_k(v_1)\otimes T_l(v_2)=0
		\end{equation}
		for all $v_1\in V_1$ and $v_2\in V_2$.  Writing this as
		\begin{equation}
			\bigg( \sum _{k=1}^mS_k(v_1)\bigg) \bigg( \sum _{l=1}^nT_l(v_2)\bigg) =0,
		\end{equation}
		using \cref{crl5.3.59}, we deduce that either $\sum _{k=1}^mS_k(v_1)=0$ or $\sum _{l=1}^nT_l=0$.  In the the former case, we can replace $S_m$ in $\sum _{k=1}^m\sum _{l=1}^nS_k\otimes T_l$ with $-\sum _{k=1}^{n-1}S_k$, thereby writing this element with only $n-1$ $w_l$s:  a contradiction.  The latter case is identical.  Thus, it cannot be the case that a nonzero element of $\Mor (V_1,W_1)\otimes \Mor (V_2,W_2)$ is sent to $0$, and this map is injective.
		
		\blni
		\cref{TensorProductLinearTransformation(ii)} Suppose that $V_1$, $W_1$, $V_2$, and $W_2$ are finite-dimensional vector spaces.  $\Mor (V_1,W_1)\otimes \Mor (V_2,W_2)$ and $\Mor (V_1\otimes V_2,W_1\otimes W_2)$ have the same dimension, namely $\dim (V_1)\dim (W_1)\dim (V_2)\dim (W_2)$, and hence the injective map $\Mor (V_1,W_1)\otimes \Mor (V_2,W_2)\rightarrow \Mor (V_1\otimes V_2,W_1\otimes W_2)$ must in fact be an isomorphism (\cref{crl2.2.74}).
	\end{proof}
\end{thm}
\begin{crl}{$V^{\T}\otimes W\cong \Mor (V,W)$}{prp5.4.9}
	Let $V$ and $W$ be $\K$-$\K$-bimodules.  Then, the map
	\begin{equation}
		V^{\dagger}\otimes _{\K}W\ni \phi \otimes w\mapsto (v\mapsto \phi (v)w)\in \Mor _{\Mod{\K}}(V,W)
	\end{equation}
	is linear and natural.
	
	Furthermore,
	\begin{enumerate}
		\item if $V$ and $W$ are vector spaces, this map is injective; and
		\item if $V$ and $W$ are finite-dimensional vector spaces, this map is an isomorphism.
	\end{enumerate}
	\begin{rmk}
		In particular, for finite-dimensional vector spaces, using language that we will learn shortly (\cref{Tensor}), $\coord{1,1}$ tensors are `the same as' linear-transformations.\footnote{We technically don't define ``tensor'' unless $W=V$, but that doesn't really affect what's going on here---this is just a matter of language.}
	\end{rmk}
	\begin{rmk}
		Warning:  This need not be an isomorphism even for vector spaces if they are not finite-dimensional---see \cref{exm5.3.27}.
	\end{rmk}
	\begin{rmk}
		You've likely seen map before, albeit in a special case.  To elaborate on this in the most insightful manner, we wait until we have index notation, and so postpone this to \cref{exm5.3.57}.
	\end{rmk}
	\begin{proof}
		Take $W_1=\K$, $V_2=\K$, $V_1=V$, and $W_2=W$ in the previous result (\cref{TensorProductLinearTransformation}).  Using the fact that $W\cong \Mor _{\Mod{\K}}(\K ,W)$ (\cref{prp5.2.24}), $V\cong V\otimes _{\K}\K$, and $W\cong \K \otimes _{\K}W$ naturally, \cref{TensorProductLinearTransformation} reduces to exactly the statement of this corollary.
	\end{proof}
\end{crl}
\begin{crl}{$V^{\T}\otimes W^{\T}\cong (W\otimes W)^{\T}$}{crl5.4.12}
	Let $V$ and $W$ be $\K$-$\K$-bimodules.  Then, the map
	\begin{equation}
		V^{\dagger}\otimes W^{\dagger}\ni \phi \otimes \chi \mapsto (v\otimes w\mapsto \phi (v)\chi (w))\in (V\otimes W)^{\dagger}
	\end{equation}
	is linear and natural.
	
	Furthermore,
	\begin{enumerate}
		\item if $V$ and $W$ are vector spaces, this map is injective; and
		\item if $V$ and $W$ are finite-dimensional vector spaces, this map is an isomorphism.
	\end{enumerate}
	\begin{rmk}
		Warning:  This need not be an isomorphism even for vector spaces if they are not finite-dimensional---see \cref{exr5.3.30}.
	\end{rmk}
	\begin{proof}
		Take $W=W^{\T}$ in \cref{prp5.4.9}.  We thus obtain a natural linear map
		\begin{equation}
			V^{\T}\otimes _{\K}W^{\T}\rightarrow \Mor _{\Mod{\K}}(V,W^{\T}).
		\end{equation}
		However, by the \namerefpcref{TensorHomAdjunction}, we have a natural isomorphism
		\begin{equation}
			\begin{split}
				\Mor _{\Mod{\K}}(V,W^{\T}) & \ceqq \Mor _{\Mod{\K}}(V,\Mor _{\Mod{\K}}(W,\K )) \\
				& \cong \Mor _{\Mod _{\K}}(V\otimes _{\K}W,\K )\eqqc [V\otimes _{\K}W]^{\T}.
			\end{split}
		\end{equation}
		Putting these together, we obtain a natural linear map $V^{\T}\otimes _{\K}W^{\T}\rightarrow [V\otimes _{\K}W]^{\T}$, which is injective if $W$ and $W$ are vector spaces and an isomorphism if $V$ and $W$ are finite-dimensional vector spaces (by \cref{prp5.4.9} again).
	\end{proof}
\end{crl}
\begin{exr}{}{}
	Can you find $\K$-modules $V$ and $W$, $\K$ a cring, for which $V^{\T}\otimes W\rightarrow \Mor (V,W)$ is not injective.
\end{exr}
\begin{exm}{Vector space $V$ such that $V^{\T}\otimes V\rightarrow \Mor _{\Vect}(V,V)$ is not an isomorphism}{exm5.3.27}
	Define $V\ceqq \C ^{\infty}$.  As $\{ e_k:k\in \N \}$ is a basis for $V$, every element in $V^{\T}\otimes V$ can be written as a finite linear combination of elements in the set
	\begin{equation}
		\{ \phi \otimes e_k:\phi \in V^{\T},k\in \N \}.
	\end{equation}
	The linear-transformation that $\sum _{k=1}^m\alpha \phi _ke_k$ defines sends $v\in V$ to
	\begin{equation}
		\sum _{k=0}^m\alpha \phi _k(v)e_k\in \Span (\{ e_k:0\leq k\leq m \} )
	\end{equation}
	In particular, the image of any such element is finite-dimensional.  On the other hand, the identity $V\rightarrow V$ has infinite-dimensional image, and so can't possibly be of this form.  Thus, the map $V^{\T}\otimes V\rightarrow \Mor _{\Vect}(V,V)$ is not surjective.
\end{exm}
\begin{exr}{}{}
	Can you find $\K$-modules $V$ and $W$, $\K$ a cring, for which $V^{\T}\otimes W\rightarrow [V\otimes W]^{\T}$ is not injective.
\end{exr}
\begin{exr}{}{exr5.3.30}
	Find a vector space $V$ for which the canonical map $V^{\T}\otimes V^{\T}\rightarrow [V\otimes V]^{\T}$ is not an isomorphism.
\end{exr}

\section{Tensors and index notation}\label{sct5.4}

Roughly speaking, a tensor rank $\coord{k,l}$ is going to be something that takes in $l$ vectors and spits out $k$ vectors\footnote{More accurately, a element of $V^{\otimes k}$.} in a multilinear manner.  For example, linear-transformations are $\coord{1,1}$ tensors, covectors are $\coord{0,1}$ tensors, vectors are $\coord{1,0}$ tensors, and scalars are $\coord{0,0}$ tensors:  linear-transformations take in $1$ vector and spit out $1$ vector, covectors take in $1$ vector and spits out a scalar\footnote{Which, for our purposes, is to be regarded as `zero vectors'.}, vectors take in $0$ vectors and spit out $1$ vector, and scalars take in $0$ vectors and spit out $0$ vectors.  Similarly, a pairing on a dual-pair will be a tensor of type $\coord{0,2}$:  it takes in two vectors and spits out a scalar.

\subsection{The tensor algebra}

We make this precise as follows.
\begin{dfn}{Tensor}{Tensor}
	Let $V$ be a $\K$-$\K$-bimodule.  Then, a \term{tensor}\index{Tensor of rank $\coord{k,l}$} of \term{rank}\index{Rank (of a tensor)} $\coord{k,l}$ over $V$ is an element of
	\begin{equation}\label{eqn5.4.2}
		\tensoralg _l^kV\ceqq \Mor _{\Mod{\K}}(\underbrace{V\otimes \cdots \otimes V}_l,\underbrace{V\otimes \cdots \otimes V}_k).
	\end{equation}\index[notation]{$\bigotimes _l^kV$}
	\begin{rmk}
		$k$ is the \term{contravariant rank}\index{Contravariant rank} and $l$ is the \term{covariant rank}\index{Covariant rank}.  If $l=0$, then the tensor is \term{contravariant}\index{Contravariant tensor}, and if $k=0$, then the tensor is \term{covariant}\index{Covariant tensor}.
		
		We write
		\begin{equation}
			\tensoralg ^kV\ceqq \tensoralg _0^kV
		\end{equation}\index[notation]{$\tensoralg ^kV$}
		and
		\begin{equation}
			\tensoralg _lV\ceqq \tensoralg _l^0V
		\end{equation}\index[notation]{$\tensoralg _lV$}
		respectively for the spaces of rank $k$ contravariant tensors and rank $l$ covariant tensors.
	\end{rmk}
	\begin{rmk}
		Though uncommon, I have seen the term \term{valence}\index{Valence} used synonymously with ``rank'' in this context.
	\end{rmk}
	\begin{rmk}
		Instead of saying ``$T$ is a tensor of rank $\coord{k.l}$'', we may use the less verbose ``$T$ is a $\coord{k,l}$ tensor''.
	\end{rmk}
	\begin{rmk}
		Thus, by the definition of the tensor product (\cref{Tensor}), a tensor of rank $\coord{k,l}$ is `the same as' a multilinear map from $\underbrace{V\times \cdots \times V}_l$ to $\underbrace{V\otimes \cdots \otimes V}_k$.  Thus, a tensor of rank $\coord{k,l}$ is a thing that takes in $l$ vectors and `spits out' `$k$ vectors'\footnote{More accurately, a contravariant tensor of rank $k$.} in a multilinear manner.
	\end{rmk}
	\begin{rmk}
		If $V$ is a finite-dimensional vector space, by virtue of \cref{prp5.4.9}, we have natural isomorphisms like, for example,
		\begin{equation}
			\begin{multlined}
				\Mor _{\Mod{\K}}(V\otimes V^{\T}\otimes V^{\T},V\otimes V^{\T})\cong \\ \Mor _{\Mod{\K}}(V\otimes V,V\otimes V\otimes V).
			\end{multlined}
		\end{equation}
		Thus, it suffices to only look at elements of \eqref{eqn5.4.2}---if a dual were to appear in that morphism set, we could always `move it over' to the other side to get rid of the dual.
		
		\emph{However}, this does rely on the hypothesis that $V$ is finite-dimensional, and in general one will need to look at maps, for example, $V\otimes V^{\T}\rightarrow V^{\T}$.  I suppose we technically should have included this above in the `official' definition, but this would really obfuscate what's going on.  Just be aware that we may need to make this distinction if $V$ is not a finite-dimensional vector space.  For example, we work in this general case when introducing index notation (\cref{IndexNotation}).
	\end{rmk}
	\begin{rmk}
		Using the same sort of isomorphisms referenced in the previous remark, for $V$ a finite-dimensional vector space, there is a natural isomorphism
		\begin{equation}
			\begin{split}
				\tensoralg _l^kV & \ceqq \Mor _{\Mod{\K}}(\underbrace{V\otimes \cdots \otimes V}_l,\underbrace{V\otimes \cdots \otimes V}_k) \\
				& \cong \Mor _{\Mod{\K}}(\underbrace{V\otimes \cdots \otimes V}_l\otimes \underbrace{V^{\T}\otimes \cdots \otimes V^{\T}}_k,\K ).
			\end{split}
		\end{equation}
		Thus, in regards to the question ``But what actually \emph{is} a tensor?'', this says that, for $V$ a finite-dimensional vector space anyways:
		\begin{important}
			A tensor of rank $\coord{k,l}$ is a multilinear map
			\begin{equation}\label{eqn5.4.6}
				\underbrace{V\times \cdots \times V}_l\times \underbrace{V^{\T}\times \cdots \times V^{\T}}_k\rightarrow \K .
			\end{equation}
		\end{important}
	\end{rmk}
	\begin{rmk}
		Recall that (\cref{TensorProduct}) the empty tensor product is defined to be $\K$.  Thus, $\tensoralg _0^0V\cong \K$, $\tensoralg _0^1V\cong V$ (by \cref{prp5.2.24}), and $\tensoralg _1^0V\eqqc V^{\T}$.
	\end{rmk}
	\begin{rmk}
		Note that the notation $\tensoralg _l^kV$ is nonstandard (though based on the standard notation $\Lambda ^lV$ for something new which we will become acquainted with later on).
	\end{rmk}
	\begin{rmk}
		If you still feel uncomfortable with this, you might consider taking a glance at \cref{sbsThePhysicistsDefinition}.  This subsection briefly explains the definition usually taken by physicists, and in particular reproduces (from another source) a precise version of this definition.  I personally find this unenlightening, and it's technically not exactly the same thing as what we discuss, but I have had at least some students find it useful.
	\end{rmk}
\end{dfn}
\begin{thm}{}{thm5.4.6}
	Let $V$ be a finite-dimensional vector space over a field.  Then, the canonical map
	\begin{equation}
		\begin{multlined}
			\underbrace{V\otimes \cdots \otimes V}_k\otimes \underbrace{V^{\T}\otimes \cdots \otimes V^{\T}}_l\rightarrow \\ \Mor _{\Vect}(\underbrace{V\otimes \cdots \otimes V}_l,\underbrace{V\otimes \cdots \otimes V}_k)
		\end{multlined}
	\end{equation}
	is a natural isomorphism.
	\begin{rmk}
		We write
		\begin{equation}
			\begin{split}
				V^{k\otimes ,l\otimes \T} & \ceqq V^{k\otimes}\otimes [V^{\T}]^{l\otimes} \\
				& \ceqq \underbrace{V\otimes \cdots \otimes V}_k\otimes \underbrace{V^{\dagger}\otimes \cdots \otimes V^{\dagger}}_l.
			\end{split}
		\end{equation}\index[notation]{$V^{k\otimes ,l\otimes \T}$}\index[notation]{$V^{k\otimes}$}
		Thus, in brief, we could say that $\bigotimes _l^kV$ and $V^{k\otimes ,l\otimes \T}$ are naturally isomorphic (for $V$ and $W$ finite-dimensional vector spaces).
	\end{rmk}
	\begin{rmk}
		We may, by abuse of language, refer to elements of $V^{k\otimes ,l\otimes \T}$ as \term{tensors}\index{Tensor (of rank $\coord{k,l}$)}, even when this space isn't actually isomorphic, or even contained, in the space of `actual' tensors $\tensoralg _l^kV$.  Given $T\in V^{k\otimes ,l\otimes \T}$, we can always consider it to be an `actual' tensor if need be by considering its image in $\tensoralg _l^kV$ under the map appearing in the statement.
	\end{rmk}
	\begin{rmk}
		Thus, in finite-dimensional vector spaces, \emph{$\coord{k,l}$ tensors are the same as elements in the tensor product of $V$ with itself $k$ times and the tensor product of $V^{\T}$ with itself $l$ times}.
		
		Thus, in this context, we will not make a distinction between these two spaces, and both of them will be referred to as the ``space of tensors of rank $\coord{k,l}$''.
	\end{rmk}
	\begin{proof}
		This follows from combining \cref{prp5.4.9} and \cref{crl5.4.12}.\footnote{These respectively say in particular that $V^{\T}\otimes V\cong \Mor _{\Vect}(V,V)$ and that $V^{\T}\otimes V^{\T}\cong [V\otimes V]^{\T}$.}
	\end{proof}
\end{thm}
\begin{thm}{The tensor algebra}{TensorAlgebra}
	Let $V$ be a $\K$-$\K$-bimodule and define
	\begin{equation}
		\tensoralg _{\bullet}^{\bullet}V\ceqq \bigoplus _{k,l\in \N}\tensoralg _l^kV.
	\end{equation}\index[notation]{$\tensoralg _{\bullet}^{\bullet}$}\index[notation]{$V^{\otimes ,\otimes \T}$} 
	Then, $\tensoralg _{\bullet}^{\bullet}V$ is a $\K$-algebra with multiplication given by the tensor product.
	\begin{rmk}
		Elements of $\tensoralg _{\bullet}^{\bullet}V$ are \term{tensors}\index{Tensor}.
		
		Thus, for example, if $S$ is a tensor of rank $\coord{2,3}$ and $T$ is a tensor of rank $\coord{4,1}$, then $S+T$ now makes sense\footnote{Before introducing $\tensoralg _{\bullet}^{\bullet}V$ this notation would have been nonsense as you can't add things if they don't `live in' the same module!} and is considered a tensor (though it doesn't have a definite rank).
	\end{rmk}
	\begin{rmk}
		$\tensoralg _{\bullet}^{\bullet}$ is the \term{tensor algebra}\index{Tensor algebra} over $V$.
	\end{rmk}
	\begin{rmk}	
		If we ever have the need, then we shall write
		\begin{equation}
			\tensoralg ^{\bullet}V\ceqq \bigoplus _{k\in \N}\tensoralg ^kV
		\end{equation}\index[notation]{$\tensoralg ^{\bullet}V$}
		and
		\begin{equation}
			\tensoralg _{\bullet}V\ceqq \bigoplus _{l\in \N}\tensoralg _lV.
		\end{equation}\index[notation]{$\tensoralg _{\bullet}V$}
		respectively for the subalgebras of contravariant and covariant tensors.
	\end{rmk}
	\begin{rmk}
		If $V$ is a finite-dimensional vector space, so that we have $V^{k\otimes ,l\otimes \T}\cong \tensoralg _l^kV$, we may also write
		\begin{equation}
			V^{\otimes ,\otimes \T}\ceqq \bigoplus _{k,l\in \N}V^{k\otimes ,l\otimes \T}\cong \tensoralg _{\bullet}^{\bullet}V.
		\end{equation}\index[notation]{$V^{\otimes ,\otimes \T}$}
		
		Similarly, in this case, we may also write
		\begin{equation}
			V^{\otimes}\ceqq \bigoplus _{k\in \N}V^{k\otimes}\cong \tensoralg ^{\bullet}V
		\end{equation}\index[notation]{$V^{\otimes}$}
		and
		\begin{equation}
			V^{\otimes \T}\ceqq \bigoplus _{l\in \N}V^{l\otimes \T}\cong \tensoralg _{\bullet}V.
		\end{equation}\index[notation]{$V^{\otimes \T}$}
	\end{rmk}
	\begin{rmk}
		Warning:  This notation is nonstandard.  Usually people only look at the contravariant tensors, in which case the notation $T(V)$ is often used for the space of all contravariant tensors.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

\begin{prp}{Basis for $\tensoralg _{\bullet}^{\bullet}V$}{BasisForTensorAlgebra}
	Let $V$ be a finite-dimensional vector space over a field, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis of $V$, denote the dual-basis by $\basis{B}^{\T}\eqqc \{ b^1,\ldots ,b^d\}$, and let $k,l\in \N$.  Then,
	\begin{equation}
		\begin{multlined}
			\tensoralg _l^k\basis{B}\ceqq \left\{ b_{i_1}\otimes \cdots \otimes b_{i_k}\otimes b^{j_1}\otimes \cdots \otimes b^{j_l}:\right. \\ \left. 1\leq i_1,\ldots ,i_k,j_1,\ldots ,j_n\leq d\right\}
		\end{multlined}
	\end{equation}\index[notation]{$\tensoralg _l^k\basis{B}$}
	is a basis for $\tensoralg _l^kV$.
	\begin{rmk}
		This says that all possible tensor products of $k$ elements from $\basis{B}$ with all possible tensor products of $l$ elements from $\basis{B}^{\T}$ is a basis for $\tensoralg _l^kV$.  Putting these all together for all $k,l\in \N$ then gives a basis for $\tensoralg _{\bullet}^{\bullet}V$.
	\end{rmk}
	\begin{rmk}
		We require that $V$ be a finite-dimensional vector space over a field here so that the ``dual basis'' is actually a basis---see \cref{TheDualBasis}.
	\end{rmk}
	\begin{rmk}
		Note how the indices of the vectors in this case are written as \emph{subscripts}.  This is customary:  if the \emph{name} of a vector involves an index, then that index is written as a subscript; if the \emph{name} of a covector involves an index, then that index is written as a superscript.
		
		The reason for this is simply so that indices that are part of the name of the vector don't risk `colliding' with indices used in the sense of index notation.  For example, while we have not used index notation in this statement, we would have written
		\begin{equation}
			[b_i]^a\text{ and }[b^i]_a
		\end{equation}
		respectively for elements in the basis and elements in the dual basis.
		
		Another reason for doing this is to make equations like\footnote{This is the defining equation for the coordinates of $v$ with respect to the basis $\basis{B}$.}
		\begin{equation}
			v=\sum _{k=1}^dv^k\cdot b_k
		\end{equation}
		to adhere to the convention that identical pairs of indices, one upstairs and one downstairs, are to be summed over.
	\end{rmk}
	\begin{rmk}
		Of course, it follows that the union of all these sets over $k,l\in \N$ then yields a basis for all of $\tensoralg _{\bullet}^{\bullet}V$ (by \cref{prp4.4.45}).
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

\subsection{Index notation}

\subsubsection{The definition}

Index notation is notation commonly used\footnote{At least by physicists.} when working with tensors.  The basic idea is to ``decorate'' the name of the tensor with subscripts and superscripts that tell you the rank of the tensor.  This is analogous to how one may write ``$f(x,y,z)$'' instead of just ``$f$'' to indicate that $f$ is a function of three variables.
\begin{ntn}{Index notation}{IndexNotation}
	Let $V$ be a $\K$-$\K$-bimodule and let
	\begin{equation}\label{eqn5.4.15}
		\begin{multlined}
			T\in \Mor \big( \underbrace{V\otimes \cdots \otimes V}_{k_1}\otimes \underbrace{V^{\T}\otimes \cdots \otimes V^{\T}}_{k_2}, \\ \underbrace{V\otimes \cdots \otimes V}_{l_1}\otimes \underbrace{V^{\T}\otimes \cdots \otimes V^{\T}}_{l_2}\big)
		\end{multlined}
	\end{equation}
	be a tensor of rank $\coord{\coord{k_1,k_2},\coord{l_1,l_2}}$.  To indicate the rank of $T$, we shall write
	\begin{equation}
		\indices{^{a_1\cdots a_{l_1}}_{b_1\cdots b_{l_2}}}\! T\indices{^{c_1\cdots c_{k_2}}_{d_1\cdots d_{k_1}}}.
	\end{equation}
	\begin{rmk}
		\emph{Important}:  If $V$ is a finite-dimensional vector space, then as explained in a remark of the definition of tensors (\cref{Tensor}), we may essentially move all copies of the dual space appearing in \eqref{eqn5.4.15} to the ``other side''.  This results in $k_1\rightarrow k_1+l_2$, $k_2\rightarrow 0$, $l_1\rightarrow l_1+k_2$, and $l_2\rightarrow 0$.  In this case, it is customary to write all the indices to the right of $T$:
		\begin{equation}
			T\indices{^{a_1\cdots a_k}_{b_1\cdots b_l}}
		\end{equation}\index[notation]{$T\indices{^{a_1\cdots a_k}_{b_1\cdots b_l}}$}
		That said, even in this case, we may still sometimes write indices to the left of $T$ if we feel that makes the intended meaning clearer.
		
		The importance of this remark is a result of the fact that the vast majority of the time we will \emph{not} be writing indices on the left.
	\end{rmk}
	\begin{rmk}
		If we are dealing with tensors over two distinct vector spaces, we will tend to use distinct scripts for the different vector spaces.  For example, we will probably write something like $T\indices{^a_{\alpha}}$ for an element of $V\otimes W^{\T}$ instead of $T\indices{^a_b}$.  We might also write capitals for one space, or perhaps start at different place in the same alphabet (e.g.~$\alpha$, $\beta$, $\gamma$, etc.~for $V$ vs.~$\mu$, $\nu$, $\rho$, etc.~for $W$).
	\end{rmk}
	\begin{rmk}
		This is called \term{abstract index notation}\index{Abstract index notation}, \term{Penrose index notation}\index{Penrose index notation}, or just \term{index notation}\index{Index notation}.  This is similar in form, but conceptually distinct, from \emph{Einstein index notation}.  In Einstein index notation, one has chosen a basis, and then the indices indicate the coordinates with respect to that basis.  Note, however, that abstract index notation was designed so that one could do computations as if one was using Einstein index notation without actually picking a basis.  Roughly speaking, abstract index notation is to Einstein index notation as linear-transformations are to matrices, though the distinction matters even less because the notation is designed to work so similarly.  For this reason, the distinction is one that matters much more in theory than it does in practice.
	\end{rmk}
	\begin{rmk}
		\emph{Do not be sloppy by not staggering your indices!}  If you do, you will eventually make a mistake.  For example, later we will be raising and lowering indices.  Suppose I start with $T^{ab}$, I lower to obtain $T_b^a$, and then I raise again to obtain $T^{ba}$---I should obtain the same thing, but in general $T^{ab}\neq T^{ba}$, and so I have made an error.  It may seem obvious to the point of being silly when I point it out like this, but this is a mistake that is easy to make if there is a big long computation in between the raising and lowering (especially if it's more than just $a$ and $b$ floating around).  And of course, you will never have this problem if you stagger:  $T^{ab}$ goes to $T\indices{^a_b}$ goes back to $T^{ab}$.
	\end{rmk}
	\begin{rmk}
		We emphasize that this is all ``coordinate-free''---no need to pick bases---despite what the notation might superficially suggest.
	\end{rmk}
\end{ntn}

Index notation tends to be confusing for students at first, but I claim that it is quite easy.  First of all, a lot of the definitions that follow look exceedingly complicated because of all the indices and ellipses.  Don't be intimidated---the only real complication here is the notation itself.\footnote{Index notation is admittedly pretty terrible when it comes to writing general statements, but when it comes to specific tensors (e.g.~$T\indices{^{ab}_{cde}}$), it's really quite manageable.}

As for some more concrete advice for understanding, as mentioned before, consider the notation $f(x)$ so often used for functions as roughly analogous to index notation for tensors.  The function itself is technically just $f$, but people write ``$f(x,y,z)$'' all the time to denote the function.  Similarly, while the tensor itself is just $T$, it is helpful to use indices to indicate the ``variables'' that $T$ can take in, for example, $T\indices{^a_{bc}}$.\footnote{Just a random example.  Using a general tensor here would obfuscate the simplicity of what's going on, and in any case would be more directly analogous to something like $f(x_1,\ldots ,x_m)$ than it would just $f(x,y,z)$.}   The notation ``$f(x,y,z)$'' tells you that it takes in $3$ ``variables'', whereas the notation ``$f$'' tells you nothing about this.  Of course, this can be useful information which is often more convenient to indicate in this way than saying separately ``$f$ is a function of three variables''.  Similarly for $T\indices{^a_{bc}}$.

There will be more more to say about the relationship between index notation and this analogous notation for functions, but we postpone this discussion until having actually defined the relevant concepts.

\subsubsection{Constructions in index notation}

There are four key constructions involving tensors that we will need, the \emph{transpose}, the \emph{tensor product}, \emph{contraction}, and the \emph{dual-vector}.\footnote{We briefly met the definition ``dual-vector'' back in \cref{NonsingularAndNondegenerate}, but when the pairing in question is a metric (\cref{MetricVectorSpace}), this concept takes on added importance.}

\begin{ntn}{The identity in index notation}{}
	Let $V$ be a $\K$-module.  Then, we write
	\begin{equation}
	\tensor[^a]{[\id _V]}{_b}\eqqc \tensor[^a]{\delta}{_b}.
	\end{equation}\index[notation]{$\tensor[^a]{\delta}{_b}$}
	\begin{rmk}
		The reason we use this notation is because of the \emph{Kronecker delta symbol}.  Strictly speaking, the \term{Kronecker delta symbol}\index{Kronecker delta symbol} is just the identity matrix, but it is given a separate name because of the notation used to denote it in practice:
		\begin{equation}
		\tensor[^i]{\delta}{_j}\ceqq \begin{cases}1 & \text{if }i=j \\ 0 & \text{if }i\neq j.\end{cases}
		\end{equation}\index[notation]{$\tensor[^i]{\delta }{_j}$}
		($\tensor[^a]{\delta}{_b}$ is in abstract index notation, whereas $\tensor[^i]{\delta}{_j}$ as written here is in Einstein index notation.  This is a good example of how the distinction doesn't matter that much in practice.)
	\end{rmk}
\end{ntn}
The transpose we have already done in \cref{Transpose}, and so we simply explain how to write the transpose in index notation.
\begin{ntn}{Transpose in index notation}{TransposeIndex}
	Let $V$ be a $\K$-$\K$-bimodule and let $T\colon V\rightarrow V$ be linear.  Then, the \emph{transpose} of $\tensor[^a]{T}{_b}$ is denoted
	\begin{equation}
		\tensor[^a]{[T^{\T}]}{_b}\eqqc \tensor[_b]{T}{^a}.
	\end{equation}\index[notation]{$_bT^a$}
	\begin{rmk}
		In case $V$ is a finite-dimensional vector space, we also make use of a similar notation for higher rank tensors:  If $T\indices{^{a_1\cdots a_k}_{b_1\cdots b_l}}$ is a tensor of rank $\coord{k,l}$, then its transpose is similarly denoted
		\begin{equation}
			[T^{\T}]\indices{^{a_1\cdots a_k}_{b_1\cdots b_l}}\eqqc T\indices{_{b_1\cdots b_l}^{a_1\cdots a_k}}.\footnote{As $V$ is a finite-dimensional vector space, we can get away with writing all the indices on the left---see the remark in \cref{IndexNotation}.}
		\end{equation}
		We only use this notation for finite-dimensional vector spaces because otherwise the transpose would be, for example, a map
		\begin{equation}
			\Mor ((W_1\otimes W_2)^{\T})\rightarrow \Mor ((V_1\otimes V_2)^{\T}),
		\end{equation}
		whereas the index notation would imply that it is in fact a map
		\begin{equation}
			\Mor (W_1^{\T}\otimes W_2^{\T})\rightarrow \Mor (V_1^{\T}\otimes V_2^{\T}).
		\end{equation}
		These are of course the same if $V$ is a finite-dimensional vector space by \cref{crl5.4.12}.
	\end{rmk}
	\begin{rmk}
		In case we can get away with writing all the indices on the right, this same definition reads
		\begin{equation}
			[T^{\T}]\indices{^a_b}\ceqq T\indices{_b^a}.
		\end{equation}
	\end{rmk}
	\begin{rmk}
		See \eqref{eqn5.2.72}---this equation lists the coordinates of $T^{\T}$, and its relationship with the coordinates of $T$ is superficially exactly as we have defined here.
	\end{rmk}
\end{ntn}
The tensor product we have already done in \cref{TensorProduct}, and so we simply explain the how to write the tensor product in index notation.
\begin{ntn}{Tensor product in index notation}{ntn5.4.4}
	Let $V$ be a $\K$-$\K$-bimodule and let $T_1$ and $T_2$ respectively be tensors over $V$ of rank $\coord{k_1,l_1}$ and $\coord{k_2,l_2}$.  The \emph{tensor product} of $[T_1]\indices{^{a_1\ldots a_{k_1}}_{b_1\ldots b_{l_1}}}\in V^{k_1\otimes ,l_1\otimes \T}$ and $[T_2]\indices{^{a_1\ldots a_{k_2}}_{b_1\ldots b_{l_2}}}\in V^{k_2\otimes ,l_2\otimes \T}$ is denoted
	\begin{equation}
		\begin{multlined}
			[T_1\otimes T_2]\indices{^{a_1\cdots a_{k_1}c_1\cdots c_{k_2}}_{b_1\cdots b_{l_1}d_1\cdots d_{l_2}}}\eqqc \\ [T_1]\indices{^{a_1\ldots a_{k_1}}_{b_1\ldots b_{l_1}}}[T_2]\indices{^{c_1\ldots c_{k_2}}_{d_1\ldots d_{l_2}}}.
		\end{multlined}
	\end{equation}
	\begin{rmk}
		To avoid obfuscating things even further, we omit the definition in case there are indices are the left, but it works exactly as one would expect---that is, as here, you literally just juxtapose them.
	\end{rmk}
	\begin{rmk}
		In particular, if you ever see ``$\otimes$'' used explicitly, this should be taken as an indication that we are \emph{not} using index notation (and so subscripts and superscripts should not be interpreted as such).
	\end{rmk}
	\begin{rmk}
		Note that \emph{everything commutes with everything} in index notation.\footnote{Assuming the ring you're working over is commutative of course.  For example, it is common to use index notation in super-symmetry (and related subjects), in which case you have to be very careful about commuting things as you might have gotten use to.  I once spent 8 hours looking for a minus sign to make everything in 40ish term expression cancel, and it turned out that I had accidentally commuted things without inserting a proper minus sign.  Pro-tip:  Don't do that.}  For example,
		\begin{equation}
			T\indices{^a_b}v^c=v^cT\indices{^a_b}.
		\end{equation}
		The letters keep track of what goes where---you don't need to use the order in which the symbols are written to do the same job.
	\end{rmk}
\end{ntn}
We now turn to \emph{contraction}.
\begin{dfn}{Contraction}{Contraction}
	Let $V$ be a $\K$-module, $\K$ a cring.  Then, for $k,l\in \N$, and $1\leq i\leq k$ and $1\leq j\leq l$, the $\coord{i,j}$ \term{contraction}\index{Contraction} of $\coord{k,l}$ tensors is the unique map $V^{k\otimes ,l\otimes \T}\rightarrow V^{(k-1)\otimes ,(l-1)\otimes \T}$ such that
	\begin{equation}\label{eqn5.4.41}
		\bigotimes _mv_m\otimes \bigotimes _{n}\phi _n\mapsto \pinnerprod{\phi _j|v_i}\bigotimes _{m\neq i}v_m\otimes \bigotimes _{n\neq j}\phi _n.
	\end{equation}
	\begin{rmk}
		Warning:  While this definition makes sense in general, we need $V$ to be a finite-dimensional vector space over a field for $V^{k\otimes ,l\otimes}$ be actually `be' the space of $\coord{k,l}$ tensors---see \cref{thm5.4.6}.
	\end{rmk}
	\begin{rmk}
		We need $\K$ to be commutative otherwise the defining equation \eqref{eqn5.4.41} will not be multilinear, and hence not give a map on the tensor product.
	\end{rmk}
	\begin{rmk}
		Consider $v\otimes \phi \otimes \psi \in V\otimes V^{\T}\otimes V^{\T}$.  Its $\coord{1,1}$ contraction is then $\phi (v)\otimes \psi \in \K \otimes _{\K}V^{\T}$, and \emph{not} $\phi (v)\psi$.  This reason is because, if $\K$ is not commutative, then $\phi (v)\psi$ might not be a \emph{linear}-functional anymore.  This convention is justified by the fact that the tensor product of $0$ spaces is just $\K$ itself---see the remark in \cref{TensorProduct}.
	\end{rmk}
	\begin{rmk}
		If this doesn't yet make sense, don't worry until you've read the upcoming \cref{ntnContraction} and the remarks contained therein.
	\end{rmk}
	\begin{rmk}
		If you want this to work for `actual' tensors (instead of $V^{k\otimes ,l\otimes \T}$, the condition that $V$ be finite-dimensional is fundamental---it is not just a matter of convenience.  For example, we will see later that $\tensor[^a]{T}{_a}$ is the sum over the eigenvalues of $T$, which will be an infinite sum (and so will not make sense in general) if $\dim (V)=\infty$.  On top of that, the definition is given for $V^{k\otimes ,l\otimes \T}$ which requires finite-dimensionality in order to be the `same' as $\tensoralg _l^kV$---see \cref{thm5.4.6}.
	\end{rmk}
\end{dfn}
\begin{ntn}{Contraction in index notation}{ntnContraction}
	Let $V$ be a $\K$-module, $\K$ a cring, let $k,l\in \N$, and $1\leq i\leq k$ and $1\leq j\leq l$.  Then, the $\coord{i,j}$ \emph{contraction} of the tensor $T\indices{^{a_1\ldots a_k}_{b_1\ldots b_l}}\in V^{k\otimes ,l\otimes \T}$ is denoted
	\begin{equation}
		T\indices{^{a_1\ldots a_{i-1}ca_{i+1}\ldots a_k}_{b_1\ldots b_{j-1}cb_{j+1}\ldots b_l}}
	\end{equation}
	\begin{rmk}
		All these indices might make this seem unapproachable, but it's actually quite simple.  Covectors take in vectors and spit out numbers, and so the contraction of a tensor product in its $a_i$ and $b_j$ index is formed by plugging in the $i^{\text{th}}$ vector into the $j^{\text{th}}$ covector, which is denoted by using the same letter for both the $i^{\text{th}}$ index upstairs and the $j^{\text{th}}$ index downstairs.
	\end{rmk}
	\begin{rmk}
		Keep in mind that you can \emph{only} contract upper-indices (contravariant) with lower (covariant) ones.
	\end{rmk}	
	\begin{rmk}
		Note that the letter you use for contraction doesn't matter---it just needs to be the same upstairs as it is downstairs and not conflict with other indices.  This is analogous to the fact that
		\begin{equation}
			\int \dif x\, f(x)=\int \dif t\, f(t):
		\end{equation}
		it doesn't matter whether you use $x$ or $t$, only that the letter in ``$\dif \blank$'' agree with the letter in ``$f(\blank )$''.
	\end{rmk}
	\begin{rmk}
		Note that contraction of indices reduces both the contravariant and covariant rank by $1$.  For this reason, contracted indices are usually ignored when it comes to determine the type of the tensor.  For example, people will say things like ``The left-hand side of the following equation has only an $a$ index upstairs.
		\begin{equation}
			T\indices{^{ab}_b}=v^a\text{''.}
		\end{equation}
		This is analogous to how $\int \dif x\, f(x,y)$ is only a function of one variable.
	\end{rmk}
	\begin{rmk}
		As contraction is so ubiquitous when working with index notation,\footnote{For example, it appears even in expressions which don't actually require contraction to define (such a compositions of linear-transformations (\cref{exm5.4.47})).} index notation is infrequently used outside the context of finite-dimensional vector spaces over fields.  In infinite-dimensions can still be of limited use if you are careful to keep in mind that any contractions you might have written should \emph{not} be interpreted using the definition, but rather, a dictionary (e.g.~$\phi _av^a$ corresponds to $\phi (v)$ and so on).
	\end{rmk}
\end{ntn}
We mentioned some of the following examples before, but let's now do them `officially'.
\begin{exm}{}{}
	\begin{enumerate}
		\item Vectors (written $^av$ or $v^a$) themselves are tensors of type $\coord{1,0}$.
		\item Covectors (or linear functionals) (written $\omega _a$) are of type $\coord{0,1}$.  For $\omega$ a linear functional and $v$ a vector, $\omega (v)$ is written as $\omega _a\tensor[^a]{v}{}$.
		\item The dot product (written temporarily as $g_{ab}$) is an example of a tensor of type $\coord{0,2}$---it takes in two vectors and spits out a number, written $v\cdot w=g_{ab}\tensor[^a]{v}{}\tensor[^b]{w}{}$ or $v\dot w=g_{ab}v^aw^b$.
		\item Linear-transformations (written $\tensor[^a]{T}{_b}$ or $T\indices{^a_b}$) are tensors of type $\coord{1,1}$---it takes in a single vector and spits out another vector (written $\tensor[^a]{v}{}\mapsto \tensor[^a]{T}{_b}\tensor[^b]{v}{}$ or $v^a\mapsto T\indices{^a_b}v^b$).
	\end{enumerate}
	\begin{rmk}
		In accordance with the remark in \cref{IndexNotation}, as we are working over a finite-dimensional vector space, we are free to work with all indices on the right of the tensor.  Here, however, in relevant cases, we use both notations (on the right and on the left) to illustrate the difference.  Indeed, in some cases, I think having on the indices on the left makes things easier to read (e.g.~in $\tensor[^a]{T}{_b}\tensor[^b]{v}{}$), though I don't know if writing indices on the left is worth this small benefit when you can get away with not doing so.
		
		My best guess is that, were one to get used to writing indices on the left, it wouldn't feel weird at all, but the reality is almost no one does this (because it's not necessary in the cases people use index notation), and as such, I personally find writing indices on the left a bit awkward, though I can't levy any strong objective objections to its use.
	\end{rmk}
\end{exm}
\begin{exm}{Linear-functionals in index notation}{}
	Let $V$ be a $\K$-module, $\K$ a cring, let $v^a\in V$, and let $\phi _a\in V^{\T}$.
	
	We can take their tensor product $v^a\phi _b$, which is a tensor of rank $\coord{1,1}$.
	
	There is only one possible contraction of this tensor:   $v^a\phi _a$, which is by definition equal to $\pinnerprod{\phi |v}\ceqq \phi (v)$.    Compare what this would look like in coordinates:  $\phi (v)=\sum _{k=1}^dv^k\phi _k$.
\end{exm}
\begin{exm}{Linear-transformations in index notation}{exm5.4.46}
	Let $V$ and $W$ be $\K$-modules, $\K$ a cring, let $T\indices{^{\alpha}_a}\in V^{\T}\otimes W$,\footnote{Note that this is `the same' as $\Mor _{\Mod{\K}}(V,W)$ in the case of finite-dimensional vector spaces---see \cref{prp5.4.9}.}, and let $v^a\in V$.
	
	We can take their tensor product $T\indices{^{A}_a}v^b$, which is a tensor of rank $\coord{2,1}$.\footnote{Again, we technically haven't defined tensors that `live in' more than module, but it should be clear what is meant.}  There is only one possible contraction of this tensor:\footnote{We cannot contract $A$ and $a$ as they are for different modules (hence the different scripts).  Of course, if it so happens that $W=V$, then we could contract, and one finds that $T\indices{^b_b}v^a=\tr (T)v$.}  $T\indices{^{A}_a}v^a$, which is index notation for $T(v)$---compare \eqref{eqn3.2.30}.\footnote{This equation gives a formula for $Av$, $A$ a matrix and $v$ a column vector:  $[Av]^i=\sum _{j=1}^nA\indices{^i_j}v^j$.}
\end{exm}
\begin{exm}{Composition in index notation}{exm5.4.47}
	Let $U$, $V$, and $W$ be $\K$-modules, $\K$ a cring, and let $S\indices{^A_a}\in U^{\T}\otimes V$ and $T\indices{^{\alpha}_A}\in V^{\T}\otimes W$.\footnote{Again, by \cref{prp5.4.9}, these are respectively the same as $\Mor _{\Mod{\K}}(U,V)$ and $\Mor _{\Mod{\K}}(V,W)$ for finite-dimensional vector spaces.}
	
	We can take their tensor product $T\indices{^{\alpha}_A}S\indices{^B_a}$, which is a tensor of rank $\coord{2,2}$.  There is only one possible contradiction of this  tensor:  $T\indices{^{\alpha}_A}S\indices{^A_a}$, which is index notation for $T\circ S$---compare \eqref{eqn1.1.47}.\footnote{This equation gives a formula for $AB$, $A$ and $B$ matrices:  $[AB]\indices{^i_k}=\sum _{j=1}^nA\indices{^i_j}B\indices{^j_k}$.  In fact, this similarity is exactly the reason we wrote the indices on the matrix the way we did (instead of the perhaps more common $[A]_{ij}$).}
\end{exm}
\begin{exm}{Bilinear forms in index notation}{}
	Let $V$ be a $\K$-modules, $\K$ a cring, let $B\in V^{\T}\otimes V^{\T}$,\footnote{A \emph{bilinear form} is by definition a bilinear map $V\times V\rightarrow \K$---see \cref{BilinearForm}.  By \cref{crl5.4.12}, this is the same as $V^{\T}\otimes V^{\T}$ for finite-dimensional vector spaces.} and let $v^a,w^a\in V$.
	
	We can take their tensor product $v^aB_{bc}w^d$, which is a tensor of rank $\coord{2,2}$.  Contracting in the `obvious' way yields the scalar $v^aB_{ab}w^b$, which is index notation for $B(v,w)$---compare \eqref{eqn5.8.9}.
\end{exm}
We now know enough about index notation to further elaborate on the map $V^{\T}\otimes W\rightarrow \Mor (V,W)$ of \cref{prp5.4.9}.
\begin{exm}{}{exm5.3.57}
	In index notation, this map is written
	\begin{equation}\label{eqn5.3.58}
		\phi _aw^A\mapsto (v^a\mapsto \phi _av^aw^A).
	\end{equation}
	(Recall that in `nonindex' notation this looks like $\phi \otimes w\mapsto (v\mapsto \phi (v)w)$.)
	
	However, we've now seen in \cref{exm5.4.46} that linear-transformations, being $\coord{1,1}$ tensors, are written in index notation as $T\indices{^A_a}$ (in which case $T(v)$ is written as $T\indices{^A_a}v^a$).  Thus, $T\indices{^A_a}$ is the direct analog of $T$ and $v^a\mapsto T\indices{^A_a}v^a$ is the direct analogue of $v\mapsto T(v)$.  So to, the linear-transformation $V\rightarrow W$ defined above in terms of $\phi$ and $w$ in \eqref{eqn5.3.58} in index notation can literally just be denoted $\phi _aw^A$, and it is understood that this is the name of the linear-transformation $v^a\mapsto \phi _av^aw^A$ (just as $T$ is the name of the linear-transformation $V\mapsto T(v)$).  With this understanding, the map above can literally be written as
	\begin{equation}
		\phi _aw^A\mapsto \phi _aw^A.
	\end{equation}
	Writing it this way I would say is technically a bit sloppy, but it should emphasize the point:  whether you consider $\phi _aw^A$ as an element of $V^{\T}\otimes W$ or as a linear-transformation from $V$ to $W$ is a matter of perspective.  If $V$ and $W$ are finite-dimensional vector spaces, this is \emph{literally true} (the map is a natural isomorphism (\cref{prp5.4.9})), but even when this isn't true, you can still think of $\phi _aw^A$ as defining a linear-transformation.\footnote{In infinite-dimensions, it's just not the case that \emph{every} linear-transformation is of this form.  (Over general modules, things are a bit worse in that $\phi _aw^A$ and $\psi _au^A$ can give the same linear-transformation even if $\phi \neq \psi$ and $w\neq u$, but this still doesn't change the fact that $\phi _aw^A$ defines a linear transformation.)}
	
	Okay, so now that we've explained how this map can be written in index notation, let's give a concrete example.
	
	Take $V\ceqq \R ^d$ and $W\ceqq \R ^e$, and let $\phi _a\in V^{\T}$ be a row vector and $w^A\in \R ^e$ be a column vector.  The linear-transformation is then $\phi _aw^A$.  Want to take a wild guess as to what the matrix of this linear-transformation is with respect to the standard basis?  Surprise, surprise, the $\coord{i,j}$ entry is $\phi _iw^j$.
	
	For example, if
	\begin{equation}
		\phi =\begin{bmatrix}1 & 2 & 3\end{bmatrix}\text{ and }v=\begin{bmatrix}4 \\ 5\end{bmatrix},
	\end{equation}
	then
	\begin{equation}
		\coordinates{\phi \otimes w}{\basis{S}_2\leftarrow \basis{S}_3}=\begin{bmatrix}1\cdot 4 & 2\cdot 4 & 3\cdot 4 \\ 1\cdot 5 & 2\cdot 5 & 3\cdot 5\end{bmatrix}=\begin{bmatrix}4 & 8 & 12 \\ 5 & 10 & 15\end{bmatrix},
	\end{equation}
	where $\basis{S}_k$ is the standard basis of $\R ^k$.  Also, note how this is just the matrix product
	\begin{equation}
		v\phi .
	\end{equation}
	
	The conclusion:
	\begin{important}
		Give a row vector $\phi \in [\R ^d]^{\T}$ and a column vector $v\in \R ^e$, then the matrix of the linear-transformation $\phi _av^A$ with respect to the standard basis is just the $e\times d$ matrix given by the matrix product $v\phi$.
	\end{important}
\end{exm}

These examples suggest an important point that is quite helpful for intuition.
\begin{important}
	It can help intuition to think of contracted indices being summed over.
\end{important}
Indeed, it's true generally that, upon picking a basis, contracted indices are summed over when computing with the coordinates of the tensors with respect to that basis.\footnote{We're not going to define coordinates for arbitrary tensors because we don't really have any need.  The idea is no different than it was with linear-transformations and writing it out in detail is not very enlightening with all the ellipses and such.}

For this reason, I tend to think of index contraction as being a sort of dot product.  The justification for this intuition of course comes from the fact that contracted indices are summed over in coordinates.  In particular, I think of $v^a\nabla _af(x)$ as the dot product of $v^a$ with $\nabla _af(x)$, which of course is just the directional derivative.\footnote{This is yet another strength of index notation---it allows one to write more transparent expressions in some cases.  The notation $v^a\nabla _af(x)$ here has the advantage that it looks exceedingly similar to how I would have written this in multivariable calculus ($\vec{v}\cdot \vec{\nabla}f(x)$).  Without index notation, this might be written instead as $\dif f(x)(v)$, which, at least for me, is relatively unintuitive and awkward.}

Let's now take a look at a concrete example of the use of index notation.
\begin{exm}{Associativity in index notation}{exmAssociativity}
	Let $\K$ be a ring and let $A$ be a $\K$-algebra (\cref{KAlgebra}).  From the definition, multiplication $A\times A\rightarrow A$ is bilinear, and so is given by a tensor $m\indices{^a_{bc}}$ of rank $\coord{1,2}$:
	\begin{equation}
		[X\cdot Y]^a=m\indices{^a_{bc}}X^bY^c.
	\end{equation}
	
	For the sake of practice, let's try to determine what the condition of associativity looks like in index notation.\footnote{To be honest, this is a case where using index notation is probably not very helpful (I think I've used this like once), but it is good practice.}  Associativity is `normal' notation is the statement that
	\begin{equation}
		(X\cdot Y)\cdot Z=X\cdot (Y\cdot Z).
	\end{equation}
	In index notation, this becomes
	\begin{equation}
		m\indices{^a_{bc}}[X\cdot Y]^bZ^c=m\indices{^a_{bc}}X^b[Y\cdot Z]^c,
	\end{equation}
	which in turn becomes
	\begin{equation}
		m\indices{^a_{bc}}m\indices{^b_{de}}X^dY^eZ^c=m\indices{^a_{bc}}X^bm\indices{^c_{de}}Y^dZ^e.
	\end{equation}
	Rearranging this and changing names of contracted indices,\footnote{It will be easier to read off the condition on $m\indices{^a_{bc}}$ if the indices on $X$, $Y$, and $Z$ are the same on both sides of the equation.} this becomes
	\begin{equation}
		m\indices{^a_{bc}}m\indices{^b_{de}}X^dY^eZ^c=m\indices{^a_{bd}}m\indices{^b_{ec}}X^dY^eZ^c.
	\end{equation}
	From this, we read off
	\begin{equation}
		m\indices{^a_{bc}}m\indices{^b_{de}}=m\indices{^a_{bd}}m\indices{^b_{ec}},
	\end{equation}
	which is the condition of associativity in index notation.
\end{exm}

\subsection{Metrics}

We need one more ingredient for the purposes of manipulating tensors, namely that of a \emph{metric}.
\begin{dfn}{Metric (on a vector space)}{MetricVectorSpace}
	Let $V$ be a $\K$-$\K$-bimodule.  Then, a \term{metric}\index{Metric (on a vector space)} on $V$ is a symmetric nonsingular pairing $\pinnerprod{\blankdot |\blankdot}\colon V\times V\rightarrow \K$.
	\begin{rmk}
		By definition, $\pinnerprod{\blankdot |\blankdot}$ is \term{symmetric}\index{Symmetric pairing} iff
		\begin{equation}
			\pinnerprod{v_1|v_2}=\pinnerprod{v_2|v_1}
		\end{equation}
		for all $v_1,v_2\in V$.
	\end{rmk}
	\begin{rmk}
		The idea of a notion of a metric on a vector space and a metric on a set (in the context of uniform space theory) have little to nothing to do with each other.\footnote{If you haven't heard of this before, great!  You won't be confused by the potential conflict of terminology.}  It is merely a coincidence of terminology that is so ingrained that I dare not go against it.
		
		The term ``metric'' in this sense of the word should really not be thought of as a sort of distance, but rather as a sort of dot product.  Indeed, you can verify that the dot product is a metric, and furthermore, in a sense that we don't bother to make precise, every positive-definite metric (on a vector space) is equivalent to the usual euclidean dot product.  There is \emph{some} connection with the other notion of metric, however---positive-definite metrics give us norms (the square-root $\pinnerprod{v|v}$), which in turn gives us a metric (in the other sense).
	\end{rmk}
	\begin{rmk}
		Nonsingularity is usually replaced with nondegeneracy.  In finite dimensions, this is equivalent to nonsingularity (\cref{prp5.2.16}).  In infinite dimensions, however, they are not equivalent, and it is nonsingularity that we want (so that we can raise and lower indices---see \cref{ntnMetricVectorSpace}).
	\end{rmk}
\end{dfn}
Let $V$ be a$\K$-$\K$-bimodule and let $\pinnerprod{\blankdot |\blankdot}\colon V\times V\rightarrow \K$ be a metric on $V$.  By definition (\cref{DualPair}), a pairing $\pinnerprod{\blankdot |\blankdot}\colon V\times V\rightarrow \K$ is bilinear, and hence by the defining property of the tensor product (\cref{TensorProduct}), is `the same' as a linear map $V\otimes _{\K}V\rightarrow \K$, that is, a covariant tensor of rank $2$, which in index  notation is denoted $g_{ab}$\index[notation]{$g_{ab}$}.  By (slight) abuse of terminology, $g_{ab}$ itself is referred to as the ``metric''. 
\begin{ntn}{Metric in index notation}{ntnMetricVectorSpace}
	Let $V$ be a finite-dimensional vector space over a field, and let $g_{ab}$ be a metric on $V$.	
	\begin{enumerate}
		\item \label{ntnMetricVectorSpace(i)}In terms of the original pairing, we have
		\begin{equation}
			\pinnerprod{v|w}=v^aw^bg_{ab}.
		\end{equation}
		\item \label{ntnMetricVectorSpace(ii)}The statement that $g_{ab}$ is symmetric is equivalent to the equality
		\begin{equation}
			g_{ab}=g_{ba}.\footnote{This highlights one of the strengths of index notation:  it provides a convenient way to express equality of tensors without having to `plug in' values for everything (as in $\pinnerprod{v_1|v_2}=\pinnerprod{v_2|v_1}$).}
		\end{equation}
		\item \label{ntnMetricVectorSpace(iii)}The rank $2$ contravariant tensor corresponding to the inverse map $V^{\T}\rightarrow V$ is denoted by $g^{ab}$\index[notation]{$g^{ab}$}.  From the definition, we have that
		\begin{subequations}\label{eqnMetricInverse}
			\begin{align}
				g^{cb}g_{ba} & =\delta \indices{^c_a} \\
				g_{cb}g^{ba} & =\delta \indices{_c^a}.
			\end{align}
		\end{subequations}
		\item \label{ntnMetricVectorSpace(iv)}The \emph{dual-vector} of $v^b\in V$ is defined by
		\begin{equation}
			v_a\ceqq g_{ab}v^b
		\end{equation}\index[notation]{$v_a$}
		\item \label{ntnMetricVectorSpace(v)}The \emph{dual-vector} of $\phi _b\in V^{\T}$ with respect to $g_{ab}$ is defined by
		\begin{equation}
			\phi ^a\ceqq g^{ab}\omega _b
		\end{equation}\index[notation]{$\phi ^a$}
	\end{enumerate}
	\begin{rmk}
		Recall that given any dual-pair $V\times W\rightarrow \K$, we obtain maps $V\rightarrow W^{\T}$ and $W\rightarrow V^{\T}$.  In the case of a metric $g_{ab}$ on $V$, by \emph{symmetry}, we have just one map $V\rightarrow V^{\T}$, which, in index notation, looks like
		\begin{equation}
			V\ni v^b\mapsto g_{ab}v^b=g_{ba}v^b\in V^{\T}.
		\end{equation}
		Nonsingularity says that this map has an inverse $V^{\T}\rightarrow V$, which is `the same' (by \cref{prp5.4.9}) as an element of $V\otimes V$---this element is our $g^{ab}$ in \cref{ntnMetricVectorSpace(iii)}.
	\end{rmk}
	\begin{rmk}
		\emph{Important}:  $g_{ab}$ of course starts out as a pairing on $V$, and so we could always talk about $g_{ab}v^aw^b$.  Using our new notation raising and lowering indices, this can be written
		\begin{equation}
			v_aw^a=g_{ab}v^aw^b=v^aw_a.
		\end{equation}
		In particular, $v_aw^a=v^aw_a$---when you dot the `dot product' of two vectors with respect to $g_{ab}$, it doesn't matter which one has an index downstairs vs. upstairs.
	\end{rmk}
	\begin{rmk}
		Thus, using \cref{ntnMetricVectorSpace(iv),ntnMetricVectorSpace(v)}, we may \emph{raise and lower indices}.\footnote{Though we do need a metric to do so.}  Lowering an index corresponds to applying the isomorphism $V\rightarrow V^{\T}$ and raising an index corresponds to applying its inverse $V^{\T}\rightarrow V$.
	\end{rmk}
	\begin{rmk}
		Warning:  Recall that (\cref{TransposeIndex}), in index notation we denoted the transpose $T^{\T}$ by $T\indices{_b^a}$, which potentially conflicts with $g^{ac}g_{bd}T\indices{^d_c}$.  In fact, however, these two agree, but \emph{only if $T^{\T}$ is the transpose with respect to the dual-pair defined by $g_{ab}$ (\cref{Transpose}) not $V^{\T}\times V\rightarrow \K$}.
		
		To see this, note that $T^{\T}$ is defined by the equality
		\begin{equation}
			\pinnerprod{[T^{\T}](w)|v}=\pinnerprod{w|T(v)},
		\end{equation}
		which in index notation reads
		\begin{equation}
			\begin{split}
				g_{ab}[T^{\T}]\indices{^a_c}w^cv^b & =g_{ab}w^aT\indices{^b_c}v^c=T_{ac}w^av^c \\
				& =g_{ab}T\indices{_c^a}w^cv^b,
			\end{split}
		\end{equation}
		from which we read off $[T^{\T}]\indices{^a_c}=T\indices{_c^a}$.
	\end{rmk}
	\begin{rmk}
		In view of \eqref{eqnMetricInverse}, note that
		\begin{equation}
			g\indices{^a_b}=\delta \indices{^a_b}=g\indices{_b^a}.
		\end{equation}
	\end{rmk}
\end{ntn}
It's worth noting that, everything \emph{except} raising and lowering indices we can do without a metric.  To raise and lower indices, we do need that \emph{extra} structure.  In particular, if you pick a different metric, then your meaning of $v_a$ will change even though the metric does not appear explicitly in this notation.

We end this section with a couple of exercises to get practice using index notation.
\begin{exr}{Einstein's Equation}{EinsteinsEquation}
	Let $V$ be a finite-dimensional vector space over a field $\F$ with $\dim (V)\neq 2$ and $\Char (\F )\neq 2$, let $g_{ab}$ be a metric on $V$, and let $R_{ab}$ be two rank $2$ covariant tensor on $V$.  Show that
	\begin{equation}
		R_{ab}-\tfrac{1}{2}R\indices{^x_x}g_{ab}=0
	\end{equation}
	iff $R_{ab}=0$.
	\begin{rmk}
		The above is \term{Einstein's equation}\index{Einstein's equation} in vacuum (in general, there would be a nonzero tensor on the right-hand side proportional to the \emph{stress-energy tensor}).  This equation is the defining equation in general relativity in the sense that space-time is taken to be a Riemannian manifold with metric $g_{ab}$ that satisfies this equation (actually, $R_{ab}$ is defined in terms of $g_{ab}$, but you don't need to use that for this problem).
	\end{rmk}
	\begin{solution}
		$(\Rightarrow )$ Suppose that $R_{ab}-\frac{1}{2}R\indices{^x_x}g_{ab}=0$.  Then,
		\begin{equation}
		R\indices{^a_b}-\tfrac{1}{2}R\indices{^x_x}\delta \indices{^a_b}=0,
		\end{equation}
		and so
		\begin{equation}
		R\indices{^x_x}-\tfrac{1}{2}R\indices{^x_x}\dim (V)=0.
		\end{equation}
		As $\dim (V)\neq 2$, it follows that $R\indices{^x_x}=0$, and hence $R_{ab}=0$ from the original equation.
		
		\blni
		$(\Leftarrow )$ If $R_{ab}=0$, then $R\indices{^x_x}=0$, in which case the result is immediate.
	\end{solution}
\end{exr}
\begin{exr}{Clifford algebra}{CliffordAlgebra}
	Let $V$ be a finite-dimensional vector space over a field $\F$ with $\Char (\F )=0$, let $g_{ab}$ be a metric on $V$, let $A$ be an $\F$-algebra, and let $\gamma \colon V\rightarrow A$ be linear and define
	\begin{equation}
		\gamma _{a_1\cdots a_k}\ceqq \gamma _{[a_1}\cdot \cdots \cdot \gamma _{a_k]}.
	\end{equation}
	Show that if
	\begin{equation}\label{eqnClifford}
		\gamma _a\cdot \gamma _b+\gamma _b\cdot \gamma _a=2g_{ab},
	\end{equation}
	then
	\begin{equation}
	\gamma ^{ab}\cdot \gamma _b=(\dim (V)-1)\gamma ^a.
	\end{equation}
	\begin{rmk}
		Throughout, we have omitted the indices corresponding to $A$.  For example, with these indices put in, \eqref{eqnClifford} would read
		\begin{equation}
			m\indices{^A_{BC}}\gamma \indices{^B_a}\gamma \indices{^C_b}+m\indices{^A_{BC}}\gamma \indices{^B_b}\gamma \indices{^C_a}=2g_{ab}1^A,
		\end{equation}
		where we have used uppercase for the $A$-indices, $m\indices{^A_{BC}}$ is the multiplication tensor of $A$ (confer \cref{exmAssociativity}) and $1^A\in A$ is the multiplicative identity.  We recommend you do not do this problem with the $A$-indices written explicitly.
	\end{rmk}
	\begin{rmk}
		To clarify, for example
		\begin{equation}
			\gamma _{ab}\ceqq \gamma _{[a}\gamma _{b]}\ceqq \tfrac{1}{2}(\gamma _a\cdot \gamma _b-\gamma _b\cdot \gamma _a).
		\end{equation}
	\end{rmk}
	\begin{rmk}
		$A$ is not quite a ``Clifford algebra''.  Additionally, in order for $A$ to be a \emph{Clifford algebra}, the map $V\rightarrow A$ should be ``universal''.  Learning about Clifford algebras however is not the point of this exercises, though of course feel free to look up ``Clifford algebra'' or ``gamma matrices'' if you'd like to know more.
	\end{rmk}
	\begin{rmk}
		We assume that $\Char (\F )=0$ so that the antisymmetrization makes sense.
	\end{rmk}
	\begin{solution}
		First note that from the defining equation
		\begin{equation}
		2\gamma ^a\cdot \gamma _a=\gamma ^a\cdot \gamma _a+\gamma _a\cdot \gamma ^a=2\dim (V),
		\end{equation}
		and hence $\gamma ^a\cdot \gamma _a=\dim (V)$.  Now,
		\begin{equation}
		\begin{split}
		\MoveEqLeft
		\gamma ^{ab}\gamma _b\ceqq \tfrac{1}{2}(\gamma ^a\cdot \gamma ^b\cdot \gamma _b-\gamma ^b\cdot \gamma ^a\cdot \gamma _b)=\tfrac{1}{2}\left( \dim (V)\gamma ^a+\gamma ^a\cdot \gamma ^b\cdot \gamma _b-2g^{ab}\gamma _b\right) =\tfrac{1}{2}\left( \dim (V)\gamma ^a+\dim (V)\gamma ^a-2\gamma ^a\right) =(\dim (V)-1)\gamma ^a.
		\end{split}
		\end{equation}
	\end{solution}
\end{exr}

\subsection{The physicists' definition}\label{sbsThePhysicistsDefinition}

If you've studied physics before, there is a good chance that you encountered a definition of ``tensor'' that doesn't look much at all like the one we've just given.\footnote{If you've never studied physics, then there is no possibility for confusion, and you should feel free to skip this subsubsection.}  Let us try to explain the sense in which these two different definitions are related.

First of all, the ``physicists definition'' I have in mind sounds something like the following.
\begin{important}
	A \emph{tensor} is a multidimensional array of numbers that transforms in a certain way under a certain group action.
\end{important}
Of course, this is not precise, and so the exact definition you find will not be exactly this.  For example, the definition from \cite[pg.~659]{Taylor} reads verbatim:
\begin{important}
	A four-tensor (strictly speaking a four-tensor of rank $2$) is defined as a set of sixteen numbers $T_{\mu \nu}$ (defined for every inertial frame $\mscr{S}$), where the indices $\mu$ and $\nu$ run from $1$ to $4$, which, when formed into a $4\times 4$ matrix $T$, satisfy
	\begin{equation*}
		T'=\Lambda T\tilde{\Lambda}\tag{15.137}
	\end{equation*}
	---a property that exactly parallels Equation (15.132) for three-tensors.
\end{important}
To give you perhaps a better idea of what Taylor means, we reproduce what is essentially the same definition made precise (from \cite[pg.~175]{Gelfand}).\footnote{Incidentally, Gel'fand is by any reasonable standard a fantastic mathematician.  I'm not sure where he got this wacky idea to use bases, but Hermann Weyl would not have been pleased.  (And yes, he most certainly is using ``$\mbf{R}$'' as the name of a vector space.  And yes, he is not staggering (all of) his indices.  Please don't do this.)}
\begin{important}
	Let $\mbf{R}$ be an $n$-dimensional vector space.  We say that a $p$ times covariant and $q$ times contravariant tensor is defined with every basis in $\mbf{R}$ there is associated as set of $n^{p+q}$ numbers $a_{ij\cdots}^{rs\cdots}$ (there are $p$ lower indices and $q$ upper indices) which under change of basis defined by some matrix $\| c\indices{_i^j}\}$ transform according to the rule
	\begin{equation*}
		\tag*{(6)}a'{}_{ij\cdots}^{rs\cdots}=c\indices{_i^{\alpha}}c\indices{_j^{\beta}}\cdots b\indices{_{\sigma}^r}b\indices{_{\tau}^s}\cdots a_{\alpha \beta \cdots}^{\sigma \tau \cdots}
	\end{equation*}
	with $\| b\indices{_i^j}\}$ the transpose of the inverse of $\| c\indices{_i^j}\}$.  The number $p+q$ is called the rank (valence) of the tensor.  The numbers $a_{ij\cdots}^{rs\cdots}$ are called the components of the tensor.
\end{important}

First of all, let's be clear:  \emph{this concept of ``tensor'' is \emph{not} the same as ours}---they are only related.\footnote{Incidentally, don't have your indices run from $1$ to $4$.  I don't know what he was thinking, but in relativity they should really be running from $0$ to $3$.}

In brief, a \emph{representation} of a group $G$ on a vector space $V$ is a group action of $G$ on $V$ by linear operators.  Using this language, a mathematician would say that the physicist's definition is the statement that $T$ lies in a certain representation of the group (in the example from \cite{Taylor}, the Lorentz group).

The relationship between these two definitions then stems from the fact that \emph{the vector space of tensors $V^{k\otimes ,l\otimes \T}$ carries a canonical representation of $\Aut _{\Vect}(V)$}.  From this point of view, the difference between the mathematician's point of view and the physicists is a matter of working in different categories:  for mathematicians, tensors are elements of the \emph{vector space} $V^{k\otimes ,l\otimes \T}$, whereas for physicists tensors are elements of the \emph{representation} (of $\Aut _{\Vect}(V)$) on $V^{k\otimes ,l\otimes \T}$.

One should really keep these concepts distinct in one's mind, however.  For example, one often wants to think of tensors are being in a representation of, say, the group of isometries of $V$ if $V$ comes with a metric.  If in your definition of tensor it is implicit that it already `lives' in a representation of $\Aut _{\Vect}(V)$, it's not clear how one changes the group.  To be honest, most physicists probably don't think about the mathematical formalism enough to realize that this is not clear.  In any case, I would imagine most would not have a problem with it anyways---being absolutely mathematically precise about what sort of object a tensor is (element of vector space or representation) won't affect numerical predictions, so why bother worrying?

\subsection{Summary}

In summary:\footnote{In what follows, unless otherwise stated, we assume we are working over a finite-dimensional vector space over a field.  If we didn't do this, breaking up all the cases would make the summary sufficiently cluttered so as to decrease its utility substantially.}
\begin{enumerate}
	\item A tensor of rank $\coord{k,l}$ is a linear map from
	\begin{equation}
		\underbrace{V\otimes \cdots \otimes V}_k\rightarrow \underbrace{V\otimes \cdots \otimes V}_k
	\end{equation}
	(\cref{Tensor}).
	\item A general tensor is an element of the tensor algebra $\tensoralg _{\bullet}^{\bullet}V\ceqq \bigoplus _{k,l\in \N}\tensoralg _l^kV$ (\cref{TensorAlgebra}).
	\item In finite-dimensional, a tensor of rank $\coord{k,l}$ is the same as an element of $\underbrace{V\otimes \cdots \otimes V}_k\otimes \underbrace{V^{\T}\otimes \cdots \otimes V^{\T}}_l$ (\cref{thm5.4.6}).
	\item The identity linear-transformation is denoted by $\delta \indices{^a_b}$.
	\item The transpose of $T\indices{^a_b}$ is denoted $[T^{\T}]\eqqc T\indices{_a^b}$.
	\item The tensor product of two tensors is denoted simply by juxtaposition (\cref{ntn5.4.4}).
	\item We can contract indices (\cref{ntnContraction}).  This corresponds to `plugging in' a specified vector into  specified covector (\cref{Contraction}).
	\item If we have a metric, we can also raise and lower indices (\cref{ntnMetricVectorSpace}).  This corresponds to applying the isomorphism defined by the metric $V\rightarrow V^{\T}$ and its inverse.
\end{enumerate}

\section{(Anti)symmetric tensors}

We've already encountered the concept of a metric tensor $g_{ab}$, which by definition is symmetric:  $g_{ab}=g_{ba}$.  The subject of this section is to study tensors that have a similar sort of symmetry (or antisymmetry).

Intuitively, a tensor is \emph{symmetric} iff permuting the indices doesn't change the tensor (\emph{antisymmetric} will mean it changes by a sign).  To discuss this permuting, we need to introduce the \emph{symmetric group}.

\subsection{The symmetric group}

\begin{dfn}{Symmetric group}{SymmetricGroup}
	Let $S$ be a set.  Then, the \term{symmetric group}\index{Symmetric group} of $S$ is $\Aut _{\Set}(S)$.
	\begin{rmk}
		In this context, elements of $\Aut _{\Set}(S)$ tend to be referred to as \term{permutations}\index{Permutation} of $S$.
	\end{rmk}
	\begin{rmk}
		$\Aut _{\Set}(S)$ is our fancy-schmancy category-theoretic notation for the set of all bijections $S\rightarrow S$.
	\end{rmk}
\end{dfn}
\begin{dfn}{Cycle notation}{CycleNotation}
	Let $S=\{ 1,\ldots ,m\}$ be a finite set and let $x_1,\ldots ,x_n\in S$ be distinct.  Then, $(x_1\ldots \, x_n)\in \Aut _{\Set}(S)$\index[notation]{$(x_1\ldots \, x_n)$} is the unique bijection that sends $x_k$ to $x_{k+1}$ for $1\leq k\leq n-1$, sends $x_n$ to $x_1$, and fixes everything else.
	\begin{rmk}
		Permutations of the form $(x_1\ldots \, x_n)$ are \term{cycles}\index{Cycle}.
	\end{rmk}
	\begin{rmk}
		The \term{length}\index{Length (of a cycle)} of $(x_1\ldots x_n)$ is $n$.
	\end{rmk}
	\begin{rmk}
		A \term{transposition}\index{Transposition} is a cycle of length $2$.
	\end{rmk}
	\begin{rmk}
		For example, $(325)\in \Aut _{\Set}(\{ 1,2,3,4,5\} )$ is short-hand for the function $\{ 1,2,3,4,5\} \rightarrow \{ 1,2,3,4,5\}$
		\begin{subequations}
			\begin{align}
				1 & \mapsto 1 \\
				2 & \mapsto 5 \\
				3 & \mapsto 2 \\
				4 & \mapsto 4 \\
				5 & \mapsto 3.
			\end{align}
		\end{subequations}
	\end{rmk}
\end{dfn}
To discuss antisymmetry, we're going to need to discuss the \emph{sign} of a permutation.
\begin{thm}{Sign of a permutation}{SignOfAPermutation}
	Let $S$ be a finite set and let $\sigma \in \Aut _{\Set}(S)$.
	\begin{enumerate}
		\item $\sigma$ can be written as a product of transpositions.
		\item If $s_1\cdots s_m=\sigma =t_1\cdots t_n$ with each $s_k$ and $t_k$ a transposition, then $m$ and $n$ have the same parity $\sgn (\sigma )\in \{ 1,-1\}$\index[notation]{$\sgn (\sigma )$}, the \term{sign}\index{Sign (of a permutation)} $\sigma$.\footnote{That is, $m$ is even/odd iff $n$ is even/odd.}
		\item $\sgn \colon \Aut _{\Set}(S)\rightarrow \{ 1,-1\} \cong \Z /2\Z$ is a group homomorphism.
	\end{enumerate}
	\begin{rmk}
		This says that every permutation can be written as a product of transpositions, and furthermore, the number of these transpositions is unique modulo $2$.
	\end{rmk}
	\begin{rmk}
		For $X=\{ 1,\ldots ,m\}$ a finite set and $S=\{ i_1,\ldots ,i_k\} \subseteq S$, write $S^{\comp}\eqqc \{ j_{k+1},\ldots ,j_m\}$ with $j_1<\cdots <j_m$.  Then, we shall write $\sgn (S)\ceqq \sgn (\sigma _S)$\index[notation]{$\sgn (S)$} for the unique permutation $\sigma \colon X\rightarrow X$ such that
		\begin{equation}
			\sigma (x)\ceqq \begin{cases}i_x & \text{if }x\leq k \\ j_x & \text{if }x\geq k+1\text{.}\end{cases}
		\end{equation}
		
		For example, for $S\ceqq \{ 2,4,5\} \subseteq \{ 1,2,3,4,5\}$, $\sigma _S$ sends $1$ to $2$, $2$ to $4$, $3$ to $5$, $4$ to $1$, and $5$ to $3$, that is, $\sigma _S=(124)(35)$, and so
		\begin{equation}
			\sgn (S)=-1.
		\end{equation}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

\subsection{Basic definitions}

We are now able to begin discussing (anti)symmetric tensors themselves.
\begin{dfn}{(Anti)symmetric}{Antisymmetric}
	Let $V$ be a $\K$-$\K$-bimodule and let $T^{a_1\cdots a_k}\in \bigotimes ^kV$.
	\begin{enumerate}
		\item $T^{a_1\cdots a_k}$ is \term{symmetric}\index{Symmetric (tensor)} iff
		\begin{equation}
			T^{a_1\cdots a_k}=T^{a_{\sigma (1)}\cdots a_{\sigma (k)}}
		\end{equation}
		for all $\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\} )$.
		\item $T^{a_1\cdots a_k}$ is \term{antisymmetric}\index{Antiymmetric (tensor)} iff
		\begin{equation}
			T^{a_1\cdots a_k}=\sgn (\sigma )T^{a_{\sigma (1)}\cdots a_{\sigma (k)}}$ for all $\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\}).
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		The condition of being \term{(anti)symmetric} is defined for covariant tensors in an essentially identical manner.  A general tensor is then \term{(anti)symmetric} iff it is (anti)symmetric in both its contravariant and covariant indices.
	\end{rmk}
	\begin{rmk}
		The set of symmetric tensors of rank $\coord{k,l}$ is a subspace of $\tensoralg _l^k$ which is denoted
		\begin{equation}
			\symalg _l^kV.
		\end{equation}\index[notation]{$\symalg _l^kV$}
		
		The set of antisymmetric tensors of rank $\coord{k,l}$ is a subspace of $\tensoralg _l^k$ which is denoted
		\begin{equation}
		\antialg _l^kV.
		\end{equation}\index[notation]{$\antialg _l^kV$}
	\end{rmk}
	\begin{rmk}
		We write
		\begin{equation}
			\symalg ^kV\ceqq \symalg _0^kV
		\end{equation}\index[notation]{$\symalg ^kV$}
		and
		\begin{equation}
			\symalg _lV\ceqq \symalg _l^0V
		\end{equation}\index[notation]{$\symalg _lV$}
		respectively for the spaces of symmetric rank $k$ contravariant tensors and symmetric rank $l$ covariant tensors.
		
		Similarly, we write
		\begin{equation}
			\antialg ^kV\ceqq \antialg _0^kV
		\end{equation}\index[notation]{$\antialg ^kV$}
		and
		\begin{equation}
			\antialg _lV\ceqq \antialg _l^0V
		\end{equation}\index[notation]{$\antialg _lV$}
		respectively for the spaces of antisymmetric rank $k$ contravariant tensors and antisymmetric rank $l$ covariant tensors.
	\end{rmk}
	\begin{rmk}
		As we had with $\tensoralg _l^kV$ (\cref{Tensor}), we have that $\symalg _0^0V\cong \K \cong \antialg _0^0V$, $\symalg _0^1V\cong V\cong \antialg _0^1V$, and $\symalg _1^0V=V^{\T}=\antialg _1^0V$---tensors of these ranks (along with $\symalg _1^1V=\Mor _{\Mod{\K}}(V,V)=\antialg _1^1V$) are vacuously (anti)symmetric.
	\end{rmk}
	\begin{rmk}
		This is nonstandard notation.  First of all, usually people only work with covariant tensors in this context, in which case they denote these respectively by $\operatorname{Sym}^l(V)$\index[notation]{$\operatorname{Sym}^l(V)$} and $\Lambda ^l(V)$\index[notation]{$\Lambda ^l(V)$}.
	\end{rmk}
	\begin{rmk}
		Sometimes people will say \term{totally symmetric}\index{Totally symmetric (tensor)} and \term{totally antisymmetric}\index{Totally antisymmetric (tensor)} for these concepts respectively, presumably to emphasize that one is discussing \emph{all} the indices.
	\end{rmk}
	\begin{rmk}
		Elements of $\antialg _lV$ are sometimes called \term{differential forms}\index{Differential form} or just \term{forms}\index{Form}, for reasons obviously having to do with calculus.  As such, these terms are usually reserved when doing manifold theory, in which case they probably referred not to just a single tensor but a tensor \emph{field}.\footnote{``Field'' in this context intuitively means that you associate a different tensor to each point (e.g.~``vector field'').}
	\end{rmk}
\end{dfn}
The definition of an (anti)symmetric tensor states how the tensor must ``transform'' upon permuting the indices.  As it turns out, it suffices to just check how the indices transform under permutations.
\begin{prp}{}{}
	Let $V$ be a $\K$-$\K$-bimodule and let $T^{a_1\cdots a_k}\in \bigotimes ^kV$.
	\begin{enumerate}
		\item $T^{a_1\cdots a_k}$ is symmetric iff $T^{a_1\cdots a_k}=T^{a_{\sigma (1)}\cdots a_{\sigma (k)}}$ for all transpositions $\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\} )$.
		\item $T^{a_1\cdots a_k}$ is antisymmetric iff $T^{a_1\cdots a_k}=-T^{a_{\sigma (1)}\cdots a_{\sigma (k)}}$ for all transpositions $\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\} )$.
	\end{enumerate}
	\begin{rmk}
		That is, $T^{a_1\cdots a_k}$ is antisymmetric iff switching any two indices gives you the same thing, and it antisymmetric iff switching any two indices gives you the same thing with a minus sign.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

Given an arbitrary tensor, there is a canonical way of `turning it into' an (anti)symmetric one.
\begin{dfn}{(Anti)symmetrization}{Antisymmetrization}
	Let $V$ be a $\K$-$\K$-bimodule with $\Char (\K )=0$, and let $T^{a_1\cdots a_k}\in \bigotimes ^kV$.
	\begin{enumerate}
		\item The \term{symmetrization}\index{Symmetrization} $T^{(a_1\cdots a_k)}$\index[notation]{$T^{(a_1\cdots a_k)}$} of $T^{a_1\cdots a_k}$ is defined by
		\begin{equation}
			T^{(a_1\cdots a_k)}\ceqq \frac{1}{k!}\sum _{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\})}T^{a_{\sigma (1)}\cdots a_{\sigma (k)}}.
		\end{equation}
		The \term{antisymmetrization}\index{Antisymmetrization} $T^{[a_1\cdots a_k]}$\index[notation]{$T^{[a_1\cdots a_k]}$} of $T^{a_1\cdots a_k}$ is defined by
		\begin{equation}
			T^{[a_1\cdots a_k]}\ceqq \frac{1}{k!}\sum _{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\})}\sgn (\sigma )T^{a_{\sigma (1)}\cdots a_{\sigma (k)}}.
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		The (anti)symmetrization of covariant tensors is defined in an essentially identical manner.  You can take the (anti)symmetrization of a tensor with both contravariant and covariant parts, but you do the contravariant and covariant indices separately (so that you might more accurately use the terms ``contravariant (anti)symmetrization'' and ``covariant (anti)symmetrization'').  For example, the expression $T\indices{^{ab}_{(cde)}}$ is perfectly valid, as is $T\indices{^{[abc]}_{(cd)}}$.
	\end{rmk}
	\begin{rmk}
		One can also adapt this to (anti)symmetrization in only some of the indices.  For example, given the above, it should be clear what one means by $T\indices{^{[ab]c}_{(cd)[ef]}}$.\footnote{Here we see another strength of index notation.  To be honest, I'm not sure how one would write this without indices unless you essentially just define it from scratch all over again.  With index notation, however, it's quite simple.}
	\end{rmk}
	\begin{rmk}
		The factor of $\frac{1}{k!}$ is so that we have $T^{(a_1\cdots a_k)}=T^{a_1\cdots a_k}$ iff $T$ is symmetric (and similarly for antisymmetric)---see the following definition.
	\end{rmk}
	\begin{rmk}
		Recall that (\cref{Characteristic}) the characteristic of a ring is the smallest positive integer $m$ such that $m\cdot 1=0$, unless no such $m$ exists, in which case the characteristic is taken to be $0$.  We require $\K$ to be characteristic $0$ here so that we're not dividing by $0$ with our factors of $\frac{1}{k!}$.
	\end{rmk}
\end{dfn}
\begin{exm}{}{}
	For example,
	\begin{equation}
		T_{(ab)}=\frac{1}{2}(T_{ab}+T_{ba})
	\end{equation}
	and
	\begin{equation}
		\begin{multlined}
			T^{[abc]} \\ =\frac{1}{6}(T^{abc}-T^{acb}-T^{bac}+T^{bca}+T^{cab}-T^{cba}).
		\end{multlined}
	\end{equation}
	You figure out the signs here by counting the number of `flips' to go from $\coord{a,b,c}$ to the order of indices for that term.  For example, to go from $\coord{a,b,c}$ to $\coord{a,c,b}$, I need to do just one flip ($b\leftrightarrow c$), and so the sign for the second term is $(-1)^1=-1$.  On the other hand, to go from $\coord{a,b,c}$ to $\coord{b,c,a}$ takes two flips ($a\leftrightarrow c$ and then $b\leftrightarrow c$), and so the sign for the fourth term is $(-1)^2=1$.
\end{exm}
We may now characterize the (anti)symmetric tensors using the concept of (anti)symmetrization.
\begin{prp}{}{}
	Let $V$ be a $\K$-$\K$-bimodule with $\Char (\K )=0$, and let $T^{a_1\cdots a_k}\in \bigotimes ^kV$.
	\begin{enumerate}
		\item $T^{a_1\cdots a_k}$ is symmetric iff
		\begin{equation}
			T^{a_1\cdots a_k}=T^{(a_1\cdots a_k)}.
		\end{equation}
		\item $T^{a_1\cdots a_k}$ is antisymmetric iff
		\begin{equation}
			T^{a_1\cdots a_k}=T^{[a_1\cdots a_k]}.
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		The ``$\Char (\K )=0$'' condition is imposed here only so that the (anti)symmetrizations make sense.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exr}{}{exr5.5.23}
	Let $S^{a_1\cdots a_k}$ and $T_{a_1\cdots a_k}$ be tensors over a $\K$-module, $\K$ a cring with $\Char (\K )=0$.
	\begin{enumerate}
		\item \label{exr5.5.23(i)}Show that if $T$ is symmetric,\footnote{The analogous thing is true for $T$ antisymmetric, but no need for you to prove essentially the same thing twice.} then
		\begin{equation}
		S^{a_1\cdots a_k}T_{a_1\cdots a_k}=S^{(a_1\cdots a_k)}T_{a_1\cdots a_k}.
		\end{equation}
		\item \label{exr5.5.23(ii)}Show that if $S$ is symmetric, then $S^{[a_1\cdots a_k]}=0$.\footnote{Again, an analogous result holds if $S$ is antisymmetric.}
		\item \label{exr5.5.23(iii)}Is the converse of the previous part true?  If so prove it; if not, give a counter-example.
		\item \label{exr5.5.23(iv)}Show that $S^{a_1\cdots a_k}T_{a_1\cdots a_k}=0$ if $S$ is symmetric and $T$ is antisymmetric.
	\end{enumerate}
	\begin{solution}
		\cref{exr5.5.23(i)} Let $\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\} )$.  Then, using the fact that
		\begin{equation}
		S^{a_1\cdots a_k}T_{a_{\sigma (1)}\cdots a_{\sigma (k)}},
		\end{equation}
		we find that
		\begin{equation}
		\begin{split}
		\MoveEqLeft
		S^{a_1\cdots a_k}T_{a_1\cdots a_k}=S^{a_1\cdots a_k}T_{(a_1\cdots a_k)}\ceqq S^{a_1\cdots a_k}\frac{1}{k!}\sum _{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\} )}T_{a_{\sigma (1)}\cdots a_{\sigma (k)}} \\
		& =\frac{1}{k!}\sum _{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\} )}S^{a_{\sigma (1)}\cdots a_{\sigma (k)}}T_{a_1\cdots a_k}\eqqc S^{(a_1\cdots a_k)}T_{a_1\cdots a_k}.
		\end{split}
		\end{equation}
		
		\blni
		\cref{exr5.5.23(ii)}
		\begin{equation}
		\begin{split}
		S^{[a_1\cdots a_k]} & \ceqq \frac{1}{k!}\sum _{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\})}\sgn (\sigma )S^{a_{\sigma (1)}\cdots a_{\sigma (k)}} \\
		& =\frac{1}{k!}\sum _{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k\} )}\sgn (\sigma )S^{a_1\cdots a_k}=\footnote{There are an even number of terms each, half of which are $S^{a_1\cdots a_k}$, the other half of which are $-S^{a_1\cdots a_k}$.}0.
		\end{split}
		\end{equation}
		
		\blni
		\cref{exr5.5.23(iii)} It's not true.  We define a contravariant tensor of rank $3$ $S^{abc}$ over $\R ^2$ by specifying its coordinates with respect to the standard basis.  Take $S^{11k}=0=S^{22k}$ for $k=1,2$ and $S^{12k}=S^{21k}=1$ for $k=1,2$.  This is not symmetric as $S^{112}=0\neq 1=S^{121}$.  On the other hand, $S^{[ijk]}=0$ as, because $i,j,k\in \{ 1,2\}$, we must have that two of them coincide, say $i=j$, in which case
		\begin{equation}
		S^{[ijk]}=S^{[iik]}=-S^{[iik]}=-S^{[ijk]},
		\end{equation}
		whence $S^{[ijk]}=0$.
		
		\blni
		\cref{exr5.5.23(iv)} From the first part, as $S$ is symmetric, $S^{a_1\cdots a_k}T_{a_1\cdots a_k}=S^{a_1\cdots a_k}T_{(a_1\cdots a_k)}$, which vanishes by (the analogue of) the second part.
	\end{solution}
\end{exr}

\subsection{The (anti)symmetric algebras}

We will make it `official' later, but let us tentatively write $\symalg _{\bullet}^{\bullet}V$ and $\antialg _{\bullet}^{\bullet}V$ respectively for the algebra of symmetric and antisymmetric tensors.  We saw in \cref{TensorAlgebra} that all tensors form an algebra $\tensoralg _{\bullet}^{\bullet}V$ with multiplication given by the tensor product.  And while $\symalg _{\bullet}^{\bullet}V$ and $\antialg _{\bullet}^{\bullet}V$ form subspaces of $\tensoralg _{\bullet}^{\bullet}V$, they don't form subalgebras---the tensor product of (anti)symmetric tensors need not be (anti)symmetric.  The following products are solutions to this problem.\footnote{Though note the remark with the ``Warning''!}
\begin{dfn}{(Anti)symmetric product}{AntisymmetricProduct}
	Let $V$ be a $\K$-$\K$-bimodule and let $S^{a_1\cdots a_k}\in \tensoralg ^kV$ and $T^{a_1\cdots a_l}\in \tensoralg ^lV$.
	\begin{enumerate}
		\item The \term{symmetric product}\index{Symmetric product} $[S\vee T]^{a_1\cdots a_{k+l}}$\index[notation]{$S\vee T$} of $S$ and $T$ is defined by
		\begin{equation}
			\begin{multlined}
				[S\vee T]^{a_1\cdots a_{k+l}}\ceqq \\ \sum _{\substack{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k+l\}) \\ \sigma \text{ is a }\coord{k,l}\text{ shuffle.}}}S^{a_{\sigma (1)}\cdots a_{\sigma (k)}}T^{a_{\sigma (k+1)}\cdots a_{\sigma (k+l)}}.
			\end{multlined}
		\end{equation}
		\item The \term{antisymmetric product}\index{Antisymmetric product} $[S\wedge T]^{a_1\cdots a_{k+l}}$\index[notation]{$S\wedge T$} of $S$ and $T$ is defined by
		\begin{equation}
			\begin{multlined}
				[S\wedge T]^{a_1\cdots a_{k+l}}\ceqq \\ \sum _{\substack{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k+l\}) \\ \sigma \text{ is a }\coord{k,l}\text{ shuffle.}}}\sgn (\sigma )S^{a_{\sigma (1)}\cdots a_{\sigma (k)}}T^{a_{\sigma (k+1)}\cdots a_{\sigma (k+l)}}.
			\end{multlined}
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		$\sigma \in \Aut _{\Set}(\{ 1,\ldots ,k+l\} )$ is a \term{$\coord{k,l}$ shuffle} iff
		\begin{equation}
			\sigma (1)<\cdots <\sigma (k)\text{ and }\sigma (k+1)<\cdots <\sigma (k+l).
		\end{equation}
		The term comes from the fact that if you break up a deck of $k+l$ cards into two piles, one with $k$ cards and one with $l$ cards, and you ``shuffle'' them, then the cards will be `mixed up' in such a way so that the first pile of $k$ cards is still `in order' compared to other cards from that pile, and similarly for the second pile of $l$ cards.
		
		Note that the subset of shuffles is \emph{not} a subgroup of the symmetric group---the composition of two shuffles need not be a shuffle.\footnote{And in fact, it had better not be if we're going to shuffle our cards like this!}
	\end{rmk}
	\begin{rmk}
		As with everything else we've done so far, this can be extended to general tensors of mixed rank.
	\end{rmk}
	\begin{rmk}
		The antisymmetric product is more commonly referred to as the \term{wedge product}\index{Wedge product}, simply because that is the symbol used to denote it.
	\end{rmk}
	\begin{rmk}
		While our notation for the antisymmetric product is standard, our notation for the symmetric product is not.  The symmetric product is more commonly denoted by $S\odot T$\index[notation]{$S\odot T$}.
	\end{rmk}
	\begin{rmk}
		Warning:  We'll see in a bit that these do make $\symalg _{\bullet}^{\bullet}V$ and $\antialg _{\bullet}^{\bullet}V$ respectively into algebras, however they are \emph{not} subalgebras of $\tensoralg _{\bullet}^{\bullet}V$.  The reason is simply because the multiplications are not the same.
	\end{rmk}
\end{dfn}
\begin{exm}{}{}
	For example, if $S^a\in \antialg ^1V$ and $T^{bc}\in \antialg ^2V$, then
	\begin{equation}
		\begin{split}
			[S\wedge T]^{abc} & \ceqq \frac{3!}{2!1!}S^{[a}T^{bc]} \\
			& \ceqq \frac{3!}{2!1!}\cdot \frac{1}{3!}\left[ S^aT^{bc}-S^aT^{cb}-S^bT^{ac}\right. \\ & \qquad \left. +S^bT^{ca}+S^cT^{ab}-S^cT^{ba}\right] \\
			& =\frac{1}{2}\left[ S^aT^{bc}+T^aT^{bc}+S^bT^{ca}\right. \\ & \qquad \left. +S^bT^{ca}+S^cT^{ab}+S^cT^{ab}\right] \\
			& =\frac{1}{2}\left[ 2S^aT^{bc}+2S^bT^{ca}+2S^cT^{ab}\right] \\
			& =S^aT^{bc}+S^bT^{ca}+S^cT^{ab}.
		\end{split}
	\end{equation}
\end{exm}
In case $\Char (\K )=0$, this definition can be written in another, arguably simpler, form.
\begin{prp}{}{}
	Let $V$ be a $\K$-$\K$-bimodule with $\Char (\K )=0$, and let $S^{a_1\cdots a_k}\in \tensoralg ^kV$ and $T^{a_1\cdots a_l}\in \tensoralg ^lV$.
	\begin{enumerate}
		\item 
		\begin{equation}
			[S\vee T]^{a_1\cdots a_{k+l}}=\frac{(k+l)!}{k!l!}S^{(a_1\cdots a_k}T^{a_{k+1}\cdots a_{k+l})}.
		\end{equation}
		\item
		\begin{equation}
			[S\wedge T]^{a_1\cdots a_{k+l}}=\frac{(k+l)!}{k!l!}S^{[a_1\cdots a_k}T^{a_{k+1}\cdots a_{k+l}]}.
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		$\Char (\K )=0$ is only needed so that we don't risk dividing by $0$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
A trivial but quite important corollary of the definition is that the antisymmetric product of two vectors vanishes.
\begin{prp}{$\wedge$ is super-commutative}{}
	Let $V$ be $\K$-module, $\K$ a cring, and let $S\in \antialg ^pV$ and $T\in \antialg ^qV$.  Then,
	\begin{equation}
		S\wedge T=(-1)^{pq}T\wedge S.
	\end{equation}
	\begin{rmk}
		In particular, if the rank of $T$ is odd, $T\wedge T=-T\wedge T$, and hence $T\wedge T=0$.\footnote{Actually, if $\Char (\K )=2$, it is still true, but not so immediate (because this rearranges to $2T\wedge T=0$, but in characteristic $2$ we cannot divide by $2$ to obtain $T\wedge T=0$)---see the following result.}
	\end{rmk}
	\begin{rmk}
		Warning:  It is \emph{not} the case that $T\wedge T=0$ all the time (even when $2\in \K$ is a unit).  For this to be the case, the rank of $T$ should be odd.
	\end{rmk}
	\begin{rmk}
		\emph{Commutative} would mean of course that $ST=TS$.  \emph{Anticommutative} would mean that $ST=-TS$.  \term{Super-commutativity}\index{Super-commutativity} on the other hand refers to the fact that two tensors of definite rank\footnote{For example, the sum of a tensor of rank $2$ with a tensor of rank $3$ is still a tensor, but not one of definite rank.} commute or anticommute depending on their ranks.
	\end{rmk}
	\begin{proof}
		\begin{equation}
		S^{[a_1\cdots a_p}T^{a_{p+1}\cdots a_{p+q}]}=(-1)^pS^{[a_{p+1}a_1\cdots a_{p-1}}T^{a_pa_{p+2}\cdots a_{p+q}]}.
		\end{equation}
		In words, to move the index $a_{p+1}$ all the way to the left, we move it past $p$ other indices, picking up $p$ minus signs along the way.  Doing the same for $a_{p+2}$ gives another $p$ minus signs, and so on.  Moving all the indices $a_{p+1},\ldots ,a_{p+q}$ all the way to the left thus generates $p\cdot q$ minus signs, giving
		\begin{equation}
		S^{[a_1\cdots a_p}T^{a_{p+1}\cdots a_{p_q}]}=(-1)^{pq}S^{[a_{p+1}\cdots a_{p+q}}T^{a_1\cdots a_p]}.
		\end{equation}
		As $\K$ is commutative (so that everything commutes in index notation), it follows that $S\wedge T=(-1)^{pq}T\wedge S$.
	\end{proof}
\end{prp}
\begin{prp}{}{}
	Let $V$ be a $\K$-module, $\K$ a cring, and let $T\in \antialg ^kV$.  Then, if $k$ is odd, then $T\wedge T=0$.
	\begin{proof}
		Suppose that $k$ is odd.  Let $\sigma \in \Aut _{\Set}(\{ 1,\ldots ,2k\} )$ and denote by $\breve{\sigma}\in \Aut _{\Set}(\{ 1,\ldots ,2k\} )$ the permutation defined by
		\begin{equation}
			\breve{\sigma}(i)\ceqq (1(k+1))(2(k+2))\cdots ((k-1)(2k-1))(k(2k))\sigma .\footnote{That is, $\breve{\sigma}$ is the product of $\sigma$ by the $k$ transpositions $(i(k+i))$ as $i$ runs from $1$ to $k$.  Intuitively, $\breve{\sigma}$ does to $\{ 1,\ldots ,k\}$ what $\sigma$ did to $\{ k+1,\ldots ,2k\}$ and vice versa.}
		\end{equation}
		Note that $\breve{\breve{\sigma}}=\sigma$ and that $\breve{\sigma}$ is a $\coord{k,k}$ shuffle iff $\sigma$ is.  Furthermore, because $k$ is odd, we have that $\sgn (\breve{\sigma})=-\sgn (\sigma )$.
		
		We now have that
		{\footnotesize
		\begin{equation}
			\begin{split}
				\MoveEqLeft {}
				[T\wedge T]^{a_1\cdots a_{2k}}\ceqq \sum _{\substack{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,2k\} ) \\ \sigma \text{ is a }\coord{k,k}\text{ shuffle.}}}\sgn (\sigma )T^{a_{\sigma (1)}\cdots a_{\sigma (k)}}T^{a_{\sigma (k+1)}\cdots a_{\sigma (2k)}} \\
				& =\sum _{\substack{\sigma \colon \{ 1,\ldots ,k\} \rightarrow \{ 1,\ldots ,2k\} \\ \text{injective.}}}\left[ \sgn (\sigma )T^{a_{\sigma (1)}\cdots a_{\sigma (k)}}T^{a_{\sigma (k+1)}\cdots a_{\sigma (2k)}}+\right. \\ & \qquad \left. \sgn (\breve{\sigma})T^{a_{\breve{\sigma}(1)}\cdots a_{\breve{\sigma}(k)}}T^{a_{\breve{\sigma}(k+1)}\cdots a_{\breve{\sigma}(2k)}}\right] \\
				& =\sum _{\substack{\sigma \colon \{ 1,\ldots ,k\} \rightarrow \{ 1,\ldots ,2k\} \\ \text{injective.}}}\left[ \sgn (\sigma )T^{a_{\sigma (1)}\cdots a_{\sigma (k)}}T^{a_{\sigma (k+1)}\cdots a_{\sigma (2k)}}+\right. \\ & \qquad \left.-\sgn (\sigma )T^{a_{\sigma (k+1)}\cdots a_{\sigma (2k)}}T^{a_{\sigma (1)}\cdots a_{\sigma (k)}}\right] \\
				& =0.
			\end{split}
		\end{equation}
		}
	\end{proof}
	\begin{rmk}
		The basic idea of the proof is that the terms in this sum come in pairs, one for $\sigma$ and one for $\breve{\sigma}$, which by definition of $\breve{\sigma}$, are the same except for possibly $\sgn (\breve{\sigma})$.  As for $k$ odd, $\sgn (\breve{\sigma})=-\sgn (\sigma )$, everything cancels to yield $0$.
	\end{rmk}
\end{prp}
\begin{thm}{The symmetric algebra}{SymmetricAlgebra}
	Let $V$ be a $\K$-$\K$-bimodule and define
	\begin{equation}
		\symalg _{\bullet}^{\bullet}V\ceqq \bigoplus _{k,l\in \N}\symalg _l^kV.
	\end{equation}\index[notation]{$\symalg _{\bullet}^{\bullet}V$} 
	Then, $\symalg _{\bullet}^{\bullet}V$ is a $\K$-algebra with multiplication given by the symmetric product.
	\begin{rmk}
		$\symalg _{\bullet}^{\bullet}V$ is the \term{symmetric algebra}\index{Symmetric algebra} over $V$.
	\end{rmk}
	\begin{rmk}
		If we ever have the need, then we shall write
		\begin{equation}
			\symalg ^{\bullet}V\ceqq \bigoplus _{k\in \N}\symalg ^kV
		\end{equation}\index[notation]{$\symalg ^{\bullet}V$}
		and
		\begin{equation}
			\symalg _{\bullet}V\ceqq \bigoplus _{l\in \N}\symalg _lV
		\end{equation}\index[notation]{$\symalg _{\bullet}V$}
		respectively for the subalgebras of contravariant and covariant symmetric tensors.
	\end{rmk}
	\begin{rmk}
		This notation is nonstandard.  Usually people only look at the contravariant tensors, in which case the notation $S(V)$ is often used for the space of all contravariant tensors.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
\begin{thm}{The antisymmetric algebra}{AntiymmetricAlgebra}
	Let $V$ be a $\K$-$\K$-bimodule and define
	\begin{equation}
		\antialg _{\bullet}^{\bullet}V\ceqq \bigoplus _{k,l\in \N}\antialg _l^kV
	\end{equation}\index[notation]{$\antialg _{\bullet}^{\bullet}V$} 
	Then, $\antialg _{\bullet}^{\bullet}V$ is a $\K$-algebra with multiplication given by the antisymmetric product.
	\begin{rmk}
		$\antialg _{\bullet}^{\bullet}V$ is the \term{antisymmetric algebra}\index{Antisymmetric algebra} over $V$, though it is more commonly referred to as the \term{exterior algebra}\index{Exterior algebra}.\footnote{It is called this because of its relationship with the \emph{exterior derivative}.  The ``exterior derivative'' is in turn referred to as ``exterior'' to contrast it with the notion of \emph{interior derivative}.}
	\end{rmk}
	\begin{rmk}
		If we ever have the need, then we shall write
		\begin{equation}
			\antialg ^{\bullet}V\ceqq \bigoplus _{k\in \N}\antialg ^kV
		\end{equation}\index[notation]{$\antialg ^{\bullet}V$}
		and
		\begin{equation}
		\antialg _{\bullet}V\ceqq \bigoplus _{l\in \N}\antialg _lV
		\end{equation}\index[notation]{$\antialg _{\bullet}V$}
		respectively for the subalgebras of contravariant and covariant antisymmetric tensors.
	\end{rmk}
	\begin{rmk}
		This notation is nonstandard.  Usually people only look at the contravariant tensors, in which case the notation $\Lambda (V)$ is often used for the space of all contravariant tensors.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
We saw above in \cref{BasisForTensorAlgebra} an explicit description of a basis for $\tensoralg _{\bullet}^{\bullet}$ give a basis for $V$.  We now present the analogous results for $\symalg _{\bullet}^{\bullet}$ and $\antialg _{\bullet}^{\bullet}V$.
\begin{prp}{Basis for $\symalg _{\bullet}^{\bullet}V$}{BasisForSymmetricAlgebra}
	Let $V$ be a finite-dimensional vector space over a field, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis of $V$, denote the dual-basis by $\basis{B}^{\T}\eqqc \{ b^1,\ldots ,b^d\}$, and let $k,l\in \N$.  Then,
	\begin{equation}
		\begin{multlined}
			\symalg* _l^k\basis{B}\ceqq \left\{ b_{i_1}\vee \cdots \vee b_{i_k}\vee b^{j_1}\vee \cdots \vee b^{j_l}:\right. \\ \left. 1\leq i_1\leq \cdots \leq i_k\leq d; \right. \\ \left. 1\leq j_1\leq \cdots \leq j_n\leq d\right\}
		\end{multlined}
	\end{equation}\index[notation]{$\symalg _l^k\basis{B}$}
	is a basis for $\symalg _l^kV$.
	\begin{rmk}
		This says that all possible symmetric products of $k$ elements from $\basis{B}$ with nondecreasing indices with all possible symmetric products of $l$ elements from $\basis{B}^{\T}$ with nondecreasing indices is a basis for $\symalg _l^kV$.  Putting these all together for all $k,l\in \N$ then gives a basis for $\symalg _{\bullet}^{\bullet}V$.
		
		The reason we require the indices to be nondecreasing is because, for example, $b_3\vee b_2\vee b_4=b_2\vee b_3\vee b_4$.  Thus, we may always rearrange the elements so that this is the case.
	\end{rmk}
	\begin{rmk}
		Of course, it follows that the union of all these sets over $k,l\in \N$ then yields a basis for all of $\symalg _{\bullet}^{\bullet}V$ (by \cref{prp4.4.45}).
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{prp}{Basis for $\antialg _{\bullet}^{\bullet}V$}{BasisForAntisymmetricAlgebra}
	Let $V$ be a finite-dimensional vector space over a field, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis of $V$, denote the dual-basis by $\basis{B}^{\T}\eqqc \{ b^1,\ldots ,b^d\}$, and let $k,l\in \N$.  Then,
	\begin{equation}
		\begin{multlined}
			\antialg* _l^k\basis{B}\ceqq \left\{ b_{i_1}\wedge \cdots \wedge b_{i_k}\wedge b^{j_1}\wedge \cdots \wedge b^{j_l}:\right. \\ \left. 1\leq i_1<\cdots <i_k\leq d; \right. \\ \left. 1\leq j_1<\cdots <j_n\leq d\right\}
		\end{multlined}
	\end{equation}\index[notation]{$\antialg _l^k\basis{B}$}
	is a basis for $\antialg _l^kV$.
	\begin{rmk}
		This says that all possible antisymmetric products of $k$ elements from $\basis{B}$ with (strictly) increasing indices with all possible antisymmetric products of $l$ elements from $\basis{B}^{\T}$ with (strictly) increasing indices is a basis for $\antialg _l^kV$.  Putting these all together for all $k,l\in \N$ then gives a basis for $\antialg _l^kV$.
		
		Note how this is exactly as it was in the symmetric case except now the indices are \emph{strictly} increasing instead of just nondecreasing.  This essentially follows from the fact that $b_i\wedge b_i=0$, and so all the elements in the above wedge products must be distinct.
	\end{rmk}
	\begin{rmk}
		Of course, it follows that the union of all these sets over $k,l\in \N$ then yields a basis for all of $\antialg _{\bullet}^{\bullet}V$ (by \cref{prp4.4.45}).
	\end{rmk}
	\begin{rmk}
		For the antisymmetric algebra, there is alternative notation that can be used to list the basis elements that can be more transparent.  For $S\subseteq \{ 1,\ldots ,d\}$, define
		\begin{equation}
			b_S\ceqq \bigwedge _{k\in S}b_k,
		\end{equation}
		where the wedge product is taken in order of increasing $i$.  For example,
		\begin{equation}
			b_{\{ 1,3,7\}}\ceqq b_1\wedge b_3\wedge b_7.
		\end{equation}
		Using this notation, we have that
		\begin{equation}
			\antialg* ^k\basis{B}\ceqq \left\{ b_S:S\subseteq \{ 1,\ldots ,d\} ,\abs{S}=k\right\} ,
		\end{equation}
		that is, the basis for $\antialg ^k\basis{B}$ is given by the set of $b_S$ for which $S$ has $k$ elements.\footnote{Of course you can do something similar with $\antialg _l^k\basis{B}$.  We just did the contravariant case because it's less messy (and so hopefully more understandable) to write down.}
	\end{rmk}
	\begin{rmk}
		It follows that
		\begin{equation}
			\dim \left( \antialg _l^kV\right) =\binom{\dim (V)}{k}\binom{\dim (V)}{l},
		\end{equation}
		and in particular that
		\begin{equation}
			\antialg _l^kV=0\text{ if }k>\dim (V)\text{ or }l>\dim (V)\text{.}
		\end{equation}
		Hence,
		\begin{equation}
			\begin{split}
				\dim \left( \antialg _{\bullet}^{\bullet}V\right) & =\sum _{k,l=0}^{\dim (V)}\binom{\dim (V)}{k}\binom{\dim (V)}{l} \\
				& =2^{\dim (V)}\cdot 2^{\dim (V)}=2^{2\dim (V)}.
			\end{split}
		\end{equation}
		Similar results hold for the purely contravariant and covariant cases.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{exm}{}{exm1.6.51}
	Let $V$ be a $3$-dimensional vector space over a field $\F$ with basis $\{ b_1,b_2,b_3\}$.  Then,
	\begin{enumerate}
		\item $\{ 1\}$ is a basis of $\antialg ^0V\cong \F$;
		\item $\{ b_1,b_2,b_3\}$ is a basis of $\antialg ^1V\cong V$;
		\item $\{ b_1\wedge b_2,b_1\wedge b_3,b_2\wedge b_3\}$ is a basis of $\antialg ^2V$;
		\item $\{ b_1\wedge b_2\wedge b_3\}$ is a basis of $\antialg ^3V$;
		\item and $\antialg ^kV=0$ for $k>3$.
	\end{enumerate}
	\begin{rmk}
		Feel free to write out bases for $\tensoralg ^kV$ and $\symalg ^kV$ if you like.  We only refrained from doing so as these are infinite-dimensional in general, which necessitates the use of ellipses, which means an explicit example isn't going to be significantly more readable than the general results (\cref{BasisForTensorAlgebra,BasisForAntisymmetricAlgebra}).
	\end{rmk}
\end{exm}
Note that $\tensoralg _{\bullet}^{\bullet}V$ and $\symalg _{\bullet}^{\bullet}V$ are both infinite-dimensional (unless $V=0$).  For example, if $v\in V$ is nonzero, then
\begin{equation}
	\{ \underbrace{v\vee \cdots \vee v}_k:k\in \Z ^+\}
\end{equation}
is linearly-independent (and similarly for $\otimes$).  This is one place where the antisymmetric differs from the symmetric case, and this difference is one reason why the antisymmetric algebra is more frequently encountered than the symmetric one.

Essentially this difference stems from the fact that $v\wedge v=0$, and so $b_{i_1}\wedge \cdots \wedge b_{i_k}=0$ if any of the $b_{i_j}$s coincide.  This forces all the $b_{i_j}$s to be distinct, and so if the basis is finite with $d$ elements, there are most $\binom{d}{k}$ nonzero elements of that form (and accordingly to the above, these nonzero elements constitute a basis for $\antialg ^{\bullet}V$). 

\section{Extension of scalars}\label{sctExtensionOfScalars}

Because of its intimate connection with the antisymmetric algebra, our next objective is to investigate the \emph{determinant}.  However, before we do so, it will be convenient to first investigate something that is important in its own right, the \emph{algebraic closure} of a vector space,\footnote{This term is nonstandard.  I looked for awhile, but while the concept itself is ubiquitous, I couldn't actually find a name for the concept in general (though for $\R$/$\C$ it's called \emph{complexification}.} which in turn is a specific example of the construction known as \emph{extension of scalars}.

To see the motivation for this, recall the standard example of the matrix that is not diagonalizable.
\begin{equation}
	\begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}.
\end{equation}
In some sense, it \emph{should} be diagonalizable, but of course it can't be (over $\R$)---it doesn't have any eigenvalues!  But we know that really, I mean, come on, this thing is diagonalizable.  Just don't be an idiot and forget about the complex numbers!  They exist, why would you ignore them!?  Believe it or not, the tensor product gives us a tool to `remember' the complex numbers, even if originally we are working over $\R$.

\subsection{The definitions}

\begin{thm}{Extension of scalars (for a module)}{ExtensionOfScalars}
	Let $V$ be a $\K$-module and let $\L$ be a $\K$-algebra.  Then, there is a unique $\K$-linear map $V\rightarrow V^{\L}$ into the $\L$-module $V^{\L}$\index[notation]{$V^{\L}$} such that if $V\rightarrow W$ is any other $\K$-linear map into an $\L$-module $W$, then there is a unique $\L$-linear map $V^{\L}\rightarrow W$ such that the following diagram commutes.
	\begin{equation}
		\begin{tikzcd}
			V \ar[r] \ar[rd] & V^{\L} \ar[d,dashed] \\
			& W
		\end{tikzcd}
	\end{equation}

	Furthermore,
	\begin{enumerate}
		\item explicitly,
		\begin{equation}
			V^{\L}\ceqq \L \otimes _{\K}V;
		\end{equation}
		and
		\item if $\K$ and $\L$ are fields, then $V\rightarrow V^{\L}$ an embedding.
	\end{enumerate}
	\begin{rmk}
		Given a $\K$-module $V$ and a $\K$-algebra $\L$, the passage from the $\K$-module $V$ to the $\L$-module $\L \otimes _{\K}V$ is referred to as \term{extension of scalars}\index{Extension of scalars}.
	\end{rmk}
	\begin{rmk}
		Let $V$ be a vector space over a field $\F$ and let $\A$ be an algebraic closure of $\A$.  In this case, $V^{\A}$ is the \term{algebraic closure}\index{Algebraic closure} of $V$.
		
		Recall that such an $\A$ exists by \cref{AlgebraicClosure}.  In brief, $\A$ (i) contains $\F$, (ii) is algebraically closed (\cref{AlgebraicallyClosed}), and (iii) is algebraic over $\F$ (\cref{AlgebraicKAlgebra}).  The important point is (ii), which means that every nonconstant polynomial with coefficients in $\A$ can be written as a product of linear factors.
		
		As mentioned above, the term ``algebraic closure'' is nonstandard here (this is essentially exclusively reserved for the contexts of fields), but I don't believe there is such a term used for this in general, and so I just made one up.
			
		The exception is the case $\F =\R$, so that we may take $\A =\C$, in which case $V^{\C}$ is referred to as the \term{complexification}\index{Complexification (of a vector space)} of $V$.
	\end{rmk}
	\begin{rmk}
		As in \cref{TensorProduct}, $V^{\L}$ is ``unique up to unique isomorphism''.
	\end{rmk}
	\begin{rmk}
		In the case $\K$ and $\L$ are division rings, as the ``universal'' map $V\rightarrow V^{\L}$ is an embedding, we identify $V$ with its image in $V^{\L}$, so that $V\subseteq V^{\L}$.
	\end{rmk}
	\begin{rmk}
		Being a tensor product, $V^{\L}$ is spanned by elements of the form
		\begin{equation}
			\alpha \otimes v
		\end{equation}
		for $\alpha \in \L$ and $v\in V$.  Intuitively, just think of this as $\alpha \cdot v$, as if $\alpha \cdot v$ made sense all along.  Thus, in a sense, you're just `cheating' by `throwing in' all possible scalings of elements of $V$ by elements in $\L$.  For example, if $V$ is a real vector space and $v\in V$, then $\im \cdot v\in V^{\C}$ and is literally defined to be $\im \cdot v\ceqq \im \otimes v$.
		
		In fact, I would go so far as to say that this concept is so easy, that it's possible you would perform this construction do this construction without even realizing it.  For example, if I tell you that $\coord{1,2,3}\in \R ^3$, and $5$ minutes later you write something down like $\coord{-2\im ,-4\im ,-6\im}$, first of all, shame on you,\footnote{I told you to work over $\R$!} and second of all, you have implicitly just passed from $\R ^3$ to $[\R ^3]^{\C}\cong \C ^3$.
	\end{rmk}
	\begin{rmk}
		Note that we can always regard $V^{\L}$ as a $\K$-module if we like.  In the case of fields, so that $\K \subseteq \L$, this is easily understood:  because $\K \subseteq \L$, we can simply ``forget'' about the scaling by elements in $\L \setminus \K$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
We have an analogous construction for linear-transformations.  We begin with a preliminary result of importance in its own right.
\begin{thm}{Extension of scalars (for a linear-transformation)}{thm5.6.7}
	Let $V$ and $W$ be $\K$-modules, let $\L$ be a $\K$-algebra, and let $T\colon V\rightarrow W$ be a linear-transformation.  Then, there exists a unique linear-transformation $T^{\L}\colon V^{\L}\rightarrow W^{\L}$\index[notation]{$T^{\L}$} such that $T^{\L}(v)=T(v)$ for $v\in V\subseteq V^{\L}$.
	
	Furthermore,
	\begin{enumerate}
		\item \label{thm5.6.7(i)}explicitly,
		\begin{equation}
			T^{\L}(\alpha \otimes v)=\alpha \otimes T(v);\footnote{In other words, $T^{\L}=\id _{\L}\otimes T$.};
		\end{equation}
		and
		\item \label{thm5.6.7(ii)}for any $\L$-module $U$, the map
		\begin{equation}\label{eqn5.6.9}
			\Mor _{\Mod{\K}}(V,U)\ni T\mapsto T^{\L}\Mor _{\Mod{\L}}(V^{\L},U)
		\end{equation}
		is a natural isomorphism of commutative groups.
	\end{enumerate}
	\begin{rmk}
		If $\F$ is a field and $\A$ is an algebraic closure, then $T^{\A}$ is the \term{algebraic closure}\index{Algebraic closure (of a linear-transformation)}.  In case $\F =\R$ and $\A =\C$, $T^{\A}$ is the \term{complexification}\index{Complexification (of a linear-transformation)} of $T$.
	\end{rmk}
	\begin{rmk}
		As in the previous case, in some sense this construction is so easy you might perform it without realizing it.  For example, consider the real matrix
		\begin{equation}
			\begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}.
		\end{equation}
		Now consider it as a complex matrix.  Congratulations!  You've just complexified the above matrix.
		
		Seriously though, complexifying in this case essentially just changes the domain from $\R ^2$ to $\C ^2$.  Superficially, things might seem pretty much the same, but you should know by now that the domain and codomain are integral parts to the definition of any function.
		
		In fact, this works quite generally---see \cref{prp5.6.12}.
	\end{rmk}
	\begin{rmk}
		Warning:  Note how in \eqref{eqn5.6.9} the $^{\L}$ is \emph{not} applied to $U$.  In the proof, to show that this is an isomorphism, we're going to construct an inverse.  It turns out that this inverse will be simply restriction to $V\rightarrow V^{\L}$.  However, if we had used $W^{\L}$ instead of $U$, there is no guarantee that the image of $V$ lies in $W$:  all we would know is that it lies in $W^{\L}$, and so restriction wouldn't actually yield a map $V\rightarrow W$.
	\end{rmk}
	\begin{rmk}
		We met the tensor-hom adjunction in \cref{TensorHomAdjunction}.  It turns out that \eqref{eqn5.6.9} is another example of this fancy thing called an ``adjunction''.  Before, we saw\footnote{Or rather, I told you.} that $U\otimes _S\blank$ is left adjoint to its right adjoint $\Mor _{\Mod{R}}(U,\blank )$, and that $\blank \otimes _SV$ is left adjoint to its right adjoint $\Mor _{\Mod*{T}}(V,\blank )$.  Here, $\blank ^{\L}$ is left adjoint to its right adjoint the \emph{forgetful functor} from $\Mod{L}$ to $\Mod{K}$, that is, the functor that throws away the extra structure of being an $\L$-module and only looks at the $\K$-module structure.
		
		Of course you don't need to know what that means yet.  But hopefully this is getting you all excited ``Oh boy I better dedicate my life to mathematics because, among all the other oodles and oodles of neat-o things there are to learn, I can study adjunctions in the future!.''  Yay!
	\end{rmk}
	\begin{rmk}
		\emph{Important}:  Let $T\colon V\rightarrow V$ be a linear operator on a finite-dimensional vector space $V$ over a field $\F$ and let $\A$ be an algebraic closure of $\F$.  $T$ itself may not have any eigenvalues, but we know from the previous chapter that $T^{\A}$ will.  Thus, this allows us to discuss the `eigenvalues' of $T$ that should be there, even when they aren't:  we will instead just refer to the eigenvalues of $T^{\A}$.
		
		For us, this will be the most important application of extension of scalars.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

\subsection{The case of vector spaces}

In a remark of the previous theorem (\cref{thm5.6.7}), we said that complexifying a matrix amounts to doing nothing to the matrix itself and just changing the domain of the corresponding linear-transformation.  The precise statement of this says something like ``The matrix with respect to a basis in $V$ is the same as the matrix with respect to a basis in $V^{\L}$.''.  To make sense of this, however, we need to know if, given a basis $\basis{B}$ of $V$, is $\basis{B}\subseteq V^{\L}$ also a basis of $V^{\L}$ (regarded as a vector space over $\L$).  The answer, as it turns out, is ``Yes.''.
\begin{prp}{}{}
	Let $\K$ and $\L$ be division rings with $\K \subseteq \L$, and let $V$ be a vector space over $\K$.
	\begin{enumerate}
		\item Let $\basis{B}\subseteq V$ be a basis.  Then, $\basis{B}\subseteq V^{\L}$ is a basis when $V^{\L}$ is regarded as a vector space over $\L$.
		\item Let $\basis{B}\subseteq V$ be a basis and let $\basis{C}\subseteq \L$ be a basis when $\L$ is regarded as a vector space over $\K$.  Then,
		\begin{equation}
			\left\{ \alpha \otimes b:\alpha \in \basis{C},b\in \basis{B}\right\} \subseteq V^{\L}
		\end{equation}
		is a basis when $V^{\L}$ is regarded as a vector space over $\L$.
	\end{enumerate}
	\begin{rmk}
		In particular,
		\begin{align}
			\dim _{\L}(V^{\L}) & =\dim _{\K}(V) \\
			\dim _{\K}(V^{\L}) & =\dim _{\K}(V)\dim _{\K}(\L ).
		\end{align}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			
		\end{exr}
	\end{proof}
\end{prp}
\begin{exm}{}{}
	Let $V$ be a $d$-dimensional vector space over $\R$ with basis $\{ b_1,\ldots ,b_d\}$.
	
	Note that $\C$ is a $2$-dimensional vector space over $\R$ with a basis given by $\{ 1,\im \} \subseteq \C$.
	
	Then, $V^{\C}$ is a $d$-dimensional vector space over $\C$ with basis $\{ b_1,\ldots ,b_d\}$ and is a $2d$-dimensional vector space over $\R$ with basis
	\begin{equation}
		\{ b_1,\im b_1,\ldots ,b_d,\im b_d\} .
	\end{equation}
\end{exm}
\begin{prp}{}{prp5.6.12}
	Let $\K$ and $\L$ be division rings with $\K \subseteq \L$, let $V$ and $W$ be finite-dimensional vector spaces over $\K$ with respective bases $\basis{B}$ and $\basis{C}$, and let $T\colon V\rightarrow W$ be a linear-transformation.  Then,
	\begin{equation}
		\coordinates{T^{\L}}{\basis{C}\leftarrow \basis{B}}=\coordinates{T^}{\basis{C}\leftarrow \basis{B}}.
	\end{equation}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
In fact, this is just one way in which things ``don't change'' when passing from $\K$ to $\L$.  Another is the minimal polynomial.
\begin{prp}{}{}
	Let $\K$ and $\L$ be division rings with $\K \subseteq \L$, let $V$ be a finite-dimensional vector space over $\K$, and let $T\colon V\rightarrow V$ be linear.  Then,
	\begin{equation}
		p_{\min ,T^{\L}}=p_{\min ,T}.
	\end{equation}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
This has a satisfying corollary.
\begin{crl}{}{}
	Let $\K$ and $\L$ be division rings with $\K \subseteq \L$, let $V$ be a finite-dimensional vector space over $\K$, and let $T\colon V\rightarrow V$ be linear.  Then,
	\begin{equation}
		\Eig (T)=\Eig (T^{\L})\cap \K.
	\end{equation}
	\begin{proof}
		From \cref{thm4.5.9}, we know that $\lambda \in \K$ is an eigenvalue of $T$ iff $p_{\min ,T}(\lambda )=0=p_{\min ,T^{\L}}$ iff $\lambda$ is an eigenvalue of $T^{\L}$.
	\end{proof}
\end{crl}

\horizontalrule

Before moving on, you might want to take a look at \cref{semisimple_linear_operator} in the appendix.  It requires a bit too much background to place here, but the result gives a characterization of what it means for $T^{\A}$ to be diagonalizable.  Thus, we can make a distinction between things which are not diagonalizable only because there aren't enough eigenvalues, like the matrix
\begin{equation}
	\begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix},
\end{equation}
and things which are fundamentally not diagonalizable, like nonzero nilpotent operators.

\section{The determinant}

Related to the determinant is the \emph{trace} of a linear operator.  As it will be used when we discuss the determinant and characteristic polynomial, we investigate it first.

\subsection{The trace}

Given a matrix $A$, it turns out that the trace is going to be the sum of the diagonal elements, $\sum _{k=1}^dA\indices{^k_k}$.  That \emph{could} work, but\textellipsis matrices, ew.  Fortunately, this suggests that we could instead define the trace of a linear-transformation $T$ to be $T\indices{^a_a}$, and if your intuition for index notation is as it should be by now, you'll find it obvious that this agrees with the previous notion (upon picking a basis to obtain a matrix) but is ``coordinate-independent'' and doesn't involve matrices.  This is basically perfect, except for the fact that we've only defined contraction (\cref{Contraction}) when the ground ring is a field.
\begin{dfn}{Trace}{Trace}
	Let $V$ be a finite-dimensional vector space over a field and let $T\colon V\rightarrow V$ be linear.  Then, the \term{trace}\index{Trace} of $T$, $\tr (T)$\index[notation]{$\tr (T)$}, is defined by
	\begin{equation}
		\tr (T)\ceqq T\indices{^a_a}.
	\end{equation}
	\begin{rmk}
		As a matter of fact, the same is true if instead $V$ is a finite rank $\K$-module (\cref{FreeModule}).  The reason we didn't state it `officially' this way is because we are implicitly using the result \cref{thm5.4.6} so that we may identify $T$ as an element of $V\otimes V^{\T}$, so that in turn the contraction (\cref{Contraction}) is defined.  The conclusion of \cref{thm5.4.6} should still remain true of $V$ is a finite rank $\K$-module, but for reasons of keeping the exposition `clean', we didn't state this.
		
		In fact, throughout the entirety of these notes, when it came to anything that required finite-dimensionality of any sort, we always just worked over vector spaces, the logic being that one can only define the dimension for vector spaces.  That's not quite true, however, in that you can define the ``dimension'' of \emph{some} modules, even those not over division rings, though in this context it's not usually called ``dimension'' but rather \emph{rank}.  In any case, some of these instances, we could have gotten away with working with ``finite-rank modules'' instead of ``finite-dimensional vector spaces'', but in order to avoid complicating the exposition, we just didn't consider those cases.
		
		Here we do, at least tangentially in this remark, so that you are aware one can define the trace over any ring, not just over fields or division rings.
		
		Throughout this section, it should be assumed that things generalize to finite-rank modules unless otherwise indicated.
	\end{rmk}	
\end{dfn}
\begin{thm}{$\tr (T)=\text{sum of eigenvalues}$}{}
	Let $V$ be a finite-dimensional vector space over a field $\F$ with algebraic closure $\A$ and let $T\colon V\rightarrow V$ be linear.  Then,
	\begin{equation}
		\tr (T)=\sum _{\lambda \in \Eig (T^{\A})}\dim (\Eig _{\lambda}^{\infty})\lambda .
	\end{equation}
	\begin{rmk}
		The dimension factor is there to take into account the ``multiplicity'' of the eigenvalue.  For example, on a three-dimensional vector space, $\tr (2\id )=2+2+2=6$, not just $2$.
	\end{rmk}
	\begin{rmk}
		As a linear-transformation on a finite-dimensional vector spaces has finitely many eigenvalues, this sum is finite and there are no worries of convergence.
	\end{rmk}
	\begin{rmk}
		We actually met the trace briefly before as an example of a linear-functional---see \cref{TheTrace}.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
We finish this subsection with a couple of important properties of the trace.
\begin{prp}{}{prp5.7.6}
	Let $V$ be a finite-dimensional vector space over a field and let $S,T\colon V\rightarrow V$ be linear.
	\begin{enumerate}
		\item \label{prp5.7.6(i)}$\tr (S+T)=\tr (S)+\tr (T)$.
		\item \label{prp5.7.6(ii)}$\tr (S\circ T)=\tr (T\circ S)$.
	\end{enumerate}
	\begin{rmk}
		Warning:  \cref{prp5.7.6(ii)} does \emph{not} imply that $\tr (R\circ S\circ T)=\tr (S\circ R\circ T)$.  On the other hand, it \emph{does} say that
		\begin{equation}
			\tr (R\circ S\circ T)=\tr (S\circ T\circ R)=\tr (T\circ R\circ S).
		\end{equation}
		That is, inside a trace, you can ``\emph{cyclically}'' permute the operators, but you can't permute them any way you like.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}

Having seen the trace, we return to the determinant.

\subsection{The determinant}

Let $V$ be a finite-dimensional vector space over a field and let $T\colon V\rightarrow V$ be linear.  We may then take the tensor product of $T$ with itself:  $T\otimes T\in \End (V)\otimes \End (V)\cong \End (V\otimes V,V\otimes V)$, where we have used the natural isomorphism of \cref{TensorProductLinearTransformation}.  In index notation, this is written $T\indices{^a_b}T\indices{^c_d}$ and is defined in the obvious way:  $v^bw^d\in V\otimes V\mapsto T\indices{^a_b}T\indices{^c_d}v^bv^d\in V\otimes V$.  Looking at $T\otimes T\otimes T$, etc., we see that we obtain linear-transformations $\tensoralg ^kT\colon \tensoralg ^kV\rightarrow \tensoralg ^kV$ for all $k\in \N$.\footnote{Recall that this was originally defined in \cref{TensorProductLinearTransformationx}.}

What about for the symmetric and antisymmetric tensors?  Do we have $\symalg ^kT$ and $\antialg ^kT$?  The answer is of course ``Yes.''.
\begin{thm}{$\symalg ^kT$ and $\antialg ^kT$}{}
	Let $V$ and $W$  $\K$-$\K$-bimodules, let $T\colon V\rightarrow W$ be linear, and let $k\in \N$.
	\begin{enumerate}
		\item There is a unique linear-transformation $\symalg ^kT\colon \symalg ^kV\rightarrow \symalg ^kW$\index[notation]{$\symalg ^kT$} such that
		\begin{equation}
			v_1\vee \cdots \vee v_k\mapsto T(v_1)\vee \cdots \vee T(v_k).
		\end{equation}
		\item There is a unique linear-transformation $\antialg ^kT\colon \antialg ^kV\rightarrow \symalg ^kW$\index[notation]{$\antialg ^kT$} such that
		\begin{equation}
			v_1\wedge \cdots \wedge v_k\mapsto T(v_1)\wedge \cdots \wedge T(v_k).
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		If it's clear from context, it's possible I may abuse notation and simply write ``$T$'' for ``$\symalg ^kT$'' or ``$\antialg ^kT$''.  For example, I can unambiguously write something like
		\begin{equation}
			T(v_1\wedge v_2\wedge v_2)\ceqq T(v_1)\wedge T(v_2)\wedge T(v_3),
		\end{equation}
		for $v_1,v_2,v_3\in V$, as it is clear from context that this must in fact be $\antialg ^3T$.
	\end{rmk}
	\begin{rmk}
		Warning:  This can be also be defined for covariant rank,\footnote{$T^{\T}$ will be used in this case, e.g.~$\phi \wedge \psi \mapsto T^{\T}(\phi )\wedge T^{\T}(\psi )$.} but \emph{not} mixed rank.  The reason is that $\antialg _lT$ will be a map from $\antialg _lW$ to $\antialg _lV$, \emph{not} a map from $\antialg _lV$ to $\antialg _lW$.  I suppose one could do mixed rank in the case $V=W$, but that's a bit awkward.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

Of course, $\tensoralg ^0T=\symalg ^0T=\antialg ^0T=\id _V$ and $\tensoralg ^1T=\symalg ^1T=\antialg ^1T=T$.  If $V$ is finite-dimensional, however, something special happens with $\antialg ^{\dim (V)}T$.  See, $\antialg ^{\dim (V)}V$ is $\binom{\dim (V)}{\dim (V)}=1$ dimensional vector space, which means that linear-transformations $\antialg ^{\dim (V)}V\rightarrow \antialg ^{\dim (V)}V$ are given by scalars.\footnote{A choice of basis associates to every such linear-transformation a $1\times 1$ matrix, essentially just a scalar.  Furthermore, this scalar is independent of the choice of basis (why?).}  This scalar has a name:  it is the \emph{determinant}.
\begin{thm}{Determinant}{Determinant}
	Let $V$ be a finite-dimensional vector space over a field $\F$ and let $T\colon V\rightarrow V$ be linear.  Then, there is a unique scalar $\det (T)\in \F$\index[notation]{$\det (T)$}, the \term{determinant}\index{Determinant} of $T$, such that
	\begin{equation}
		\antialg* ^dT=\det (T)\id ,
	\end{equation}
	where $d\ceqq \dim (V)$ and $\id$ is the identity on $\antialg ^dV$.
	\begin{rmk}
		As with the trace, we can use essentially the same definition here to define the determinant over any ring, but as this would involve discussion of finite rank modules, we refrained from stating this case `officially' to not clutter the exposition with separate cases.  See the remark in the definition of the trace (\cref{Trace}) for an elaboration on this.
	\end{rmk}
	\begin{rmk}
		Some authors write $|A|$ to denote the determinant of $A$.  We shall not make use of this notation.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
It's quite likely that you've seen the determinant before, and that this doesn't at all look like the definition you've seen.  Our first objective then is to show that this agrees with the definition (or a definition---there are many) you've seen before.  Before we start, however, let's see an example of how one can actually compute the determinant using this definition.
\begin{exm}{}{}
	Let $T\colon \R ^3\rightarrow \R ^3$ be the unique linear-transformation such that
	\begin{subequations}
		\begin{align}
			T(e_1) & =e_1+e_2+e_3 \\
			T(e_2) & =e_1+2e_2+3e_3 \\
			T(e_3) & =e_1-e_2+e_3,
		\end{align}
	\end{subequations}
	where $\basis{S}\ceqq \{ e_1,e_2,e_3\} \subseteq \R ^3$ is the standard basis.
	\begin{rmk}
		As a check of understanding, make sure you can immediately read off
		\begin{equation}
			\coordinates{T}{\basis{S}}=\begin{bmatrix}1 & 1 & 1 \\ 1 & 2 & -1 \\ 1 & 3 & 1\end{bmatrix}.
		\end{equation}
		(See \eqref{eqnCoordinatesLinearTransformation} if you've forgotten.)
	\end{rmk}

	$\{e_1\wedge e_2\wedge e_3\}$ is a basis for $\antialg ^3\R ^3$ (see \cref{exm1.6.51} for a slight elaboration), and so we compute
	\begin{equation}
		\begin{split}
			\MoveEqLeft {}
			[\antialg* ^3T](e_1\wedge e_2\wedge e_3)\ceqq T(e_1)\wedge T(e_2)\wedge T(e_3) \\
			& =(e_1+e_2+e_3)\wedge (e_1+2e_2+3e_3)\wedge (e_1-e_2+e_3) \\
			& =\footnote{Here I am using the fact that $v\wedge v=0$ for vectors $v\in \R ^3$.}(2e_1\wedge e_2+3e_1\wedge e_3+e_2\wedge e_1+3e_2\wedge e_3 \\ & \qquad +e_3\wedge e_1+2e_3\wedge e_2)\wedge (e_1-e_2+e_3) \\
			& =\footnote{And here I am using the fact that $v\wedge w=-w\wedge v$ for vectors $v,w\in \R ^3$.}(e_1\wedge e_2+2e_1\wedge e_3+e_2\wedge e_3)\wedge (e_1-e_2+e_3) \\
			& =e_1\wedge e_2\wedge e_3-2e_1\wedge e_3\wedge e_2+e_2\wedge e_3\wedge e_1 \\
			& =e_1\wedge e_2\wedge e_3+2e_1\wedge e_2\wedge e_3+e_1\wedge e_2\wedge e_3 \\
			& =4e_1\wedge e_2\wedge e_3.
		\end{split}
	\end{equation}
	Hence, it follows from the definition that
	\begin{equation}
		\det (T)=4.
	\end{equation}
\end{exm}

\subsubsection{Properties of the determinant}

Before we see the equivalence of this definition to others you have more likely seen before, we will need to investigate some properties of the determinant.

One of the first properties of the determinant that we're going to be interested in is its relationship with composition of linear maps.  The answer (\cref{prp5.7.17}) will following immediately from ``functoriality'' of $\antialg ^k\blank$.
\begin{thm}{$\symalg ^k\blank$ and $\antialg ^k\blank$ are functors}{thm5.7.17}
	Let $U$, $V$, and $W$ be $\K$-$\K$-bimodules, let $S\colon U\rightarrow V$ and $T\colon V\rightarrow W$ be linear, and let $k\in \N$.
	\begin{enumerate}
		\item \label{thm5.7.17(i)}
		\begin{equation}
		\symalg* ^k[T\circ S]=\symalg* ^kT\circ \symalg* ^kS.
		\end{equation}
		\item \label{thm5.7.17(ii)}
		\begin{equation}
		\symalg* ^k\id _V=\id _{\symalg* ^kV}.
		\end{equation}
		\item \label{thm5.7.17(iii)}
		\begin{equation}
		\antialg* ^k[T\circ S]=\antialg* ^kT\circ \antialg* ^lS.
		\end{equation}
		\item \label{thm5.7.17(iv)}
		\begin{equation}
		\antialg* ^k\id _V=\id _{\antialg* ^kV}.
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		The covariant versions $\symalg _l\blank$ and $\antialg _l\blank$ are \emph{co}functors (so the same result is true, except the order of composition is reversed on the right-hand side).
	\end{rmk}
	\begin{proof}
		\cref{thm5.7.17(i)} Let $u_1,\ldots ,u_k\in U$.  Then,
		\begin{equation}
			\begin{split}
			\MoveEqLeft
			\left[ \symalg* ^k[T\circ S]\right] (u_1\vee \cdots \vee u_k) \\
			& \ceqq [T\circ S](u_1)\vee \cdots \vee [T\circ S](u_k) \\
			& \ceqq T(S(u_1))\vee \cdots \vee T(S(u_k)) \\
			& \eqqc [\symalg* ^kT]\left( S(u_1)\vee \cdots \vee S(u_k)\right) \\
			& \eqqc [\symalg* kT]\left( [\symalg* ^kS](u_1\vee \cdots \vee u_k)\right) \\
			& \eqqc [\symalg* ^kT\circ \symalg* ^kS](u_1\vee \cdots u_k).
			\end{split}
		\end{equation}
		
		\blni
		\cref{thm5.7.17(ii)} Let $v_1,\ldots ,v_k\in V$.  Then,
		\begin{equation}
			\begin{split}
				\MoveEqLeft {}
				[\symalg* ^k\id _V](v_1\vee \cdots \vee v_k)\ceqq \id _v(v_1)\vee \cdots \vee \id _V(v_k) \\
				& \ceqq v_1\vee \cdots \vee v_k \\
				& \eqqc \id _{\symalg* ^kV}(v_1\vee \cdots \vee v_k).
			\end{split}
		\end{equation}
		
		\blni
		\cref{thm5.7.17(iii)} Essentially the same as \cref{thm5.7.17(i)}.
		
		\blni
		\cref{thm5.7.17(iv)} Essentially the same as \cref{thm5.7.17(ii)}.
	\end{proof}
\end{thm}
\begin{prp}{}{prp5.7.17}
	Let $V$ be a finite-dimensional vector space over a field $\F$ and let $S,T\colon V\rightarrow V$ be linear.  Then,
	\begin{equation}
	\det (S\circ T)=\det (S)\det (T).
	\end{equation}
	\begin{proof}
		Write $d\ceqq \dim (V)$.  Then,
		\begin{equation}
			\begin{split}
				\det (S\circ T)\id & =\antialg* ^d[T\circ S]=\footnote{By the previous result.}\antialg* ^dT\circ \antialg* ^dS \\
				& =[\det (T)\id ]\circ [\det (S)\id ]=\det (T)\det (S)\id .
			\end{split}
		\end{equation}
		Hence, $\det (S\circ T)=\det (S)\det (T)$.
	\end{proof}
\end{prp}
We now turn to show that this definition is equivalent to any definition you likely have seen before.  There are a lot of equivalent definitions given, and we start with the one that does not require a reduction to the case of matrices.
\begin{prp}{}{}
	Let $S\oplus T\colon U\oplus V\rightarrow U\oplus V$ be a linear-transformation on a finite-dimensional vector space.  Then,
	\begin{equation}
		\det (S\oplus T)=\det (S)\det (T).
	\end{equation}
	\begin{rmk}
		As mentioned in the remark of \cref{Determinant}, most results hold here over finite-rank modules as well.  This is one of them.  Again, unless otherwise specified, this should be assumed throughout, and we will not provide a reminder again.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{thm}{$\det =\text{product of eigenvalues}$}{thm5.7.28}
	Let $V$ be a finite-dimensional vector space over a field $\F$ with algebraic closure $\A$ and let $T\colon V\rightarrow V$ be linear.  Then,
	\begin{equation}
		\det (T)=\prod _{\lambda \in \Eig (T^{\A})}\lambda ^{d_{\lambda}},
	\end{equation}
	where $d_{\lambda}\ceqq \dim (\Eig _{\lambda ,T^{\A}}^{\infty})$.
	\begin{rmk}
		The power here is to take into account the ``multiplicity'' of the eigenvalue.  For example, on a three-dimensional vector space, $\det (2\id )=2\cdot 2\cdot 2=8$, not just $2$.
	\end{rmk}
	\begin{rmk}
		As a linear-transformation on a finite-dimensional vector spaces has finitely many eigenvalues, this product is finite and there are no worries of convergence.
	\end{rmk}
	\begin{proof}
		We first check that $\det (T)=\det (T^{\A})$.  Let $\{ b_1,\ldots ,b_d\}$ be a basis for $V$.  As $V\subseteq V^{\A}$, $\{ b_1,\ldots ,b_d\}$ is also a subset of $V^{\A}$, and in fact a basis for $V^{\A}$.  From its defining result, we have $T^{\A}(b_k)=T(b_k)$.  It thus follows from the definition of the determinant that $\det (T)=\det (T^{\A})$.
		
		Thus, as $\det (T)=\det (T^{\A})$, without loss of generality assume that $\F$ itself is algebraically closed.  Then, by the previous result, it suffices to show that the determinant of $T\colon \Eig _{\lambda}^{\infty}\rightarrow \Eig _{\lambda}^{\infty}$ is $\lambda ^{\dim (\Eig _{\lambda}^{\infty})}$.  So, without loss of generality, assume that $T$ only has a single eigenvalue $\lambda \in \F$.
		
		Write $d\ceqq \dim (V)=\dim (\Eig _{\lambda}^{\infty})$ and let $\basis{B}=\{ b_1,\ldots ,b_d\}$ be a Jordan basis of $V$ for $T$ so that either $[T-\lambda ](b_k)=b_{k-1}$ or $[T-\lambda ](b_k)=0$ for $1\leq k\leq d$.\footnote{These cases correspond respectively to the case where the corresponding column does and does not have a $1$ above the diagonal.}  Either way, we may write $T(b_k)=\lambda b_k+n_kb_{k-1}$, where $n_k=0,1$.  Now, note that
		\begin{equation}
			T(b_1)\wedge T(b_2)=(\lambda b_1)\wedge (\lambda b_2+n_1b_1)=\lambda ^2b_1\wedge b_2,
		\end{equation}
		and hence
		\begin{equation}
			\begin{split}
				T(b_1)\wedge T(b_2)\wedge T(b_3) & =(\lambda ^2b_1\wedge b_2)\wedge (\lambda b_3+n_2b_2) \\
				& =\lambda ^3b_1\wedge b_2\wedge b_3.
			\end{split}
		\end{equation}
		Proceeding inductively, we find that
		\begin{equation}
			\begin{split}
				\MoveEqLeft {}
				[\antialg* ^dT](b_1\wedge \cdots \wedge b_d)\ceqq T(b_1)\wedge \cdots \wedge T(b_d) \\
				& =\lambda ^db_1\wedge \cdots \wedge b_d,
			\end{split}
		\end{equation}
		and hence $\det (T)=\lambda ^d$, as desired.
	\end{proof}
\end{thm}
This has a couple of important corollaries.
\begin{crl}{}{crl5.7.33}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then, $T$ is invertible iff $\det (T)\neq 0$.
	\begin{rmk}
		A linear-transformation with nonzero determinant is said to be \term{nonsingular}\index{Nonsingular (linear-transformation)}.  Thus, this theorem says that $T$ is invertible iff it is nonsingular.
	\end{rmk}
	\begin{rmk}
		Over a general commutative ring, this instead reads ``$T$ is invertible iff $\det (T)$ is invertible.''.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{crl}
\begin{crl}{$\det (T^{\T})=\det (T)$}{}
	Let $V$ be a finite-dimensional vector space and let $T\colon V\rightarrow V$ be linear.  Then,
	\begin{equation}
		\det (T^{\T})=\det (T).
	\end{equation}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  Use the previous result together with \cref{prp5.2.44}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{crl}

\begin{exr}{}{}
	Let $V$ be a finite-dimensional vector space over a field.
	Show that $\{ v_1,\ldots ,v_m\} \subseteq V$ is linearly-dependent iff $v_1\wedge \cdots \wedge v_m=0$.
	\begin{rmk}
		You can probably do this without determinants, though the solution I have in mind uses them.
	\end{rmk}
	\begin{solution}
		$(\Rightarrow )$ Suppose that $\{ v_1,\ldots ,v_m\} \subseteq V$ is linearly-dependent.  Then, we can write one of these vectors as a linear-combination of the others, without loss of generality, say
		\begin{equation}
		v_1=\alpha _2\cdot v_2+\cdots +\alpha _m\cdot v_m.
		\end{equation}
		Then, upon distributing, $v_1\wedge \cdots \wedge v_m$ will be a sum of $m-1$ terms, each of which vanishes, for the $k^{\text{th}}$ term is a wedge product which contains two copies of $v_{k+1}$.
		
		\blni
		$(\Leftarrow )$ Suppose that $v_1\wedge \cdots \wedge v_m=0$.  We proceed by contradiction:  suppose that $\{ v_1,\ldots ,v_m\}$ is linearly-independent.  Then, we can extend this set to a basis $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ of $V$.  The coordinate map $\coordinates{\blankdot}{\basis{B}}\colon V\rightarrow \F ^d$, $\F$ the ground field and $d\ceqq \dim (V)$, is invertible, and so $\det (\coordinates{\blankdot}{\basis{B}})\neq 0$.  On the other hand,
		\begin{equation}
		[\antialg* ^d\coordinates{\blankdot}{\basis{B}}^{-1}](e_1\wedge \cdots \wedge e_d)\ceqq b_1\wedge \cdots \wedge b_d=0,
		\end{equation}
		where $\{ e_1,\ldots ,e_d\}$ is the standard basis of $\F ^d$:  a contradiction.  Thus, $\{ v_1,\ldots ,v_m\}$ is linearly-independent.
	\end{solution}
\end{exr}

\subsubsection{Determinants of matrices}

We now take a look at the definitions of determinant in terms of matrices you might have seen before.  The first and most important one says that the determinant is the unique scalar-valued function on matrices that assigns $1$ to the identity and transforms a certain way upon performing row operations (\cref{RowOperation}).
\begin{thm}{}{}
	Let $\K$ be a cring and let $m\in \N$.  Then, $\det \colon \Matrix _n(\K )\rightarrow \K$ is the unique function such that
	\begin{align}
		\det (\id _n) & =1 \\
		\det (\Row _{i_1\rightarrow i_1+\alpha i_2}(A)) & =\det (A) \\
		\det (\Row _{i_0\rightarrow \alpha i_0}(A)) & =\alpha \det (A) \\
		\det (\Row _{i_1\leftrightarrow i_2}(A)) & =-\det (A).
	\end{align}
	\begin{rmk}
		Thus, (i) the determinant of the identity matrix is $1$, (ii) the determinant is invariant under adding a scalar multiple of one row to a different one, (iii) the determinant scales (by the same factor) upon scaling a row, and (iv) the determinant changes by a sign when you swap rows.
	\end{rmk}
	\begin{rmk}
		This is both conceptually and practically significant.  First of all, it gives a definition\footnote{Of course, it's a theorem for us, but one \emph{could} take it as the definition (for matrices).} of the determinant of matrices that doesn't look absolutely disgusting.\footnote{Can you believe that some people actually define the determinant in terms of its cofactor expansion?  Ew.}  But besides being aesthetically pleasing, it's actually quite practical for calculating determinants:  a common strategy is to row-reduce the given matrix to something simple enough you can compute the determinant directly (e.g.~to an upper-triangular matrix), and then modify the determinant of that matrix as needed according to the row operations one used.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
There are still at least two other things I have seen taken as the definition of the determinant.  We start with the more practical, though it will take a little bit to set-up---see \cref{CofactorExpansion}.
\begin{dfn}{Minor}{Minor}
	Let $V$ and $W$ be a a finite-dimensional vector space over a field, let $\basis{B}$ and $\basis{C}$ be bases for $V$ and $W$ respectively, and let $T\colon V\rightarrow W$ be a linear-transformation.  Then, the \term{matrix of $k$-minors}\index{Matrix of minors} is $\coordinates{\antialg ^kT}{\antialg ^k\basis{C}\leftarrow \antialg ^k\basis{B}}$.
	\begin{rmk}
		The entries of the matrix of $k$-minors are themselves called \term{minors}\index{Minor}.
	\end{rmk}
	\begin{rmk}
		Write $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ and $\basis{C}\eqqc \{ c_1,\ldots ,c_e\}$.  Recall that (\cref{BasisForAntisymmetricAlgebra})
		\begin{equation}
			\antialg* ^k\basis{B}=\left\{ b_S:S\subseteq \{ 1,\ldots ,d\} ,\abs{S}=k\right\} 
		\end{equation}
		and similarly for $\basis{C}$, where
		\begin{equation}
			b_S\ceqq \antialg _{i\in S}b_i.
		\end{equation}
		
		Thus, the entries of the matrix of minors are not indexed by numbers themselves, but rather \emph{subsets} of numbers.\footnote{I suppose more accurately they're indexed by basis elements, but those are in one-to-one correspondence with subsets of $\{ 1,\ldots ,d\}$ that have $k$ elements.}  Thus, given a subset $S\subseteq \{ 1,\ldots ,d\}$ and $T\subseteq \{ 1,\ldots ,e\}$, we have a corresponding entry of the matrix $\coordinates{\antialg ^kT}{\antialg ^k\basis{C}\leftarrow \antialg ^k\basis{B}}\indices{^T_S}$.  In fact, let's make this notation ``official''.
		
		Given an $m\times n$ matrix $A$, $S\subseteq \{ 1,\ldots ,m\}$, and $T\subseteq \{ 1,\ldots ,n\}$, both with $k$ elements, let us denote by
		\begin{equation}
			A\indices{^S_T}
		\end{equation}
		the $\coord{S,T}$-entry of the matrix of $k$-minors.\footnote{$S$ corresponds to the rows that we keep and $T$ corresponds to the columns that we keep when computing the determinant.}
	\end{rmk}
	\begin{rmk}
		Note this is one of the few places in this section where we don't require the domain and codomain to coincide.  In terms of matrices, this stems from three facts:  (i) one can only talk about the determinant of square matrices, (ii) minors will be the determinants of ``submatrices'', and (iii) a nonsquare matrix can have submatrices that are square.
	\end{rmk}
\end{dfn}
Okay, so as it stands, this definition is pretty impenetrable.  Let's look at an example.
\begin{exm}{}{exm5.7.36}
	Let $V\ceqq \R ^3$, $W\ceqq \R ^2$, take $\basis{B}\ceqq \{ e_1,e_2,e_2\}$ and $\basis{C}\ceqq \{ e_1,e_2\}$ to be the standard bases, and let $T\colon V\rightarrow W$ be the linear-transformation defined by the matrix
	\begin{equation}
		\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix}.
	\end{equation}
	
	$\antialg ^0T\colon \R \rightarrow \R$ is multiplication by $1$, and not very interesting.  On the other hand, $\antialg ^1T\colon V\rightarrow W$ is just $T$ itself, which we already understand.  So, let's take a look at $\antialg ^2T\colon \antialg ^2V\rightarrow \antialg ^2W$.
	
	First of all, let's look at the bases $\antialg ^2\basis{B}$ and $\antialg ^2\basis{C}$.  According to \cref{BasisForAntisymmetricAlgebra}, we have
	\begin{equation}
		\antialg* ^2\basis{B}=\{ e_1\wedge e_2,e_1\wedge e_3,e_2\wedge e_3\}
	\end{equation}
	and
	\begin{equation}
		\antialg* ^2\basis{C}=\{ e_1\wedge e_2\} .
	\end{equation}
	We want to compute the matrix of $\antialg ^2T$ with respect to these two bases, so going all the way back to \cref{CoordinatesLinearTransformation}, we compute what $\antialg ^2T$ does do the elements of the basis $\antialg ^2\basis{B}$.
	\begin{equation}
		\begin{split}
			[\antialg* ^2T](e_1\wedge e_2) & \ceqq T(e_1)\wedge T(e_2) \\
			& \ceqq (e_1+4e_2)\wedge (2e_1+5e_2) \\
			& =5e_1\wedge e_2+8e_2\wedge e_1 \\
			& =-3e_1\wedge e_2.
		\end{split}
	\end{equation}
	Similarly,
	\begin{equation}
		\begin{split}
			[\antialg* ^2T](e_1\wedge e_3) & \ceqq T(e_1)\wedge T(e_3) \\
			& \ceqq (e_1+4e_2)\wedge (3e_1+6e_2) \\
			& =6e_1\wedge e_2+12e_2\wedge e_1 \\
			& =-6e_1\wedge e_2.
		\end{split}
	\end{equation}
	Finally,
	\begin{equation}
		\begin{split}
			[\antialg* ^2T](e_2\wedge e_3) & \ceqq T(e_2)\wedge T(e_3) \\
			& \ceqq (2e_1+5e_2)\wedge (3e_1+6e_2) \\
			& =12e_1\wedge e_2+15e_2\wedge e_1=-3e_1\wedge e_2.
		\end{split}
	\end{equation}
	Thus,
	\begin{equation}
		\coordinates{\antialg ^2T}{\antialg ^2\basis{C}\leftarrow \antialg ^2\basis{B}}=\begin{bmatrix}-3 & -6 & -3\end{bmatrix}.
	\end{equation}
	
	\begin{exr}[breakable=false]{}{}
		Check that $\antialg ^kT=0$ for $k\geq 3$, so there is nothing further to investigate.
	\end{exr}
\end{exm}
We mentioned in a remark of the definition that minors are secretly determinants of ``submatrices''.  Let us make this precise.
\begin{thm}{Minors as determinants of submatrices}{}
	Let $V$ and $W$ be a a finite-dimensional vector space over a field, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ and $\basis{C}\eqqc \{ c_1,\ldots ,c_e\}$ be bases for $V$ and $W$ respectively, and let $T\colon V\rightarrow W$ be a linear-transformation.  Then, the entry of $\coordinates{\antialg ^kT}{\antialg ^k\basis{C}\leftarrow \antialg ^k\basis{B}}$ corresponding to $c_{j_1}\wedge \cdots \wedge c_{j_k}\in \antialg ^k\basis{C}$ and $b_{i_1}\wedge \cdots \wedge b_{i_k}\in \antialg ^k\basis{B}$ is the determinant of the $k\times k$ matrix obtained from $\coordinates{T}{\basis{C}\leftarrow \basis{B}}$ by removing all the rows except for rows $j_1,\ldots ,j_k$ and removing all columns except for columns $i_1,\ldots ,i_k$.
	\begin{rmk}
		For example, suppose I compute $T(b_2\wedge b_4\wedge b_7)$, and when I write this as a linear combination of the elements of $\antialg ^k\basis{C}$, I find that the coefficient of $c_1\wedge c_2\wedge c_5$ is $20$, this means that the determinant of the of the matrix formed from the $2^{\text{nd}}$, $4^{\text{th}}$, and $7^{\text{th}}$ columns and the $1^{\text{st}}$, $2^{\text{nd}}$, and $5^{\text{th}}$ rows is $20$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
Okay, again, impenetrable.  So, again, an example.
\begin{exm}{}{}
	Let everything be as it was in \cref{exm5.7.36}, that is, Let $V\ceqq \R ^3$, $W\ceqq \R ^2$, take $\basis{B}\ceqq \{ e_1,e_2,e_2\}$ and $\basis{C}\ceqq \{ e_1,e_2\}$ to be the standard bases, and let $T\colon V\rightarrow W$ be the linear-transformation defined by the matrix
	\begin{equation}\label{eqn5.7.45}
		\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix}.
	\end{equation}
	
	The $0\times 0$ submatrices are, well, not interesting, at least not to those of us who are not scholars of the empty matrix.  The $1\times 1$ submatrices are just the entries of the matrix themselves.  Again, not so interesting.  There are no square $k\times k$ submatrices for $k\geq 3$.  On the other hand, there are $3$ $2\times 2$ submatrices:
	\begin{equation}\label{eqn5.7.46}
		\begin{bmatrix}1 & 2 \\ 4 & 5\end{bmatrix}\qquad \begin{bmatrix}1 & 3 \\ 4 & 6\end{bmatrix}\qquad \begin{bmatrix}2 & 3 \\ 5 & 6\end{bmatrix}.
	\end{equation}
	The determinant of these matrices are respectively $-3$, $-6$, and $-3$.\footnote{You can of course use the definition, though I (secretly) used facts we have yet to cover ``officially''---see \cref{crl5.7.60} (this tells you how to compute determinants of $2\times 2$ matrices).}
	
	On the other hand, recall from \cref{exm5.7.36} that we found that
	\begin{align}
		[\antialg* ^2T](e_1\wedge e_2) & =-3e_1\wedge e_2 \\
		[\antialg* ^2T](e_1\wedge e_3) & =-6e_1\wedge e_2 \\
		[\antialg* ^2T](e_2\wedge e_3) & =-3e_1\wedge e_2.
	\end{align}
	In other words, the $\coord{e_1\wedge e_2,e_1\wedge e_2}$ entry of the matrix is $-3$, the $\coord{e_1\wedge e_2,e_1\wedge e_3}$ entry of the matrix is $-6$, and the $\coord{e_1\wedge e_2,e_2\wedge e_3}$ entry of the matrix is $-3$.\footnote{Here, ``the matrix'' refers to the matrix of $\antialg ^2T$ with respect to the bases $\antialg ^2\basis{B}$ and $\antialg ^2\basis{C}$.}
	
	Sure enough, the first matrix in \eqref{eqn5.7.46} is the one obtained from \eqref{eqn5.7.45} by only retaining the first and second rows (corresponding to $e_1\wedge e_2$) and the first and second columns (corresponding to $e_1\wedge e_2$.  The second matrix is the one obtained from \eqref{eqn5.7.45} by only retaining the first and second rows (corresponding to $e_1\wedge e_2)$ and the first and third columns (corresponding to $e_1\wedge e_3$).  The third matrix is the one obtained from \eqref{eqn5.7.45} by only retaining the first and second rows (corresponding to $e_1\wedge e_2$) and the second and third columns (corresponding to $e_2\wedge e_3$).
	
	Thus, the entries of the $2$-minor matrix
	\begin{equation}
		\begin{bmatrix}-3 & -6 & -3\end{bmatrix}
	\end{equation}
	do correspond to the determinants of the corresponding $2\times 2$ submatrices.
\end{exm}
Great.  Minors.  Fantastic.  That's all good and dandy, and minors certainly have their uses, but right now we're interested in their relationship to the determinant of the \emph{original} matrix, not submatrices.  It turns out that you can calculate the determinant of the entire matrix by summing over products of determinants of submatrices, that is, the minors.
\begin{thm}{Laplace expansion}{LaplaceExpansion}\index{Laplace expansion}
	Let $V$ be a finite-dimensional vector space over a field, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis for $V$, let $T\colon V\rightarrow V$ be a linear-transformation, and write $A\ceqq \coordinates{T}{\basis{B}}$.
	\begin{enumerate}
		\item Let $S\subseteq \{ 1,\ldots ,d\}$.  Then,
		\begin{equation}
			\det (A)=\sum _{\substack{X\subseteq \{ 1,\ldots ,d\} \\ \abs{X}=\abs{S}}}\sgn (S)A\indices{^S_X}A\indices{^{S^{\comp}}_{X^{\comp}}}.
		\end{equation}
		\item Let $T\subseteq \{ 1,\ldots ,d\}$.  Then,
		\begin{equation}
			\det (A)=\sum _{\substack{X\subseteq \{ 1,\ldots ,d\} \\ \abs{X}=\abs{T}}}\sgn (T)A\indices{^X_T}A\indices{^{X^{\comp}}_{T^{\comp}}}.
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		I recommend you just look at an example (\cref{exm5.7.69}) to understand this.
	\end{rmk}
	\begin{rmk}
		See the remark in \cref{SignOfAPermutation} to recall the notation $\sgn (S)$ and $\sgn (T)$.
	\end{rmk}
	\begin{rmk}
		See the remark in \cref{Minor} to recall the notation $A\indices{^S_X}$, etc..
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
So that's neat.  But again, impenetrable.  And so again, an example.
\begin{exm}{}{exm5.7.69}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}5 & 4 & 2 & 1 \\ 0 & 1 & -1 & -1 \\ -1 & -1 & 3 & 0 \\ 1 & 1 & -1 & 2\end{bmatrix}.
	\end{equation}
	We compute $\det (A)$ using the \namerefpcref{LaplaceExpansion} of $A$ across the first two rows ($S=\{ 1,2\}$ in the notation of \cref{LaplaceExpansion}).\footnote{We make use of \cref{crl5.7.60} to do the $2\times 2$ determinants `in our heads'.}
	\begin{equation}
		\begin{split}
			\det (A) & =\det \left( \begin{bmatrix}5 & 4 \\ 0 & 1\end{bmatrix}\right) \det \left( \begin{bmatrix}3 & 0 \\ -1 & 2\end{bmatrix}\right) \\ & \qquad -\det \left( \begin{bmatrix}5 & 2 \\ 0 & -1\end{bmatrix}\right) \det \left( \begin{bmatrix}-1 & 0 \\ 1 & 2\end{bmatrix}\right) \\
			& \qquad +\det \left( \begin{bmatrix}5 & 1 \\ 0 & -1\end{bmatrix}\right) \det \left( \begin{bmatrix}-1 & 3 \\ 1 & -1\end{bmatrix}\right) \\ & \qquad +\det \left( \begin{bmatrix}4 & 2 \\ 1 & -1\end{bmatrix}\right) \det \left( \begin{bmatrix}-1 & 0 \\ 1 & 2\end{bmatrix}\right) \\
			& \qquad -\det \left( \begin{bmatrix}4 & 1 \\ 1 & -1\end{bmatrix}\right) \det \left( \begin{bmatrix}-1 & 3 \\ 1 & -1\end{bmatrix}\right) \\ & \qquad +\det \left( \begin{bmatrix}2 & 1 \\ -1 & -1\end{bmatrix}\right) \det \left( \begin{bmatrix}-1 & -1 \\ 1 & 1\end{bmatrix}\right) \\
			& =5\cdot 6-(-5)\cdot (-2) \\
			& \qquad +(-5)\cdot (-2)+(-6)\cdot (-2) \\
			& \qquad -(-5)\cdot (-2)+(-1)\cdot 0 \\
			& =32.
		\end{split}
	\end{equation}
\end{exm}
As neat is it may be, I've never seen this general result used, and in fact, have only seen a special case used, which itself is called the \emph{cofactor expansion}.  Before we see that, however, we must first introduce \emph{cofactors} themselves.
\begin{dfn}{Cofactor}{Cofactor}
	Let $V$ be a finite-dimensional vector space over a field, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis for $V$, let $T\colon V\rightarrow V$ be a linear-transformation, and let $1\leq i,j\leq d$.  Then, the \term{$\coord{i,j}$-cofactor}\index{Cofactor}, $\Cof _{\basis{B}}(T)\indices{_j^i}$\index[notation]{$\Cof _{\basis{B}}(T)\indices{_j^i}$} of $T$ with respect to $\basis{B}$ is defined by
	\begin{equation}\label{eqn5.7.52}
		\Cof _{\basis{B}}(T)\indices{_j^i}\ceqq (-1)^{i+j}\coordinates{\antialg* ^{d-1}T}{\antialg* ^{d-1}\basis{B}}\indices{^{\hat{b}_i}_{\hat{b}_j}},
	\end{equation}
	where
	\begin{equation}
		\hat{b}_k\ceqq b_1\wedge \cdots \wedge b_{k-1}\wedge b_{k+1}\wedge \cdots \wedge b_d\in \antialg* ^{d-1}\basis{B}.
	\end{equation}
	\begin{rmk}
		If $T$ itself is defined by a matrix $A$, then we shall simply write $\Cof (A)\indices{_i^j}\ceqq \Cof _{\basis{S}}(T_A)\indices{_i^j}$\index[notation]{$\Cof (A)\indices{_i^j}$}, where $T_A$ is the linear-transformation defined by $A$ and $\basis{S}$ is the standard basis.
	\end{rmk}
	\begin{rmk}
		In words, \eqref{eqn5.7.52} says that the $\coord{i,j}$-cofactor is obtained by taking the determinant of the $(d-1)\times (d-1)$ submatrices obtained by `deleting' the $i^{\text{th}}$ row and $j^{\text{th}}$ column from $\coordinates{T}{\basis{B}}$ and multiplying the result by $(-1)^{i+j}$---see the following example.
	\end{rmk}
	\begin{rmk}
		The signs coming from the $(-1)^{i+j}$ look like, for a $3\times 3$ matrix, for example,
		\begin{equation}
			\begin{bmatrix}+ & - & + \\ - & + & - \\ + & - & +\end{bmatrix}.
		\end{equation}
		That is, the signs alternate going down rows and across columns starting from $+$ at the top-left.  So, for example, the ``$-$'' in the $\coord{2,3}$ entry means that, when you remove the $2^{\text{nd}}$ row and $3^{\text{rd}}$ column and take the determinant, you multiply the result by $-1$.
	\end{rmk}
	\begin{rmk}
		Note the way in which the indices are staggered.  To see why it's natural to stagger them in this way, see \cref{CofactorExpansion,Adjugate}.
	\end{rmk}
\end{dfn}
Again, we should look at an example.
\begin{exm}{}{}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9\end{bmatrix}.
	\end{equation}
	
	Suppose we want to compute the $\coord{2,3}$-cofactor.  We start by removing the $2^{\text{nd}}$ row and $3^{\text{rd}}$ column to obtain the matrix
	\begin{equation}
		\begin{bmatrix}1 & 2 \\ 7 & 8\end{bmatrix}.
	\end{equation}
	The determinant of this is $-6$, and hence
	\begin{equation}
		\Cof (A)\indices{_2^3}=(-1)^{2+3}\cdot (-6)=6.
	\end{equation}
\end{exm}
Finally, we are able to state how this allows us to compute determinants.
\begin{crl}{Cofactor expansion}{CofactorExpansion}
	Let $V$ be a finite-dimensional vector space over a field, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis for $V$, let $T\colon V\rightarrow V$ be a linear-transformation, write $A\ceqq \coordinates{T}{\basis{B}}$, and let $1\leq i,j\leq d$.  Then,
	\begin{equation}
		\sum _{k=1}^dA\indices{^i_k}\Cof (A){_i^k}=\det (A)=\sum _{k=1}^dA\indices{^k_j}\Cof (A)\indices{_k^j}.
	\end{equation}
	\begin{rmk}
		The left equality says that we can pick any row of $A$, and them sum across the row multiplying the entry of $A$ by the corresponding cofactor.  The right equality says essentially the same thing, but for columns.
	\end{rmk}
	\begin{rmk}
		I have also see this referred to as the Laplace expansion.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{crl}
\begin{crl}{}{crl5.7.60}
	Let $\K$ be a cring and let $a,b,c,d\in \K$.  Then,
	\begin{equation}
		\det \left( \begin{bmatrix}a & b \\ c & d\end{bmatrix}\right) =ad-bc.
	\end{equation}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{crl}
\begin{exm}{}{}
	Define
	\begin{equation}
		A\ceqq \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9\end{bmatrix}.
	\end{equation}
	
	This theorem says that, using the first \emph{row},
	\begin{equation}
		\begin{split}
			\det (A) & =1\cdot \det \left( \begin{bmatrix}5 & 6 \\ 8 & 9\end{bmatrix}\right) -2\cdot \det \left( \begin{bmatrix}4 & 6 \\ 7 & 9\end{bmatrix}\right) \\ & \qquad +3\cdot \det \left( \begin{bmatrix}4 & 5 \\ 7 & 8\end{bmatrix}\right) \\
			& =1\cdot (-3)-2\cdot (-6)+3\cdot (-3)=0.
		\end{split}
	\end{equation}
	
	On the other hand, using, for example, the second \emph{column},
	\begin{equation}
		\begin{split}
			\det (A) & =-2 \cdot \det \left( \begin{bmatrix}4 & 6 \\ 7 & 9\end{bmatrix}\right) +5\cdot \det \left( \begin{bmatrix}1 & 3 \\ 7 & 9\end{bmatrix}\right) \\ & \qquad -8\cdot \det \left( \begin{bmatrix}1 & 3 \\ 4 & 6\end{bmatrix}\right) \\
			& =-2\cdot (-6)+5\cdot (-12)-8\cdot (-6)=0.
		\end{split}
	\end{equation}
\end{exm}
There is one more variant that is sometimes taken as the definition of the determinant that we must cover, but while we're talking about cofactors, it makes sense to discuss the \emph{adjugate matrix}.
\begin{dfn}{Adjugate}{Adjugate}
	Let $\F$ be a field and let $A$ be an $n\times n$ matrix with entries in $\F$.  Then, the \term{adjugate}\index{Adjugate} of $A$, $\operatorname{Adj}(A)$\index[notation]{$\operatorname{Adj}(A)$}, is defined by
	\begin{equation}
		\operatorname{Adj}(A)\ceqq \Cof (A)^{\T}.
	\end{equation}
	\begin{rmk}
		Warning:  The ``adjugate'' is \emph{not} the same as the ``adjoint'', which we will come to when we study inner-product spaces.
	\end{rmk}
	\begin{rmk}
		That is, the adjugate matrix is the transpose of the matrix of cofactors.  Incidentally, we see now why the indicates on $\Cof (A)\indices{_i^j}$ are staggered the way they are---according to \cref{TransposeIndex}, this implies that the indices on $\operatorname{Adj}(A)$ should be staggered like $\operatorname{Adj}(A)\indices{^i_j}$.  That we want this will be clear when we get to \cref{AdjugateTheorem}
	\end{rmk}
\end{dfn}
\begin{thm}{}{AdjugateTheorem}
	Let $\F$ be a field and let $A$ be an $n\times n$ matrix with entries in $\F$.  Then,
	\begin{equation}
		A\operatorname{Adj}(A)=\det (A)\id _n=\operatorname{Adj}(A)A.
	\end{equation}
	\begin{rmk}
		In particular, if $A$ is invertible, then
		\begin{equation}
			A^{-1}=\det (A)^{-1}\operatorname{Adj}(A).
		\end{equation}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
Related to this is \emph{Cramer's rule}.
\begin{thm}{Cramer's Rule}{CramersRule}\index{Cramer's rule}
	Let $\F$ be a field, let $A$ be an invertible $n\times n$ matrix with entires in $\F$, and let $x,b\in \F ^n$.  Then, if $Ax=b$, it follows that
	\begin{equation}
		x^k=\frac{\det (A_k)}{\det (A)},
	\end{equation}
	where here $A_k$ is the matrix obtain from $A$ by replacing its $k^{\text{th}}$ column with the vector $b\in \F ^n$.
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

Finally, we return to the last statement that is sometimes taken as the definition of the determinant.\footnote{Or rather, the last one that I'll be covering.}
\begin{thm}{}{thm5.7.96}
	Let $\F$ be a field and let $A$ be an $n\times n$ matrix with entries in $\F$.
	\begin{enumerate}
		\item \label{thm5.7.96(i)}
		\begin{equation}
			\begin{multlined}
				\det (A) \\ =\sum _{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,n\} )}\sgn (\sigma )A\indices{^{\sigma (1)}_1}\cdots A\indices{^{\sigma (n)}_n}.
			\end{multlined}
		\end{equation}
		\item \label{thm5.7.96(ii)}
		\begin{equation}
			\begin{multlined}
				\det (A) \\ =\sum _{\sigma \in \Aut _{\Set}(\{ 1,\ldots ,n\} )}\sgn (\sigma )A\indices{^1_{\sigma (1)}}\cdots A\indices{^n_{\sigma (n)}}.
			\end{multlined}
		\end{equation}
	\end{enumerate}
	\begin{rmk}
		In words (for \cref{thm5.7.96(ii)}):  Pick an entry from every column of $A$.  Multiply these entries, together with possibly a minus sign depending on the order you chose the entries.  Add up all the numbers found in this way for each way of picking entries.\footnote{Of course, there are many ways (in fact, $n!$ ways) to pick exactly one entry from each column.}
		
		\cref{thm5.7.96(i)} is of course similar with the rows and columns switching roles.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

\subsubsection{The geometric interpretation of the determinant}

In $\R ^d$, the determinant can be given a geometric interpretation.  The absolute value tells us by how much the volume scales and the sign tells us whether the orientation changes or not.

We begin with the former.
\begin{thm}{}{}
	Let $T\colon \R ^d\rightarrow \R ^d$ be linear and let $S\subseteq \R ^d$.  Then,
	\begin{equation}
		\vol (T(S))=\abs{\det (T)}\vol (S),
	\end{equation}
	where $\vol (S)$ is the Lebesgue measure of $S$.
	\begin{rmk}
		You don't need to know the precise definition of Lebesgue measure to understand the meaning of this statement (or even to use it).  Lebesgue measure is a generalization of volume to any subset of $\R ^d$, but the interpretation is really still just ``volume''.\footnote{Though for $d=1$ and $d=2$, this is more commonly referred to as ``length'' and ``area'' respectively.  Additionally, people might say ``hyper-volume'' for $d\geq 4$.}  Certainly, it gives you the answer you would expect for any set of which you thought you previously knew the volume.
		
		For example, if $S$ is the ``unit cube'', then this statement reads $\vol (T(S))=\abs{\det (T)}$---no specific knowledge of Lebesgue measure required.
	\end{rmk}
	\begin{rmk}
		Yes, $S$ can be \emph{any} set, measurable or not (though in this case it's usually called Lebesgue \emph{outer} measure).
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

So that's what the absolute value of the determinant means.  It turns out that the sign tells you what happens to the orientation.  To state this result, however, we had first better way what we actually mean by ``orientation''.
\begin{dfn}{Volume form\hfill}{VolumeForm}
	Let $V$ be a $d$-dimensional vector space over a field.  Then, a \term{volume form}\index{Volume form} on $V$ is a nonzero element of $\antialg _dV$.
	\begin{rmk}
		Note how this is a \emph{covariant} tensor.  For our purposes, we could have used contravariant, but for reasons having to do with calculus, ``volume form'' refers to a, well, a \emph{form}---see the remark in \cref{Antisymmetric}.
	\end{rmk}
\end{dfn}
\begin{prp}{Orientation\hfill}{Orientation}
	Let $V$ be a $d$-dimensional real vector space, let $\omega _1,\omega _2\in \antialg ^dV$ be volume forms, and let us say that $\omega _1\sim \omega _2$ iff there is some $a>0$ such that $\omega _1=a\omega _2$.  Then, $\sim$ is an equivalence relation on the volume forms on $V$.
	\begin{rmk}
		An \term{orientation}\index{Orientation} of $V$ is an equivalence class of volume forms on $V$.
	\end{rmk}
	\begin{rmk}
		The intuition is roughly that a choice of ordered basis should give an orientation.  Given such an ordered basis $\{ b_1,\ldots ,b_d\}$, this determines the volume form $b_1\wedge \cdots \wedge b_d\in \antialg ^dV$.  Intuitively, if we swap the order of two basis elements, this should change the orientation.\footnote{In two dimensions, imagining your copy of $\R ^2$ embedded in $\R ^3$, you can think of an orientation as a choice of which side of the plane counts as ``up''.  This is determined by picking an ordered basis of $\R ^2$ and using the right-hand rule to the ``up'' direction.  Because of how the right-hand rule works, if you swap the order of these two vectors, you're going to change the ``up'' direction.}  As swapping two basis elements changes the sign of this volume form, we are lead to declaring two volume forms to be equivalent iff they are a scalar multiple of one another.\footnote{Note that, as $\dim (\antialg ^dV)=1$, they have to be scalar multiples of one another---it is then just an issue of whether or not that scalar multiple if positive or negative.}
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
\begin{dfn}{Oriented}{Oriented}
	An \term{oriented vector space}\index{Oriented vector space} is a finite-dimensional real vector space $V$ together with an orientation $\omega /\sim$ on $V$.
\end{dfn}
As now we can state the theorem giving the relationship between orientation and the sign of the determinant.
\begin{thm}{}{}
	Let $\coord{V,\omega /\sim}$ be an oriented vector space and let $T\colon V\rightarrow V$ be linear.  Then, $[\antialg _dT^{\T}](\omega )/\sim =\omega /\sim$ iff $\det (T)>0$.
	\begin{rmk}
		$T$ sends the original orientation $\omega /\sim$ to a new orientation $[\antialg _dT^{\T}](\omega )/\sim$.  Thus, this is the statement that $\det (T)>0$ iff $T$ ``preserves'' the orientation.
	\end{rmk}
	\begin{rmk}
		Up until now, we have only been dealing with the contravariant versions $\antialg ^kT$.  Here, we need to use the covariant version $\antialg _dT^{\T}$ as $\omega$ is a covariant tensor.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

\subsubsection{The characteristic polynomial}

Let $V$ be a finite-dimensional vector space over a field $\F$, let $T\colon V\rightarrow V$ be linear, and let $\lambda \in \F$.  From the last chapter, hopefully you have no trouble remembering that $\lambda$ is an eigenvalue of $T$ iff $\Ker (T-\lambda )\neq =0$.  This is of course the statement that $T-\lambda$ is \emph{not} injective, which, as we are in finite dimensions, is equivalent to the statement that $T-\lambda$ is \emph{not} invertible.  But we've just discovered that (\cref{crl5.7.33}) $T-\lambda$ is not invertible iff $\det (T-\lambda )=0$.  Hm.  I wonder what happens if we try to compute $\det (T-\lambda )$.
\begin{thm}{Characteristic polynomial}{CharacteristicPolynomial}
	Let $V$ be a $d$-dimensional vector space over a field $\F$, let $T\colon V\rightarrow V$ be linear, let $\lambda \in \F$.  Then,
	\begin{equation}
		\det (T-\lambda )=\sum _{k=0}^d(-1)^k\tr (\antialg* ^{d-k}T)\lambda ^k.
	\end{equation}
	\begin{rmk}
		\begin{equation}
			p_{\operatorname{char},T}(\lambda )\ceqq \det (T-\lambda )
		\end{equation}\index[notation]{$p_{\operatorname{char},T}$}
		is the \term{characteristic polynomial}\index{Characteristic polynomial} of $T$.  As per usual, we shall write $p_{\operatorname{char}}\ceqq p_{\operatorname{char},T}$\index[notation]{$p_{\operatorname{char}}$} if $T$ is clear from context.
	\end{rmk}
	\begin{rmk}
		In particular, note that the coefficient of $\lambda ^d$ is $(-1)^d$, the coefficient of $\lambda ^{d-1}$ is $(-1)^{d-1}\tr (T)$, and the constant term is $\det (T)$.
		
		In general, the coefficient of $\lambda ^k$ is, up to a sign, the trace of the matrix of $(d-k)$-minors, though this isn't quite as useful for other values of $k$.
	\end{rmk}
	\begin{rmk}
		Warning:  Some others take the characteristic polynomial to be $\det (\lambda -T)$ instead.  Of course, these are the same up to a minus sign, and so it makes no difference.  The advantage this other definition has is that the resulting polynomial is always monic.  I chose the convention I did because I find it just slightly more natural---we've written $T-\lambda$ everywhere else, wouldn't it be just a teensy bit weird to make an exception here and write $\lambda -T$?
	\end{rmk}
	\begin{proof}
		For convenience, replacing $\lambda$ with $-\lambda$, we instead show that
		\begin{equation}
			\det (T+\lambda )=\sum _{k=0}^d\tr (\antialg* ^{d-k}T)\lambda ^k.
		\end{equation}
		
		So, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis of $V$.  Then, for $0\leq k\leq d$, by \cref{BasisForAntisymmetricAlgebra},
		\begin{equation}
			\antialg* ^k\basis{B}=\left\{ b_S:S\subseteq \{ 1,\ldots ,d\} ,\abs{S}=k\right\} 
		\end{equation}
		is a basis of $\antialg ^kV$, where
		\begin{equation}\label{eqn5.7.115}
			b_S\ceqq \bigwedge _{i\in S}b_i,
		\end{equation}
		where the wedge product is taken in order of increasing $i$.  As this is a basis, we can speak of coordinates, and so let us abbreviate
		\begin{equation}
			[T]\indices{^S_T}\ceqq \coordinates{\antialg* ^kT}{\antialg* ^k\basis{B}}\indices{^S_T},
		\end{equation}
		where $S,T\subseteq \{ 1,\ldots ,d\}$ with $\abs{S}=k=\abs{T}$.  That is, $[T]\indices{^S_T}$ is the coefficient of $b_S$ when you write $[\antialg ^kT](b_T)$ as a linear-combination of the elements of $\antialg ^k\basis{B}$.  Thus,
		\begin{equation}
			\tr (\antialg* ^kT)=\sum _{\substack{S\subseteq \{ 1,\ldots ,d\} \\ \abs{S}=k}}[T]\indices{^S_S}.
		\end{equation}
		
		Also note that
		\begin{equation}
			[\antialg* ^kT](b_S)\wedge b_{S^{\comp}}=\sgn (S)[T]\indices{^S_S}b_1\wedge \cdots \wedge b_d,\footnote{The $\sgn (S)$ (\cref{SignOfAPermutation}) arises as a result of taking all the factors of $b_S$ and of $b_{S^{\comp}}$ and putting them `in order' to yield $b_1\wedge \cdots \wedge b_d$.}
		\end{equation}
		for, upon writing $[\antialg ^kT](b_S)$ as a linear-combination of elements of $\antialg ^k\basis{B}$, we see that all the $b_T$-term will vanish when you take the wedge product with $b_{S^{\comp}}$ unless $b_T$ and $b_{S^{\comp}}$ have no factors in common.\footnote{This of course, from the definition \eqref{eqn5.7.115}, is true iff $T=S$.}
		
		Using this, we see that
		\begin{equation}
			\begin{split}
				\MoveEqLeft {}
				[\antialg* ^d[T+\lambda ]](v_1\wedge \cdots \wedge v_d) \\
				& \ceqq [[T+\lambda ](v_1)]\wedge \cdots \wedge [[T+\lambda ](v_d)] \\
				& \ceqq (T(v_1)+\lambda v_1)\wedge \cdots \wedge (T(v_d)+\lambda v_d) \\
				& =\sum _{k=0}^d\lambda ^k\left[ \sum _{\substack{S\subseteq \{ 1,\ldots ,d\} \\ \abs{S}=k-d}}\sgn (S)[\antialg* ^kT](b_S)\wedge b_{S^{\comp}}\right] \\
				& =\sum _{k=0}^d\lambda ^k\left[ \sum _{\substack{S\subseteq \{ 1,\ldots ,d\} \\ \abs{S}=k-d}}\sgn (S)^2[T]\indices{^S_S}b_1\wedge \cdots \wedge b_d\right] \\
				& =\left( \sum _{k=0}^d\lambda ^k\tr (\antialg* ^{d-k}T)\right) b_1\wedge \cdots \wedge b_d,
			\end{split}
		\end{equation}
		and hence
		\begin{equation}
			\det (T+\lambda )=\sum _{k=0}^d\tr (\antialg* ^{d-k}T)\lambda ^k,
		\end{equation}
		as desired.
	\end{proof}
\end{thm}
\begin{thm}{}{CharacteristicPolynomialEigenvalue}
	Let $V$ be a $d$-dimensional vector space over a field $\F$ with algebraic closure $\A$ and let $T\colon V\rightarrow V$ be linear.  Then,
	\begin{equation}
		p_{\operatorname{char}}(x)=(-1)^d\prod _{\lambda \in \Eig (T^{\A})}(x-\lambda )^{d_{\lambda}},
	\end{equation}
	where $d_{\lambda}\ceqq \dim (\Eig _{\lambda ,T^{\A}}^{\infty})$.
	\begin{rmk}
		In particular, for $\lambda \in \F$, $\lambda \in \Eig (T)$ off $p_{\operatorname{char}}(\lambda )=0$.
	\end{rmk}
	\begin{proof}
		This follows immediately from \cref{thm5.7.28} by replacing $T$ with $T-\lambda$.
	\end{proof}
\end{thm}
\begin{crl}{Cayley-Hamilton Theorem}{CayleyHamiltonTheorem}\index{Cayley-Hamilton Theorem}
	Let $V$ be a finite-dimensional vector space over a field and let $T\colon V\rightarrow V$ be linear.  Then,
	\begin{equation}
		p_{\operatorname{char}}(T)=0.
	\end{equation}
	\begin{rmk}
		In fact, it follows from \cref{MinimalPolynomial} (the defining result of the minimal polynomial) that $p_{\min}$ divides $p_{\operatorname{char}}$.
	\end{rmk}
	\begin{proof}
		Let $\F$ be the ground field of $V$ and let $\A$ be an algebraic-closure of $\F$.  By \cref{MinimalJordan}, we have that
		\begin{equation}
			p_{\min}(x)=\prod _{\lambda \in \Eig (T^{\A})}(x-\lambda )^{n_{\lambda}},
		\end{equation}
		where $n_{\lambda}$ is the size of the largest Jordan block with eigenvalue $\lambda$ appearing in the Jordan canonical form of $T^{\A}$.  In particular, $n_{\lambda}\leq \dim (\Eig _{\lambda, T^{\A}}^{\infty})$.  By the definition of the minimal polynomial, we certainly have that
		\begin{equation}
			p_{\min}(T)=0.
		\end{equation}
		Then, from the previous result, as $n_{\lambda}\leq \dim (\Eig _{\lambda ,T^{\A}})$, it follows that
		\begin{equation}
			p_{\operatorname{char}}(T)=0.
		\end{equation}
	\end{proof}
\end{crl}

As you may already be aware, one of the most important applications of the characteristic polynomial is in the computation of eigenvalues (of matrices).\footnote{If your linear-transformation is not defined by a matrix or a matrix cannot be easily be computed for it, please, for the love of \texttt{insert nonexclusive deity reference here}, just use the definition.  For example, if I ask you to find the eigenvalues of the derivative operator, don't try to take the determinant of $\frac{\dif}{\dif x}-\lambda$ or something crazy like that.}  Let us return to the matrix of \cref{exm4.5.32} where we more or less just told you what the eigenvalues were.
\begin{exm}{}{exm4.5.32x}
	As in \cref{exm4.5.32}, define
	\begin{equation}
		A\ceqq \begin{bmatrix}5 & 4 & 2 & 1 \\ 0 & 1 & -1 & -1 \\ -1 & -1 & 3 & 0 \\ 1 & 1 & -1 & 2\end{bmatrix},
	\end{equation}
	so that
	\begin{equation}
		A-\lambda \ceqq \begin{bmatrix}5-\lambda & 4 & 2 & 1 \\ 0 & 1-\lambda & -1 & -1 \\ -1 & -1 & 3-\lambda & 0 \\ 1 & 1 & -1 & 2-\lambda \end{bmatrix}.
	\end{equation}
	Using the cofactor expansion along the first column, we compute
	{\scriptsize
	\begin{equation}
		\begin{split}
			\MoveEqLeft
			\det (A-\lambda )=(5-\lambda )\cdot \det \left( \begin{bmatrix}1-\lambda & -1 & -1 \\ -1 & 3-\lambda & 0 \\ 1 & -1 & 2-\lambda \end{bmatrix}\right) \\ & \qquad -1\cdot \det \left( \begin{bmatrix}4 & 2 & 1 \\ 1-\lambda & -1 & -1 \\ 1 & -1 & 2-\lambda \end{bmatrix}\right) \\ & \qquad -1\cdot \det \left( \begin{bmatrix}4 & 2 & 1 \\ 1-\lambda & -1 & -1 \\ -1 & 3-\lambda & 0\end{bmatrix}\right) \\
			& =(5-\lambda)\cdot \left( -1\cdot \det \left( \begin{bmatrix}-1 & 3-\lambda \\ 1 & -1\end{bmatrix}\right) +(2-\lambda )\cdot \det \left( \begin{bmatrix}1-\lambda & -1 \\ -1 & 3-\lambda\end{bmatrix}\right) \right) \\ & \qquad -\left( 4\cdot \det \left( \begin{bmatrix}-1 & -1 \\ -1 & 2-\lambda \end{bmatrix}\right) -2\cdot \det \left( \begin{bmatrix}1-\lambda & -1 \\ 1 & 2-\lambda \end{bmatrix}\right) \right. \\ & \qquad \qquad \left. +1\cdot \det \left( \begin{bmatrix}1-\lambda & -1 \\ 1 & -1\end{bmatrix}\right) \right) \\ & \qquad -\det \left( 1\cdot \det \left( \begin{bmatrix}1-\lambda & -1 \\ -1 & 3-\lambda \end{bmatrix}\right) +1\cdot \det \left( \begin{bmatrix}4 & 2 \\ -1 & 3-\lambda \end{bmatrix}\right) \right) \\
			& =(5-\lambda) \cdot \left( -(1-(3-\lambda ))+(2-\lambda )\cdot ((1-\lambda )(3-\lambda )-1)\right) \\ & \qquad -\left( 4\cdot (-(2-\lambda )-1)-2\cdot ((1-\lambda )(2-\lambda )+1)+(-(1-\lambda )+1)\right) \\ & \qquad -\left( ((1-\lambda )(3-\lambda )-1)+(4(3-\lambda )+2)\right) \\
			& =\lambda ^4-11\lambda ^3+42\lambda ^2-64\lambda +32 \\
			& =(\lambda -1)(\lambda -2)(\lambda -4)^2.
		\end{split}
	\end{equation}
	}
	Thus, this matrix has three distinct eigenvalues, $1$, $2$, and $4$, with respective multiplicities $1$, $1$, and $2$.
\end{exm}

\section{Bilinear and quadratic forms}

We end this chapter with a discussion of what are called \emph{bilinear forms} and \emph{quadratic forms}.  Our interest comes from the fact that all metric furnish examples of bilinear forms.

\subsection{Basic definitions}

\begin{dfn}{Bilinear form}{BilinearForm}
	Let $V$ be a $\K$-$\K$-bimodule.  Then, a \term{bilinear form}\index{Bilinear form} on $V$ is a bilinear map $\pinnerprod \colon V\times V\rightarrow \K$.
	\begin{rmk}
		Thus, a bilinear form is just a dual-pair in which both spaces are the same.  In this language then, a metric on $V$ is a nonsingular symmetric bilinear form on $V$.
		
		The term ``bilinear form'' is common, but often times I will simply say ``Let $\pinnerprod \colon V\times V\rightarrow \K$ be bilinear\textellipsis ''.  A lot of the time, the extra word ``form'' is unnecessary (it's only necessary when for some reason it's not clear what the domain of the pairing is).
	\end{rmk}
\end{dfn}
\begin{dfn}{Quadratic form}{QuadraticForm}
	Let $V$ be a $\K$-$\K$-bimodule.  Then, a \term{quadratic form}\index{Quadratic form} on $V$ is a function $Q\colon V\rightarrow \K$ of the form $Q(v)=\pinnerprod{v|v}$ for $\pinnerprod \colon V\times V\rightarrow \K$ symmetric and bilinear.
\end{dfn}
\begin{thm}{Quadratic forms are `the same' as symmetric bilinear forms}{}
	Let $V$ be a $\K$-$\K$-bimodule and let $Q\colon V\times V\rightarrow \K$ be a quadratic form on $V$.  Then, if $2\in \K$ is a unit,
	\begin{equation}
		\pinnerprod{v|w}\ceqq \frac{1}{2}\left( Q(v+w)-Q(v)-Q(w)\right) 
	\end{equation}
	is the unique symmetric bilinear form on $V$ such that $Q(v)=\pinnerprod{v|v}$ for all $v\in V$.
	\begin{rmk}
		Every symmetric bilinear form $\pinnerprod$ of course determines a quadratic form $Q(v)\ceqq \pinnerprod{v|v}$ by definition.  This result says that, under the mild hypothesis that $2\in \K$ is a unit, every quadratic form in turn defines a \emph{unique} symmetric bilinear form.
	\end{rmk}
	\begin{rmk}
		For this reason, I will mostly not work with quadratic forms themselves, and shall simply stick to a study of symmetric bilinear forms (and bilinear forms in general).  This only makes a difference if $2$ is not a unit anyways, and which is not often an issue.  An exception, however, is when it comes to diagonalization:  we will see that quadratic forms are always diagonalizable but symmetric bilinear forms are only diagonalizable if $2$ is a unit---see \cref{DiagonalizableForm}.
	\end{rmk}
	\begin{proof}
		We first check that $\pinnerprod$ is in fact bilinear (it is manifestly symmetric).  By definition, there is some symmetric bilinear $B\colon V\times V\rightarrow \K$ such that $Q(v)=B(v,v)$.  Using this, we find
		\begin{equation}
			\begin{split}
				\pinnerprod{v,w} & \ceqq \frac{1}{2}\left( B(v+w,v+w)-B(v,v)-B(w,w)\right) \\
				& =\frac{1}{2}\left( B(v,v)+2B(v,w)+B(w,w)\right. \\ & \qquad \left. -B(v,v)-B(w,w)\right) \\
				& =B(v,w).
			\end{split} 
		\end{equation}
		This shows that that $\pinnerprod$ is bilinear.
		
		We next check that
		\begin{equation}
			\begin{split}
				\pinnerprod{v|v} & \ceqq \frac{1}{2}\left( B(v+v,v+v)-B(v,v)-B(v,v)\right) \\
				& =\frac{1}{2}\left( 4B(v,v)-2B(v,v)\right) =B(v,v)=Q(v).
			\end{split}
		\end{equation}
		
		To show uniqueness, low let $\innerprod \colon V\times V\rightarrow \K$ be symmetric bilinear and such that $\innerprod{v|v}=Q(v)$.  Then,
		\begin{equation}
			\begin{split}
				\innerprod{v|w} & =\frac{1}{2}\left( \innerprod{v+w|v+w}-\innerprod{v|v}-\innerprod{w|w}\right) \\
				& =\frac{1}{2}(Q(v+w)-Q(v)-Q(w))\eqqc \pinnerprod{v|w}.
			\end{split}
		\end{equation}
	\end{proof}
\end{thm}
\begin{exr}{}{}
	Give an example of a vector space $V$ over $\Z /2\Z$ and two distinct symmetric bilinear maps $\pinnerprod ,\innerprod \colon V\times V\rightarrow \K$ such that
	\begin{equation}
		\pinnerprod{v|v}=\innerprod{v|v}
	\end{equation}
	for all $v\in V$.
	\begin{rmk}
		In other words, find a counter-example to the previous result in characteristic $2$.
	\end{rmk}
\end{exr}

\subsection{Bilinear forms and matrices}

Bilinear forms are of course bilinear maps, and as such, in index notation will be written as $B_{ab}$.  This suggests that, upon picking a basis, one will obtain a double-indexed array of scalars $B_{ij}$, $1\leq i,j\leq \dim (V)$.  This is indeed the case, and the other way around, matrices can be used to define bilinear forms.  We stress caution, however, as matrices cannot tell the difference between a $\coord{0,2}$ tensor (a bilinear form) or a $\coord{1,1}$ tensor (a linear transformation) (not to mention $\coord{2,0}$ tensors).  Nevertheless, matrices remain the most convenient way to define bilinear forms.
\begin{prp}{Bilinear form of a matrix}{}
	Let $\K$ be a ring and let $A$ be an $m\times m$ matrix with entries that are central in $\K$.  Then, $B_A\in \tensoralg _2\K ^n$, the \term{bilinear form defined by $A$}, defined by
	\begin{equation}
		v^a[B_A]_{ab}w^b\ceqq \sum _{i,j=1}^nv^iA_{ij}w^j
	\end{equation}
	is bilinear.
	
	Furthermore,
	\begin{enumerate}
		\item $B_A$ is symmetric iff $A_{ij}=A_{ji}$ for all $1\leq i,j\leq n$; and
		\item $B_A$ is nonsingular iff $B_A$ is nondegenerate iff $\det (A)\neq 0$.
	\end{enumerate}
	\begin{rmk}
		Note how we have staggered our indices on $A_{ij}$ different than normally.  This is to suggest that this matrix is supposed to defined a bilinear form, not a linear-transformation.
	\end{rmk}
	\begin{rmk}
		We require that the entries of $A$ be central so that $B_A(v\alpha ,w)=B_A(v,\alpha w)$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
The other way around, we can associate a matrix to a given bilinear form given a choice of basis.
\begin{thm}{Coordinates (of a bilinear form)}{thm5.8.13}
	Let $V$ be a $\K$-module, $\K$ a cring, let $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ be a basis for $V$, and let $B\colon V\times V\rightarrow \K$ be a bilinear form.  Then, there is a unique $d\times d$ matrix, $\coordinates{B}{\basis{B}}$\index[notation]{$\coordinates{B}{\basis{B}}$}, the \term{coordinates}\index{Coordinates (of a bilinear form)} of $B$ with respect to the basis $\basis{B}$ such that
	\begin{equation}\label{eqn5.8.9}
		v^aB_{ab}w^b=\sum _{i,j=1}^dv^i\left[ \coordinates{B}{\basis{B}}\right] _{ij}w^j
	\end{equation}
	for all $v,w\in V$.
	
	Furthermore, explicitly,
	\begin{equation}\label{eqn5.8.14}
		\coordinates{B}{\basis{B}}_{ij}=B(b_i,b_j).
	\end{equation}
	\begin{rmk}
		There are of course analogues for tensors of arbitrary rank.  We hesitate to present them because of their limited usefulness.  For rank at least $4$, the arrays of numbers become essentially impossible to write down on a sheet of paper, and for rank $3$ it's still quite difficult.  Even in the rank $2$ case, there is always the issue that a matrix can't distinguish between $\coord{1,1}$, $\coord{2,0}$, and $\coord{0,2}$ tensors.  Still, given its usefulness in this context, we reluctantly present the result for bilinear forms.
	\end{rmk}
	\begin{rmk}
		This doesn't seem to work in the noncommutative case as there is no way of guaranteeing that the entries of $\coordinates{B}{\basis{B}}$ be central (which we need in order for $B$ to be bilinear.
	\end{rmk}
	\begin{rmk}
		$\coordinates{\blankdot}{\basis{B}}\colon \tensoralg _2V\rightarrow \Matrix _2(\K )$ is an isomorphism and satisfies $\coordinates{B_A}{\basis{S}}=A$, where $\basis{S}$ is the standard basis of $\K ^n$.  There are the direct analogues of \cref{prp3.2.117,prp3.2.100}.  We don't bother reproducing what are essentially carbon copies of previous results here.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}

\subsection{Diagonalizable bilinear forms}

\begin{dfn}{Diagonalizable (bilinear form)}{}
	Let $V$ be a finite-dimensional vector space over a field $\F$ and $B\colon V\times V\rightarrow \F$ be bilinear.  Then, $T$ is \term{diagonalizable}\index{Diagonalizable (bilinear form)} iff there is a basis $\basis{B}$ of $V$ such that $\coordinates{B}{\basis{B}}$ is a diagonal matrix.
	\begin{rmk}
		In this case, $\coordinates{B}{\basis{B}}$ is the \term{diagonalization}\index{Diagonaliation (of a bilinear form)} of $B$ with respect to $\basis{B}$.
	\end{rmk}
	\begin{rmk}
		Warning:  Unlike the case of linear-transformations, where the diagonalizations were unique up to permutation of the diagonal elements, this is \emph{not} the case for diagonalizations of bilinear forms.  For example, if I replace $b$ with $\alpha \cdot b$ in $\basis{B}$, while this would have no effect for the diagonalization of a linear-transformation, this changes the corresponding diagonal element of $\coordinates{B}{\basis{B}}$ by a factor of $\alpha ^2$---see \eqref{eqn5.8.14}.
	\end{rmk}
	\begin{rmk}
		Note that diagonalizable bilinear forms are automatically symmetric (because diagonal matrices are symmetric).
		
		Note also that this did \emph{not} make sense for linear-transformations as there is no notion of what it means for a linear-transformation to be symmetric.\footnote{Unless you have extra structure around, like a metric, in which case you can say that the linear-transformation is symmetric iff the associated bilinear form is.}
	\end{rmk}
\end{dfn}
\begin{dfn}{Diagonalizable (quadratic form)}{}
	Let $V$ be a finite-dimensional vector space over a field $\F$ and let $Q\colon V\rightarrow \F$ be a quadratic form.  Then, $Q$ is \term{diagonalizable}\index{Diagonalizable (quadratic form)} iff there is a diagonalizable symmetric bilinear $\pinnerprod \colon V\times V\rightarrow \F$ such that $Q(v)=\pinnerprod{v|v}$.
\end{dfn}
There is a relatively simple characterization of diagonalizable bilinear forms, analogous to \cref{FundamentalTheoremOfDiagonalizability} for linear-transformations.
\begin{thm}{}{DiagonalizableForm}
	Let $V$ be a finite-dimensional vector space over a field $\F$, let $B\colon V\times V\rightarrow \F$ be a bilinear form, and let $Q\colon V\rightarrow \F$ be a quadratic form.
	\begin{enumerate}
		\item \label{DiagonalizableForm(i)}If $\Char (\F )\neq 2$, then $B$ is diagonalizable iff it is symmetric.
		\item \label{DiagonalizableForm(ii)}$Q$ is diagonalizable.
	\end{enumerate}
	\begin{rmk}
		\cref{DiagonalizableForm(i)} most definitely fails if $\Char (\F )=2$---see the following counter-example.
	\end{rmk}
	\begin{rmk}
		Note how different this criterion is from the one for linear-transformations (\cref{FundamentalTheoremOfDiagonalizability}).
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  See \cite[Theorem 6.35]{Friedberg}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{thm}
\begin{exm}{A symmetric bilinear from that is not diagonalizable}{}\footnote{Adapted from \cite[Example 6.8.5]{Friedberg}.}
	Define $\F \ceqq \Z /2\Z$, $V\ceqq \F ^2$, and let $B_A\colon V\times V\rightarrow \F$ be the bilinear form defined by the matrix
	\begin{equation}
		A\ceqq \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix}.
	\end{equation}
	$B_A$ is symmetric as $A_{ij}=A_{ji}$.
	\begin{exr}[breakable=false]{}{}
		Show that $B_A$ is not diagonalizable.
	\end{exr}
\end{exm}
We have an analogue of \cref{prp4.3.16} which gives the relationship between a matrix that defines a linear-transformation and its diagonalization.
\begin{prp}{}{}
	Let $A$ be an $m\times m$ matrix with entries in a field $\F$, let $B_A\colon V\times V\rightarrow \F$ be the corresponding bilinear map, and let $\basis{B}$ be a diagonalizing basis for $B_A$.  Then,
	\begin{equation}\label{eqn5.8.19}
	\coordinates{B_A}{\basis{B}}=\coordinates{id}{\basis{S}\leftarrow \basis{B}}^{\T}\coordinates{B_A}{\basis{S}}\coordinates{\id}{\basis{S}\leftarrow \basis{B}},
	\end{equation}
	where $\basis{S}$ is the standard basis of $\F ^m$.
	\begin{rmk}
		Note that $\coordinates{B_A}{\basis{S}}=A$ by \cref{thm5.8.13}.
		
		Furthermore, $\coordinates{B_A}{\basis{B}}$ is the diagonalization of $B_A$ (by definition).  Thus, writing $D\ceqq \coordinates{B_A}{\basis{B}}$ and $P\ceqq \coordinates{\id}{\basis{S}\leftarrow \basis{B}}$, this equation is sometimes written more concisely (but perhaps less transparent) as
		\begin{equation}
		D=P^{\T}AP.
		\end{equation}
		Compare \eqref{eqn4.3.22}.
	\end{rmk}
	\begin{rmk}
		One can see how this should be $P^{\T}$ (instead of $P$ or $P^{-1}$) by writing this in index notation.  It should be clearer in index notation that
		\begin{equation}
		D_{ij}=P\indices{_i^x}A_{xy}P\indices{^y_j}.\footnote{You can figure this out using heuristics:  there are only so many ways to combine these indices in such a way that makes sense.  Indicentally, this is one case where writing indices on the left might make this even more obvious:  $D_{ij}=\tensor[_i]{P}{^x}A_{xy}\tensor[^y]{P}{_j}$.}
		\end{equation}
		But in terms of matrices, this is just $D=P^{\T}AP$.
		
		Indeed, I would argue that we should really be writing \eqref{eqn5.8.19} in terms of indices in the first place---matrix multiplication is suggestive of composition, and that's not really what's going on (it's contraction, not composition).
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{prp}
For linear-transformations, to actually compute the diagonalization, we compute the eigenvalues and eigenvectors.  Then, $D$ would be diagonal with eigenvalues along the diagonal and $P$ would be the matrix whose columns were the corresponding eigenvectors of $A$.  We now investigate how to compute the diagonalization and relate it to the original symmetric bilinear form.
\begin{thm}{}{}
	Let $\F$ be a field, let $A$ be a symmetric $m\times m$ matrix with entries in $\F$, and let $R\colon \Matrix _m(\F )\rightarrow \Matrix _m(\F )$ be any composition of row operations such that $R(A)$ is upper-triangular.  Then, $E_RAE_R^{\T}$ is a diagonalization of $B_A\colon \F ^m\times \F ^m\rightarrow \F$ with respect to the basis given by the columns of $E_R^{\T}$, where $E_R$ is the elementary matrix of $R$.
	\begin{rmk}
		In practice, this is used as follows.  Take the matrix $A$ and ``augment'' it with the identity matrix $\id _{m\times m}$ to form an $m\times 2m$ matrix as follows.
		\begin{equation}
			\begin{bmatrix}[c | c]
				A & \id _{m\times m}
			\end{bmatrix}
		\end{equation}
		Now, ``row-column-reduce'' this until you obtain a diagonal matrix on the left, but, while doing so, only before the \emph{row} operations to the right hand side.  Then, what pops out on the right is $R$.
	\end{rmk}
	\begin{rmk}
		Recall that (\cref{ElementaryMatrices}) $E_R$ is the unique matrix such that $R(X)=E_RX$ for all matrices $X\in \Matrix _m(\F )$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
		\end{exr}
	\end{proof}
\end{thm}
What follows is an example of how to use this in practice.
\begin{exm}{}{}\footnote{Adapted from \cite[Example 6.8.6]{Friedberg}.}
	Take $\F \ceqq \R$ and define
	\begin{equation}
		A\ceqq \begin{bmatrix}1 & -1 & 3 \\ -1 & 2 & 1 \\ 3 & 1 & 1\end{bmatrix}.
	\end{equation}
	Augment this to form
	\begin{equation}
		\begin{bmatrix}[c c c | c c c]
			1 & -1 & 3 & 1 & 0 & 0 \\
			-1 & 2 & 1 & 0 & 1 & 0 \\
			3 & 1 & 1 & 0 & 0 & 1
		\end{bmatrix}.
	\end{equation}
	Adding the first column to the second, we obtain\footnote{Note how we did \emph{not} do anything to the right-hand side.}
	\begin{equation}
		\begin{bmatrix}[c c c | c c c]
			1 & 0 & 3 & 1 & 0 & 0 \\
			-1 & 1 & 1 & 0 & 1 & 0 \\
			3 & 4 & 1 & 0 & 0 & 1
		\end{bmatrix}.
	\end{equation}
	Adding the first row to the second, we obtain\footnote{Note how we \emph{did} do the same thing to the right-hand side this time.}
	\begin{equation}
		\begin{bmatrix}[c c c | c c c]
			1 & 0 & 3 & 1 & 0 & 0 \\
			0 & 1 & 4 & 1 & 1 & 0 \\
			3 & 4 & 1 & 0 & 0 & 1
		\end{bmatrix}.
	\end{equation}
	
	We continue similarly:
	\begin{equation}
		\begin{bmatrix}[c c c | c c c]
			1 & 0 & 0 & 1 & 0 & 0 \\
			0 & 1 & 4 & 1 & 1 & 0 \\
			3 & 4 & -8 & 0 & 0 & 1
		\end{bmatrix}
	\end{equation}
	\begin{equation}
		\begin{bmatrix}[c c c | c c c]
			1 & 0 & 0 & 1 & 0 & 0 \\
			0 & 1 & 4 & 1 & 1 & 0 \\
			0 & 4 & -8 & -3 & 0 & 1
		\end{bmatrix}
	\end{equation}
	\begin{equation}
		\begin{bmatrix}[c c c | c c c]
			1 & 0 & 0 & 1 & 0 & 0 \\
			0 & 1 & 0 & 1 & 1 & 0 \\
			0 & 4 & -24 & -3 & 0 & 1
		\end{bmatrix}
	\end{equation}
	\begin{equation}
		\begin{bmatrix}[c c c | c c c]
			1 & 0 & 0 & 1 & 0 & 0 \\
			0 & 1 & 0 & 1 & 1 & 0 \\
			0 & 0 & -24 & -7 & -4 & 1
		\end{bmatrix}
	\end{equation}
	
	According to this, we should have that
	\begin{equation}
		\begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -24\end{bmatrix}=P^{\T}AP,
	\end{equation}
	where
	\begin{equation}
		P\ceqq \begin{bmatrix}1 & 1 & -7 \\ 0 & 1 & -4 \\ 0 & 0 & 1\end{bmatrix},
	\end{equation}
	and you can verify that this is indeed the case by a computation.
\end{exm}

\subsection{Sylvester's Law of Inertia}

We mentioned before that diagonalizations of bilinear forms are not necessarily unique, as, upon scaling a basis element by $\alpha$, the corresponding diagonal element gets scaled by $\alpha ^2$.  \emph{Sylvester's Law of Inertia} tells us to what extent we can obtain uniqueness over the real numbers.  As the set of real numbers that are squares are precisely the nonnegative real numbers, one would expect that we can always choose a basis in which the diagonal elements are either $1$, $-1$, or $0$.\footnote{If the entry on the diagonal was previously $a\neq 0$, scale the corresponding basis element by $\frac{1}{\sqrt{\abs{a}}}$.}
\begin{thm}{Sylvester's Law of Inertia}{SylvestersLawOfInertia}
	Let $V$ be a finite-dimensional real vector space and let $B\colon V\times V\rightarrow \R$ be symmetric bilinear.  Then, there is a basis $\basis{B}$ of $V$ such that $\coordinates{B}{\basis{B}}$ is a diagonal matrix whose diagonal entries are all $+1$, $-1$, or $0$.
	
	Furthermore, the number of diagonal entries which are $+1$, $-1$, and $0$ are all independent of the choice of $\basis{B}$.
	\begin{rmk}
		The number of $+1$s on the diagonal is the \term{positive index of inertia}\index{Positive index of inertia} $m_+$ and the number of $-1$s on the diagonal is the \term{negative index of inertia}\index{Negative index of inertia} $m_-$.  The number of $0$s on the diagonal, $m_0$, is equal to the dimension of the kernel of the map $V\rightarrow V^{\T}$, $v^a\mapsto B_{ab}v^b$.  $\coord{n_0,n_+,n_-}$ is the \term{signature}\index{Signature} of $B$.\footnote{Some authors take $n_--n_+$ to be the \term{signature}.}
	\end{rmk}
	\begin{rmk}
		Thus, there is a basis $\basis{B}\eqqc \{ b_1,\ldots ,b_d\}$ such that $B(b_i,b_j)=0$ if $i\neq j$ and $B(b_i,b_i)=\pm 1$ otherwise.  Such a basis is sometimes said to be an \emph{orthonormal} basis for $B$.
	\end{rmk}
	\begin{rmk}
		Essentially the same thing is true over $\C$, except now there will only be $+1$ and $0$ on the diagonal.  This essentially follows from the fact that we can now scale the diagonal elements by $-1$, as this is a square ($-1=\im ^2$) in $\C$.
	\end{rmk}
	\begin{proof}
		We leave this as an exercise.
		\begin{exr}[breakable=false]{}{}
			Prove the result.
			\begin{rmk}
				Hint:  See \cite[Theorem 6.38]{Friedberg}.
			\end{rmk}
		\end{exr}
	\end{proof}
\end{thm}